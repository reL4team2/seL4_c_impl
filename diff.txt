
diff --git a/.gitignore b/.gitignore
index b18989fe..9edd1325 100644
--- a/.gitignore
+++ b/.gitignore
@@ -17,11 +17,6 @@
 *.blg
 *.toc
 *.mpx
-*.acn
-*.glo
-*.gls
-*.ilg
-*.ist
 
 tools/parsetab.py
 manual/env.tex
diff --git a/.licenseignore b/.licenseignore
new file mode 100644
index 00000000..54d056fe
--- /dev/null
+++ b/.licenseignore
@@ -0,0 +1,13 @@
+# Copyright 2020 Data61, CSIRO (ABN 41 687 119 230)
+# SPDX-License-Identifier: BSD-2-Clause
+
+VERSION
+configs/*/autoconf.h
+manual/figs/*
+manual/references.bib
+.licenseignore
+*.pyc
+.git/*
+*parsetab.py
+CHANGES
+.reuse/dep5

diff --git a/include/api/types.h b/include/api/types.h
index 3f667506..b2b747a1 100644
--- a/include/api/types.h
+++ b/include/api/types.h
@@ -20,11 +20,6 @@
 /* seL4_CapRights_t defined in mode/api/shared_types.bf */
 
 typedef word_t prio_t;
-
-/* The kernel uses ticks_t internally to represent time to make it easy to
- * interact with hardware timers. The userland API uses time in micro seconds,
- * which is represented by time_t in the kernel.
- */
 typedef uint64_t ticks_t;
 typedef uint64_t time_t;
 
@@ -103,11 +98,11 @@ static inline word_t CONST wordFromMessageInfo(seL4_MessageInfo_t mi)
 #ifdef CONFIG_COLOUR_PRINTING
 #define ANSI_RESET "\033[0m"
 #define ANSI_GREEN ANSI_RESET "\033[32m"
-#define ANSI_BOLD  ANSI_RESET "\033[1m"
+#define ANSI_DARK  ANSI_RESET "\033[30;1m"
 #else
 #define ANSI_RESET ""
 #define ANSI_GREEN ANSI_RESET ""
-#define ANSI_BOLD  ANSI_RESET ""
+#define ANSI_DARK  ANSI_RESET ""
 #endif
 
 /*
@@ -135,8 +130,8 @@ extern struct debug_syscall_error current_debug_error;
  */
 #define userError(M, ...) \
     do {                                                                       \
-        out_error(ANSI_BOLD "<<" ANSI_GREEN "seL4(CPU %" SEL4_PRIu_word ")"    \
-                ANSI_BOLD " [%s/%d T%p \"%s\" @%lx]: " M ">>" ANSI_RESET "\n", \
+        out_error(ANSI_DARK "<<" ANSI_GREEN "seL4(CPU %" SEL4_PRIu_word ")"    \
+                ANSI_DARK " [%s/%d T%p \"%s\" @%lx]: " M ">>" ANSI_RESET "\n", \
                 CURRENT_CPU_INDEX(),                                           \
                 __func__, __LINE__, NODE_STATE(ksCurThread),                   \
                 THREAD_NAME,                                                   \



diff --git a/include/arch/arm/arch/64/mode/fastpath/fastpath.h b/include/arch/arm/arch/64/mode/fastpath/fastpath.h
index 31332826..e3bd6fb1 100644
--- a/include/arch/arm/arch/64/mode/fastpath/fastpath.h
+++ b/include/arch/arm/arch/64/mode/fastpath/fastpath.h
@@ -25,7 +25,7 @@ compile_assert(SysReplyRecv_Minus2, SysReplyRecv == -2)
 
 /* Use macros to not break verification */
 #define endpoint_ptr_get_epQueue_tail_fp(ep_ptr) TCB_PTR(endpoint_ptr_get_epQueue_tail(ep_ptr))
-#define cap_vtable_cap_get_vspace_root_fp(vtable_cap) VSPACE_PTR(cap_vspace_cap_get_capVSBasePtr(vtable_cap))
+#define cap_vtable_cap_get_vspace_root_fp(vtable_cap) cap_vtable_root_get_basePtr(vtable_cap)
 
 static inline void FORCE_INLINE
 switchToThread_fp(tcb_t *thread, vspace_root_t *vroot, pde_t stored_hw_asid)
@@ -96,8 +96,8 @@ static inline void mdb_node_ptr_set_mdbPrev_np(mdb_node_t *node_ptr, word_t mdbP
 
 static inline bool_t isValidVTableRoot_fp(cap_t vspace_root_cap)
 {
-    return cap_capType_equals(vspace_root_cap, cap_vspace_cap)
-           && cap_vspace_cap_get_capVSIsMapped(vspace_root_cap);
+    return cap_capType_equals(vspace_root_cap, cap_vtable_root_cap)
+           && cap_vtable_root_isMapped(vspace_root_cap);
 }
 
 /* This is an accelerated check that msgLength, which appears
diff --git a/include/arch/arm/arch/64/mode/kernel/vspace.h b/include/arch/arm/arch/64/mode/kernel/vspace.h
index d089d1c6..d6635c52 100644
--- a/include/arch/arm/arch/64/mode/kernel/vspace.h
+++ b/include/arch/arm/arch/64/mode/kernel/vspace.h
@@ -18,6 +18,11 @@
 bool_t CONST isVTableRoot(cap_t cap);
 bool_t CONST isValidNativeRoot(cap_t cap);
 
+pgde_t *pageUpperDirectoryMapped(asid_t asid, vptr_t vaddr, pude_t *pud);
+pude_t *pageDirectoryMapped(asid_t asid, vptr_t vaddr, pde_t *pd);
+void unmapPageUpperDirectory(asid_t asid, vptr_t vaddr, pude_t *pud);
+void unmapPageDirectory(asid_t asid, vptr_t vaddr, pde_t *pd);
+
 void unmapPageTable(asid_t asid, vptr_t vaddr, pte_t *pt);
 void unmapPage(vm_page_size_t page_size, asid_t asid, vptr_t vptr, pptr_t pptr);
 
@@ -38,28 +43,99 @@ static const region_t BOOT_RODATA *mode_reserved_region = NULL;
 #define PAR_EL1_MASK 0x0000fffffffff000ul
 #define GET_PAR_ADDR(x) ((x) & PAR_EL1_MASK)
 
-static inline exception_t performASIDPoolInvocation(asid_t asid, asid_pool_t *poolPtr, cte_t *vspaceCapSlot)
+#ifdef AARCH64_VSPACE_S2_START_L1
+
+#define cap_vtable_root_cap cap_page_upper_directory_cap
+#define cap_vtable_root_get_mappedASID(_c) \
+    cap_page_upper_directory_cap_get_capPUDMappedASID(_c)
+#define cap_vtable_root_get_basePtr(_c) \
+    VSPACE_PTR(cap_page_upper_directory_cap_get_capPUDBasePtr(_c))
+#define cap_vtable_root_isMapped(_c) \
+    cap_page_upper_directory_cap_get_capPUDIsMapped(_c)
+
+#ifdef CONFIG_ARM_SMMU
+#define cap_vtable_root_get_mappedCB(_c) \
+    cap_page_upper_directory_cap_get_capPUDMappedCB(_c)
+#define cap_vtable_root_ptr_set_mappedCB(_c, cb) \
+    cap_page_upper_directory_cap_ptr_set_capPUDMappedCB(_c, cb)
+#define cap_vtable_cap_new(_a, _v, _m) cap_page_upper_directory_cap_new(_a, _v, _m, 0, CB_INVALID)
+#else
+#define cap_vtable_cap_new(_a, _v, _m) cap_page_upper_directory_cap_new(_a, _v, _m, 0)
+#endif  /*!CONFIG_ARM_SMMU*/
+
+#define vtable_invalid_get_stored_asid_valid(_v) \
+    pude_pude_invalid_get_stored_asid_valid(_v)
+#define vtable_invalid_get_stored_hw_asid(_v) pude_pude_invalid_get_stored_hw_asid(_v)
+
+static inline exception_t performASIDPoolInvocation(asid_t asid, asid_pool_t *poolPtr, cte_t *cte)
+{
+    cap_page_upper_directory_cap_ptr_set_capPUDMappedASID(&cte->cap, asid);
+    cap_page_upper_directory_cap_ptr_set_capPUDIsMapped(&cte->cap, 1);
+    asid_map_t asid_map = asid_map_asid_map_vspace_new(
+#ifdef CONFIG_ARM_SMMU
+                              /* bind_cb: Number of bound context banks */
+                              0,
+#endif
+                              /* vspace_root: reference to vspace root page table object */
+                              cap_page_upper_directory_cap_get_capPUDBasePtr(cte->cap)
+#ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
+                              /* stored_hw_vmid, stored_vmid_valid: Assigned hardware VMID for TLB. */
+                              , 0, false
+#endif
+                          );
+    poolPtr->array[asid & MASK(asidLowBits)] = asid_map;
+    return EXCEPTION_NONE;
+}
+
+
+#else
+
+#define cap_vtable_root_cap cap_page_global_directory_cap
+#define cap_vtable_root_get_mappedASID(_c) \
+    cap_page_global_directory_cap_get_capPGDMappedASID(_c)
+#define cap_vtable_root_get_basePtr(_c) \
+    PGDE_PTR(cap_page_global_directory_cap_get_capPGDBasePtr(_c))
+#define cap_vtable_root_isMapped(_c) cap_page_global_directory_cap_get_capPGDIsMapped(_c)
+
+#ifdef CONFIG_ARM_SMMU
+#define cap_vtable_root_get_mappedCB(_c) \
+    cap_page_global_directory_cap_get_capPGDMappedCB(_c)
+#define cap_vtable_root_ptr_set_mappedCB(_c, cb) \
+    cap_page_global_directory_cap_ptr_set_capPGDMappedCB(_c, cb)
+#define cap_vtable_cap_new(_a, _v, _m) \
+    cap_page_global_directory_cap_new(_a, _v, _m, CB_INVALID)
+#else
+#define cap_vtable_cap_new(_a, _v, _m) \
+    cap_page_global_directory_cap_new(_a, _v, _m)
+#endif /*!CONFIG_ARM_SMMU*/
+
+#define vtable_invalid_get_stored_asid_valid(_v) \
+    pgde_pgde_invalid_get_stored_asid_valid(_v)
+#define vtable_invalid_get_stored_hw_asid(_v) pgde_pgde_invalid_get_stored_hw_asid(_v)
+
+static inline exception_t performASIDPoolInvocation(asid_t asid, asid_pool_t *poolPtr, cte_t *cte)
 {
-    cap_t cap = vspaceCapSlot->cap;
+    cap_page_global_directory_cap_ptr_set_capPGDMappedASID(&cte->cap, asid);
+    cap_page_global_directory_cap_ptr_set_capPGDIsMapped(&cte->cap, 1);
     asid_map_t asid_map = asid_map_asid_map_vspace_new(
 #ifdef CONFIG_ARM_SMMU
                               /* bind_cb: Number of bound context banks */
                               0,
 #endif
                               /* vspace_root: reference to vspace root page table object */
-                              cap_vspace_cap_get_capVSBasePtr(cap)
+                              cap_page_global_directory_cap_get_capPGDBasePtr(cte->cap)
 #ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
                               /* stored_hw_vmid, stored_vmid_valid: Assigned hardware VMID for TLB. */
                               , 0, false
 #endif
                           );
-    cap = cap_vspace_cap_set_capVSMappedASID(cap, asid);
-    cap = cap_vspace_cap_set_capVSIsMapped(cap, 1);
-    vspaceCapSlot->cap = cap;
 
     poolPtr->array[asid & MASK(asidLowBits)] = asid_map;
+
     return EXCEPTION_NONE;
 }
 
 void increaseASIDBindCB(asid_t asid);
 void decreaseASIDBindCB(asid_t asid);
+
+#endif
diff --git a/include/arch/arm/arch/64/mode/machine.h b/include/arch/arm/arch/64/mode/machine.h
index d869e392..6c84863a 100644
--- a/include/arch/arm/arch/64/mode/machine.h
+++ b/include/arch/arm/arch/64/mode/machine.h
@@ -304,7 +304,7 @@ static inline void invalidateByVA_I(vptr_t vaddr, paddr_t paddr)
 
 static inline void invalidate_I_PoU(void)
 {
-#ifdef CONFIG_ENABLE_SMP_SUPPORT
+#if CONFIG_MAX_NUM_NODES > 1
     asm volatile("ic ialluis");
 #else
     asm volatile("ic iallu");
@@ -351,7 +351,6 @@ static inline word_t ats1e2r(word_t va)
 {
     word_t par;
     asm volatile("at s1e2r, %0" :: "r"(va));
-    isb();
     MRS("par_el1", par);
     return par;
 }
@@ -360,7 +359,6 @@ static inline word_t ats1e1r(word_t va)
 {
     word_t par;
     asm volatile("at s1e1r, %0" :: "r"(va));
-    isb();
     MRS("par_el1", par);
     return par;
 }
@@ -370,7 +368,6 @@ static inline word_t ats2e0r(word_t va)
 {
     word_t par;
     asm volatile("at s12e0r, %0" :: "r"(va));
-    isb();
     MRS("par_el1", par);
     return par;
 }
diff --git a/include/arch/arm/arch/64/mode/machine/fpu.h b/include/arch/arm/arch/64/mode/machine/fpu.h
index 34d8a8a4..b0d289f3 100644
--- a/include/arch/arm/arch/64/mode/machine/fpu.h
+++ b/include/arch/arm/arch/64/mode/machine/fpu.h
@@ -134,12 +134,6 @@ static inline void enableFpu(void)
     isFPUEnabledCached[CURRENT_CPU_INDEX()] = true;
 }
 
-/* Current verification model does not include lazy FPU switching, i.e. it acts
- * as if this function always returns true, so no FPU faults could be produced.
- * In order to guard against deriving a contradiction, we don't allow the C
- * parser to translate it. */
-/** MODIFIES: */
-/** DONT_TRANSLATE */
 static inline bool_t isFpuEnable(void)
 {
     return isFPUEnabledCached[CURRENT_CPU_INDEX()];
diff --git a/include/arch/arm/arch/64/mode/machine/hardware.h b/include/arch/arm/arch/64/mode/machine/hardware.h
index 8003a688..f9533fba 100644
--- a/include/arch/arm/arch/64/mode/machine/hardware.h
+++ b/include/arch/arm/arch/64/mode/machine/hardware.h
@@ -11,39 +11,6 @@
 
 #define PAGE_BITS seL4_PageBits
 
-/* Extract the n-level PT index from a virtual address:
- * - n is page table level counting from the root page table,
- * - NUM_PT_LEVELS are either 3 or 4 page table levels depending
- *   on whether the address range being translated is 48bits, 44 bits or 40 bits.
- * - If translating an address in the kernel addrspace NUM_PT_LEVELS = 4 always.
- * - PageTables always have 512 slots (PT_INDEX_BITS = 9) but if there are only
- *   3 total levels then the root level is implemented with 4 concatenated tables
- *   meaning 2048 slots (UPT_LEVELS = 3 => seL4_VSpaceIndexBits = 12)
- *
- * PT_LEVEL_SHIFT(n) == PT_INDEX_BITS * (NUM_PT_LEVELS - n) + seL4_PageBits
- * GET_PT_INDEX(addr, n) == (addr >> PT_LEVEL_SHIFT(n)) & MASK(PT_INDEX_BITS)
- */
-#ifdef AARCH64_VSPACE_S2_START_L1
-#define UPT_LEVELS 3
-#define ULVL_FRM_ARM_PT_LVL(n) ((n)-1)
-#else
-#define UPT_LEVELS 4
-#define ULVL_FRM_ARM_PT_LVL(n) (n)
-#endif
-#define KPT_LEVELS 4
-#define KLVL_FRM_ARM_PT_LVL(n) (n)
-
-#define KPT_LEVEL_SHIFT(n) (((PT_INDEX_BITS) * (((KPT_LEVELS) - 1) - (n))) + seL4_PageBits)
-#define GET_KPT_INDEX(addr, n)  (((addr) >> KPT_LEVEL_SHIFT(n)) & MASK(PT_INDEX_BITS))
-#define GET_KLVL_PGSIZE(n)      BIT(KPT_LEVEL_SHIFT((n)))
-
-#define UPT_LEVEL_SHIFT(n) (((PT_INDEX_BITS) * (((UPT_LEVELS) - 1) - (n))) + seL4_PageBits)
-#define UPT_INDEX_MASK(n) (n == 0 ? seL4_VSpaceIndexBits : PT_INDEX_BITS)
-#define GET_UPT_INDEX(addr, n)  (((addr) >> UPT_LEVEL_SHIFT(n)) & MASK(UPT_INDEX_MASK(n)))
-#define GET_ULVL_PGSIZE_BITS(n) UPT_LEVEL_SHIFT((n))
-#define GET_ULVL_PGSIZE(n)      BIT(UPT_LEVEL_SHIFT((n)))
-
-
 /* Control register fields */
 #define CONTROL_M         0  /* MMU enable */
 #define CONTROL_A         1  /* Alignment check enable */
@@ -51,11 +18,8 @@
 #define CONTROL_SA0       4  /* Stack Alignment Check Enable for EL0 */
 #define CONTROL_SA        3  /* Stack Alignment Check for EL1 */
 #define CONTROL_I         12 /* Instruction access Cacheability control */
-#define CONTROL_UCT       15 /* Enable EL0 access to CTR_EL0   */
 #define CONTROL_E0E       24 /* Endianness of data accesses at EL0 */
 #define CONTROL_EE        25 /* Endianness of data accesses at EL1 */
-#define CONTROL_UCI       26 /* Trap EL0 execution of cache maintenance
-                                instructions to EL1 (aarch64 only) */
 
 #ifndef __ASSEMBLER__
 
diff --git a/include/arch/arm/arch/64/mode/machine/timer.h b/include/arch/arm/arch/64/mode/machine/timer.h
index 5c968675..0d6fcc25 100644
--- a/include/arch/arm/arch/64/mode/machine/timer.h
+++ b/include/arch/arm/arch/64/mode/machine/timer.h
@@ -12,10 +12,6 @@
 #include <util.h>
 
 /* timer function definitions that work for all 64 bit arm platforms */
-
-/* Get the max. ticks_t value that can be expressed in time_t (time in us). This
- * is the max. value ticksToUs() can be passed without overflowing.
- */
 static inline CONST ticks_t getMaxTicksToUs(void)
 {
 #if USE_KHZ
diff --git a/include/arch/arm/arch/64/mode/model/statedata.h b/include/arch/arm/arch/64/mode/model/statedata.h
index 9f54afaf..163c4d05 100644
--- a/include/arch/arm/arch/64/mode/model/statedata.h
+++ b/include/arch/arm/arch/64/mode/model/statedata.h
@@ -16,20 +16,16 @@
 #include <arch/object/smmu.h>
 #endif
 
-#ifdef CONFIG_ALLOW_SMC_CALLS
-#include <arch/object/smc.h>
-#endif
-
 /* The top level asid mapping table */
 extern asid_pool_t *armKSASIDTable[BIT(asidHighBits)] VISIBLE;
 
 /* This is the temporary userspace page table in kernel. It is required before running
  * user thread to avoid speculative page table walking with the wrong page table. */
 extern vspace_root_t armKSGlobalUserVSpace[BIT(seL4_VSpaceIndexBits)] VISIBLE;
-extern pte_t armKSGlobalKernelPGD[BIT(PT_INDEX_BITS)] VISIBLE;
+extern pgde_t armKSGlobalKernelPGD[BIT(PGD_INDEX_BITS)] VISIBLE;
 
-extern pte_t armKSGlobalKernelPUD[BIT(PT_INDEX_BITS)] VISIBLE;
-extern pte_t armKSGlobalKernelPDs[BIT(PT_INDEX_BITS)][BIT(PT_INDEX_BITS)] VISIBLE;
+extern pude_t armKSGlobalKernelPUD[BIT(PUD_INDEX_BITS)] VISIBLE;
+extern pde_t armKSGlobalKernelPDs[BIT(PUD_INDEX_BITS)][BIT(PD_INDEX_BITS)] VISIBLE;
 extern pte_t armKSGlobalKernelPT[BIT(PT_INDEX_BITS)] VISIBLE;
 
 #ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
@@ -39,7 +35,7 @@ extern hw_asid_t armKSNextASID VISIBLE;
 #endif
 
 #ifdef CONFIG_KERNEL_LOG_BUFFER
-extern pte_t *armKSGlobalLogPTE;
+pde_t *armKSGlobalLogPDE;
 #endif
 
 
diff --git a/include/arch/arm/arch/64/mode/object/structures.bf b/include/arch/arm/arch/64/mode/object/structures.bf
index 290bffdd..2922c89f 100644
--- a/include/arch/arm/arch/64/mode/object/structures.bf
+++ b/include/arch/arm/arch/64/mode/object/structures.bf
@@ -32,7 +32,7 @@ block frame_cap {
     padding                          6
 }
 
--- Page table caps
+-- Forth-level page table
 block page_table_cap {
     field capPTMappedASID            16
     field_high capPTBasePtr          48
@@ -44,19 +44,47 @@ block page_table_cap {
     padding                          20
 }
 
--- First-level page table (vspace_root)
-block vspace_cap {
-    field capVSMappedASID            16
-    field_high capVSBasePtr          48
+-- Third-level page table (page directory)
+block page_directory_cap {
+    field capPDMappedASID            16
+    field_high capPDBasePtr          48
 
     field capType                    5
-    field capVSIsMapped              1
-#ifdef CONFIG_ARM_SMMU
-    field capVSMappedCB              8
+    padding                          10
+    field capPDIsMapped              1
+    field_high capPDMappedAddress    19
+    padding                          29
+}
+
+-- Second-level page table (page upper directory)
+block page_upper_directory_cap {
+    field capPUDMappedASID           16
+    field_high capPUDBasePtr         48
+
+    field capType                    5
+    field capPUDIsMapped             1
+    field_high capPUDMappedAddress   10
+#if defined (CONFIG_ARM_SMMU)  && defined (AARCH64_VSPACE_S2_START_L1)
+    field capPUDMappedCB             8
+    padding                          40
+#else 
+    padding                          48
+#endif 
+}
+
+-- First-level page table (page global directory)
+block page_global_directory_cap {
+    field capPGDMappedASID           16
+    field_high capPGDBasePtr         48
+
+    field capType                    5
+    field capPGDIsMapped             1
+#ifdef CONFIG_ARM_SMMU 
+    field capPGDMappedCB             8
     padding                          50
-#else
+#else 
     padding                          58
-#endif
+#endif 
 }
 
 -- Cap to the table of 2^7 ASID pools
@@ -126,15 +154,6 @@ block cb_cap {
 
 #endif
 
-#ifdef CONFIG_ALLOW_SMC_CALLS
-block smc_cap {
-    field capSMCBadge 64
-
-    field capType  5
-    padding        59
-}
-#endif
-
 -- NB: odd numbers are arch caps (see isArchCap())
 tagged_union cap capType {
     -- 5-bit tag caps
@@ -157,7 +176,9 @@ tagged_union cap capType {
     -- 5-bit tag arch caps
     tag frame_cap                   1
     tag page_table_cap              3
-    tag vspace_cap                  9
+    tag page_directory_cap          5
+    tag page_upper_directory_cap    7
+    tag page_global_directory_cap   9
     tag asid_control_cap            11
     tag asid_pool_cap               13
 #ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
@@ -169,9 +190,6 @@ tagged_union cap capType {
     tag cb_control_cap              21
     tag cb_cap                      23
 #endif
-#ifdef CONFIG_ALLOW_SMC_CALLS
-    tag smc_cap                     25
-#endif
 }
 
 ---- Arch-independent object types
@@ -203,8 +221,7 @@ block VCPUFault {
 }
 
 block VPPIEvent {
-    padding         55
-    field irq_w      9
+    field irq_w     64
     padding         32
     padding         28
     field seL4_FaultType  4
@@ -254,27 +271,34 @@ tagged_union asid_map type {
 -- PGDE, PUDE, PDEs and PTEs, assuming 48-bit physical address
 base 64(48,0)
 
+block pgde_invalid {
+    padding                         62
+    field pgde_type                 2
+}
 
--- See the definition of pte_type for explanation
--- for pte_sw_type and pte_hw_type
-block pte_table {
-    padding                         5
-    field pte_sw_type               1
-    padding                         10
-    field_high pt_base_address      36
+block pgde_pud {
+    padding                         16
+    field_high pud_base_address     36
     padding                         10
-    field pte_hw_type               2
+    field pgde_type                 2 -- must be 0b11
 }
 
+tagged_union pgde pgde_type {
+    tag pgde_invalid                0
+    tag pgde_pud                    3
+}
+
+block pude_invalid {
+    padding                         62
+    field pude_type                 2
+}
 
--- The level 1 and 2 page pte structure
-block pte_page {
-    padding                         5
-    field pte_sw_type               1
-    padding                         3
+block pude_1g {
+    padding                         9
     field UXN                       1
     padding                         6
-    field_high page_base_address    36
+    field_high page_base_address    18
+    padding                         18
     field nG                        1
     field AF                        1
     field SH                        2
@@ -285,17 +309,28 @@ block pte_page {
     padding                         1
     field AttrIndx                  3
 #endif
-    field pte_hw_type               2
+    field pude_type                 2
+}
+
+block pude_pd {
+    padding                         16
+    field_high pd_base_address      36
+    padding                         10
+    field pude_type                 2
+}
+
+tagged_union pude pude_type {
+    tag pude_invalid                0
+    tag pude_1g                     1
+    tag pude_pd                     3
 }
 
--- The level 3 page pte structure
-block pte_4k_page {
-    padding                         5
-    field pte_sw_type               1
-    padding                         3
+block pde_large {
+    padding                         9
     field UXN                       1
     padding                         6
-    field_high page_base_address    36
+    field_high page_base_address    27
+    padding                         9
     field nG                        1
     field AF                        1
     field SH                        2
@@ -306,27 +341,38 @@ block pte_4k_page {
     padding                         1
     field AttrIndx                  3
 #endif
-    field pte_hw_type               2
+    field pde_type                  2
 }
 
-block pte_invalid {
-    padding                         5
-    field pte_sw_type               1
-    padding                         56
-    field pte_hw_type               2
+block pde_small {
+    padding                         16
+    field_high pt_base_address      36
+    padding                         10
+    field pde_type                  2
 }
 
--- There are two page type fields because the 4k page size
--- uses a different hardware encoding. We use bit 58
--- which is reserved for software use to encode this
--- difference in the tag for these types.
-tagged_union pte pte_type(pte_hw_type, pte_sw_type) {
-    tag pte_table               (3, 0)
-    tag pte_page                (1, 0)
-    tag pte_4k_page             (3, 1)
-    tag pte_invalid             (0, 0)
+tagged_union pde pde_type {
+    tag pde_large                   1
+    tag pde_small                   3
 }
 
+block pte {
+    padding                         9
+    field UXN                       1
+    padding                         6
+    field_high page_base_address    36
+    field nG                        1
+    field AF                        1
+    field SH                        2
+    field AP                        2
+#ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
+    field AttrIndx                  4
+#else
+    padding                         1
+    field AttrIndx                  3
+#endif
+    field reserved                  2 -- must be 0b11
+}
 
 block ttbr {
     field asid                      16
diff --git a/include/arch/arm/arch/64/mode/object/structures.h b/include/arch/arm/arch/64/mode/object/structures.h
index 781aff15..c64434e8 100644
--- a/include/arch/arm/arch/64/mode/object/structures.h
+++ b/include/arch/arm/arch/64/mode/object/structures.h
@@ -26,29 +26,73 @@ typedef struct arch_tcb {
 enum vm_rights {
     VMKernelOnly = 0,
     VMReadWrite = 1,
+    VMKernelReadOnly = 2,
     VMReadOnly = 3
 };
 typedef word_t vm_rights_t;
 
+/* If hypervisor support for aarch64 is enabled and we run on processors with
+ * 40-bit PA, the stage-2 translation for EL1/EL0 uses a 3-level translation, skipping the PGD level.
+ * Yet the kernel will still use a stage-1 translation with 48 bit input addresses and a 4-level
+ * translation.  Therefore, PUD and PGD size for the kernel can be different from EL1/EL0
+ * so we do not use the libsel4 definitions */
+#define PGD_SIZE_BITS       12
+#define PGD_INDEX_BITS      9
+#define PUD_SIZE_BITS       12
+#define PUD_INDEX_BITS      9
+#define UPUD_SIZE_BITS      seL4_PUDBits
+#define UPUD_INDEX_BITS     seL4_PUDIndexBits
+
+#define PDE_SIZE_BITS       seL4_PageDirEntryBits
+#define PD_INDEX_BITS       seL4_PageDirIndexBits
 #define PTE_SIZE_BITS       seL4_PageTableEntryBits
 #define PT_INDEX_BITS       seL4_PageTableIndexBits
 
 #define PT_INDEX_OFFSET     (seL4_PageBits)
+#define PD_INDEX_OFFSET     (PT_INDEX_OFFSET + PT_INDEX_BITS)
+#define PUD_INDEX_OFFSET    (PD_INDEX_OFFSET + PD_INDEX_BITS)
+#define PGD_INDEX_OFFSET    (PUD_INDEX_OFFSET + PUD_INDEX_BITS)
 
 #define VCPU_SIZE_BITS      seL4_VCPUBits
 
 #ifdef AARCH64_VSPACE_S2_START_L1
 /* For hyp with 40 bit PA, EL1 and EL0 use a 3 level translation and skips the PGD */
-typedef pte_t vspace_root_t;
+typedef pude_t vspace_root_t;
 #else
 /* Otherwise we use a 4-level translation */
-typedef pte_t vspace_root_t;
+typedef pgde_t vspace_root_t;
 #endif
 
 #define VSPACE_PTR(r)       ((vspace_root_t *)(r))
 
+#define GET_PGD_INDEX(x)    (((x) >> (PGD_INDEX_OFFSET)) & MASK(PGD_INDEX_BITS))
+#define GET_PUD_INDEX(x)    (((x) >> (PUD_INDEX_OFFSET)) & MASK(PUD_INDEX_BITS))
+#define GET_UPUD_INDEX(x)   (((x) >> (PUD_INDEX_OFFSET)) & MASK(UPUD_INDEX_BITS))
+#define GET_PD_INDEX(x)     (((x) >> (PD_INDEX_OFFSET)) & MASK(PD_INDEX_BITS))
+#define GET_PT_INDEX(x)     (((x) >> (PT_INDEX_OFFSET)) & MASK(PT_INDEX_BITS))
+
+#define PGDE_PTR(r)         ((pgde_t *)(r))
+#define PGDE_PTR_PTR(r)     ((pgde_t **)(r))
+#define PGDE_REF(p)         ((word_t)(p))
+
+#define PGD_PTR(r)          ((pgde_t *)(r))
+#define PGD_REF(p)          ((word_t)(r))
+
+#define PUDE_PTR(r)         ((pude_t *)(r))
+#define PUDE_PTR_PTR(r)     ((pude_t **)(r))
+#define PUDE_REF(p)         ((word_t)(p))
+
+#define PUD_PTR(r)          ((pude_t *)(r))
+#define PUD_PREF(p)         ((word_t)(p))
+
+#define PDE_PTR(r)          ((pde_t *)(r))
+#define PDE_PTR_PTR(r)      ((pde_t **)(r))
+#define PDE_REF(p)          ((word_t)(p))
+
+#define PD_PTR(r)           ((pde_t *)(r))
+#define PD_REF(p)           ((word_t)(p))
+
 #define PTE_PTR(r)          ((pte_t *)(r))
-#define PTE_PTR_PTR(r)      ((pte_t **)(r))
 #define PTE_REF(p)          ((word_t)(p))
 
 #define PT_PTR(r)           ((pte_t *)(r))
@@ -63,12 +107,6 @@ struct asid_pool {
 };
 typedef struct asid_pool asid_pool_t;
 
-/* Generic fastpath.c code expects pde_t for stored_hw_asid
- * that's a workaround in the time being.
- */
-typedef pte_t pde_t;
-
-
 #define ASID_POOL_PTR(r)    ((asid_pool_t*)r)
 #define ASID_POOL_REF(p)    ((word_t)p)
 
@@ -94,8 +132,14 @@ static inline word_t CONST cap_get_archCapSizeBits(cap_t cap)
     case cap_page_table_cap:
         return seL4_PageTableBits;
 
-    case cap_vspace_cap:
-        return seL4_VSpaceBits;
+    case cap_page_directory_cap:
+        return seL4_PageDirBits;
+
+    case cap_page_upper_directory_cap:
+        return seL4_PUDBits;
+
+    case cap_page_global_directory_cap:
+        return seL4_PGDBits;
 
     case cap_asid_pool_cap:
         return seL4_ASIDPoolBits;
@@ -128,7 +172,13 @@ static inline bool_t CONST cap_get_archCapIsPhysical(cap_t cap)
     case cap_page_table_cap:
         return true;
 
-    case cap_vspace_cap:
+    case cap_page_directory_cap:
+        return true;
+
+    case cap_page_upper_directory_cap:
+        return true;
+
+    case cap_page_global_directory_cap:
         return true;
 
     case cap_asid_pool_cap:
@@ -159,10 +209,16 @@ static inline void *CONST cap_get_archCapPtr(cap_t cap)
         return (void *)(cap_frame_cap_get_capFBasePtr(cap));
 
     case cap_page_table_cap:
-        return PT_PTR(cap_page_table_cap_get_capPTBasePtr(cap));
+        return PD_PTR(cap_page_table_cap_get_capPTBasePtr(cap));
 
-    case cap_vspace_cap:
-        return VSPACE_PTR(cap_vspace_cap_get_capVSBasePtr(cap));
+    case cap_page_directory_cap:
+        return PT_PTR(cap_page_directory_cap_get_capPDBasePtr(cap));
+
+    case cap_page_upper_directory_cap:
+        return PUD_PTR(cap_page_upper_directory_cap_get_capPUDBasePtr(cap));
+
+    case cap_page_global_directory_cap:
+        return PGD_PTR(cap_page_global_directory_cap_get_capPGDBasePtr(cap));
 
     case cap_asid_control_cap:
         return NULL;
@@ -181,41 +237,60 @@ static inline void *CONST cap_get_archCapPtr(cap_t cap)
     }
 }
 
-static inline bool_t pte_pte_page_ptr_get_present(pte_t *pt)
+static inline bool_t pgde_pgde_pud_ptr_get_present(pgde_t *pgd)
+{
+    return (pgde_ptr_get_pgde_type(pgd) == pgde_pgde_pud);
+}
+
+static inline bool_t pude_pude_pd_ptr_get_present(pude_t *pud)
+{
+    return (pude_ptr_get_pude_type(pud) == pude_pude_pd);
+}
+
+static inline bool_t pude_pude_1g_ptr_get_present(pude_t *pud)
 {
-    return (pte_ptr_get_pte_type(pt) == pte_pte_page);
+    return (pude_ptr_get_pude_type(pud) == pude_pude_1g);
 }
 
-static inline bool_t pte_pte_table_ptr_get_present(pte_t *pt)
+static inline pude_t pude_invalid_new(void)
 {
-    return (pte_ptr_get_pte_type(pt) == pte_pte_table);
+    return (pude_t) {
+        {
+            0
+        }
+    };
 }
 
-static inline bool_t pte_4k_page_ptr_get_present(pte_t *pt)
+static inline bool_t pde_pde_small_ptr_get_present(pde_t *pd)
 {
-    return (pte_ptr_get_pte_type(pt) == pte_pte_4k_page);
+    return (pde_ptr_get_pde_type(pd) == pde_pde_small);
 }
 
-static inline bool_t pte_ptr_get_valid(pte_t *pt)
+static inline bool_t pde_pde_large_ptr_get_present(pde_t *pd)
 {
-    return (pte_ptr_get_pte_type(pt) != pte_pte_invalid);
+    return (pde_ptr_get_pde_type(pd) == pde_pde_large);
 }
 
-static inline bool_t pte_is_page_type(pte_t pte)
+static inline pde_t pde_invalid_new(void)
 {
-    return pte_get_pte_type(pte) == pte_pte_4k_page ||
-           pte_get_pte_type(pte) == pte_pte_page;
+    return (pde_t) {
+        {
+            0
+        }
+    };
 }
 
-/** Return base address for both of pte_4k_page and pte_page */
-static inline uint64_t pte_get_page_base_address(pte_t pte)
+static inline bool_t pte_ptr_get_present(pte_t *pt)
 {
-    assert(pte_is_page_type(pte));
-    return pte.words[0] & 0xfffffffff000ull;
+    return (pte_ptr_get_reserved(pt) == 0x3);
 }
 
-/** Return base address for both of pte_4k_page and pte_page */
-static inline uint64_t pte_page_ptr_get_page_base_address(pte_t *pt)
+static inline pte_t pte_invalid_new(void)
 {
-    return pte_get_page_base_address(*pt);
+    return (pte_t) {
+        {
+            0
+        }
+    };
 }
+
diff --git a/include/arch/arm/arch/benchmark.h b/include/arch/arm/arch/benchmark.h
index a4b152a3..140977fa 100644
--- a/include/arch/arm/arch/benchmark.h
+++ b/include/arch/arm/arch/benchmark.h
@@ -37,7 +37,7 @@ static inline void handleOverflowIRQ(void)
     if (likely(NODE_STATE(benchmark_log_utilisation_enabled))) {
         NODE_STATE(ksCurThread)->benchmark.utilisation += UINT32_MAX - NODE_STATE(ksCurThread)->benchmark.schedule_start_time;
         NODE_STATE(ksCurThread)->benchmark.schedule_start_time = 0;
-        ARCH_NODE_STATE(ccnt_num_overflows)++;
+        NODE_STATE(ccnt_num_overflows)++;
     }
     armv_handleOverflowIRQ();
 }
@@ -46,7 +46,7 @@ static inline void handleOverflowIRQ(void)
 static inline void benchmark_arch_utilisation_reset(void)
 {
 #ifdef CONFIG_ARM_ENABLE_PMU_OVERFLOW_INTERRUPT
-    ARCH_NODE_STATE(ccnt_num_overflows) = 0;
+    NODE_STATE(ccnt_num_overflows) = 0;
 #endif /* CONFIG_ARM_ENABLE_PMU_OVERFLOW_INTERRUPT */
 }
 #endif /* CONFIG_ENABLE_BENCHMARKS */
diff --git a/include/arch/arm/arch/kernel/vspace.h b/include/arch/arm/arch/kernel/vspace.h
index 6a352dd3..e8d5f661 100644
--- a/include/arch/arm/arch/kernel/vspace.h
+++ b/include/arch/arm/arch/kernel/vspace.h
@@ -31,6 +31,7 @@ extern char arm_vector_table[1];
 
 word_t *PURE lookupIPCBuffer(bool_t isReceiver, tcb_t *thread);
 exception_t handleVMFault(tcb_t *thread, vm_fault_type_t vm_faultType);
+pde_t *pageTableMapped(asid_t asid, vptr_t vaddr, pte_t *pt);
 void setVMRoot(tcb_t *tcb);
 bool_t CONST isValidVTableRoot(cap_t cap);
 exception_t checkValidIPCBuffer(vptr_t vptr, cap_t cap);
diff --git a/include/arch/arm/arch/machine/gic_common.h b/include/arch/arm/arch/machine/gic_common.h
index 584d1b50..f58cac35 100644
--- a/include/arch/arm/arch/machine/gic_common.h
+++ b/include/arch/arm/arch/machine/gic_common.h
@@ -43,8 +43,8 @@
 #define IRQ_IS_PPI(_irq) (HW_IRQ_IS_PPI(_irq.irq))
 #define CORE_IRQ_TO_IRQT(tgt, _irq) ((irq_t){.irq = (_irq), .target_core = (tgt)})
 #define IRQT_TO_IDX(_irq) (HW_IRQ_IS_PPI(_irq.irq) ? \
-                                 (_irq.target_core) * NUM_PPI + (_irq.irq) : \
-                                 (CONFIG_MAX_NUM_NODES - 1) * NUM_PPI + (_irq.irq))
+                                 (irq.target_core)*NUM_PPI + (_irq.irq) : \
+                                 (CONFIG_MAX_NUM_NODES-1)*NUM_PPI + (_irq.irq))
 
 #define IDX_TO_IRQT(idx) (((idx) < NUM_PPI*CONFIG_MAX_NUM_NODES) ? \
                         CORE_IRQ_TO_IRQT((idx) / NUM_PPI, (idx) - ((idx)/NUM_PPI)*NUM_PPI): \
diff --git a/include/arch/arm/arch/machine/gic_v2.h b/include/arch/arm/arch/machine/gic_v2.h
index b426606b..92440b64 100644
--- a/include/arch/arm/arch/machine/gic_v2.h
+++ b/include/arch/arm/arch/machine/gic_v2.h
@@ -126,6 +126,21 @@ extern volatile struct gic_dist_map *const gic_dist;
 extern volatile struct gic_cpu_iface_map *const gic_cpuiface;
 
 /* Helpers */
+static inline int is_irq_edge_triggered(word_t irq)
+{
+    int word = irq >> 4;
+    int bit = ((irq & 0xf) * 2);
+    return !!(gic_dist->config[word] & BIT(bit + 1));
+}
+
+static inline void dist_pending_clr(word_t irq)
+{
+    int word = IRQ_REG(irq);
+    int bit = IRQ_BIT(irq);
+    /* Using |= here is detrimental to your health */
+    gic_dist->pending_clr[word] = BIT(bit);
+}
+
 static inline void dist_enable_clr(word_t irq)
 {
     int word = IRQ_REG(irq);
@@ -183,6 +198,9 @@ static inline void ackInterrupt(irq_t irq)
 {
     assert(IS_IRQ_VALID(active_irq[CURRENT_CPU_INDEX()])
            && (active_irq[CURRENT_CPU_INDEX()] & IRQ_MASK) == IRQT_TO_IRQ(irq));
+    if (is_irq_edge_triggered(IRQT_TO_IRQ(irq))) {
+        dist_pending_clr(IRQT_TO_IRQ(irq));
+    }
     gic_cpuiface->eoi = active_irq[CURRENT_CPU_INDEX()];
     active_irq[CURRENT_CPU_INDEX()] = IRQ_NONE;
 
@@ -216,7 +234,7 @@ struct gich_vcpu_ctrl_map {
 };
 
 extern volatile struct gich_vcpu_ctrl_map *gic_vcpu_ctrl;
-extern word_t gic_vcpu_num_list_regs;
+extern unsigned int gic_vcpu_num_list_regs;
 
 static inline uint32_t get_gic_vcpu_ctrl_hcr(void)
 {
diff --git a/include/arch/arm/arch/machine/gic_v3.h b/include/arch/arm/arch/machine/gic_v3.h
index 933444c5..1cfbc6a7 100644
--- a/include/arch/arm/arch/machine/gic_v3.h
+++ b/include/arch/arm/arch/machine/gic_v3.h
@@ -175,11 +175,8 @@ struct gic_dist_map {
                                      * interrupt routing for SPI 32 */
 };
 
-/* __builtin_offsetof is not in the verification C subset, so we can only check this in
-   non-verification builds. We specifically do not declare a macro for the builtin, because
-   we do not want break the verification subset by accident. */
-unverified_compile_assert(error_in_gic_dist_map,
-                          0x6100 == __builtin_offsetof(struct gic_dist_map, iroutern));
+_Static_assert(0x6100 == SEL4_OFFSETOF(struct gic_dist_map, iroutern),
+               "Error in struct gic_dist_map");
 
 /* Memory map for GIC Redistributor Registers for control and physical LPI's */
 struct gic_rdist_map {          /* Starting */
@@ -234,6 +231,37 @@ extern volatile struct gic_rdist_map *gic_rdist_map[CONFIG_MAX_NUM_NODES];
 extern volatile struct gic_rdist_sgi_ppi_map *gic_rdist_sgi_ppi_map[CONFIG_MAX_NUM_NODES];
 
 /* Helpers */
+static inline int is_irq_edge_triggered(word_t irq)
+{
+    uint32_t icfgr = 0;
+    int word = irq >> 4;
+    int bit = ((irq & 0xf) * 2);
+
+    if (HW_IRQ_IS_SGI(irq)) {
+        return 0;
+    }
+    if (HW_IRQ_IS_PPI(irq)) {
+        icfgr = gic_rdist_sgi_ppi_map[CURRENT_CPU_INDEX()]->icfgr1;
+    } else {
+        icfgr = gic_dist->icfgrn[word];
+    }
+
+    return !!(icfgr & BIT(bit + 1));
+}
+
+static inline void gic_pending_clr(word_t irq)
+{
+    int word = IRQ_REG(irq);
+    int bit = IRQ_BIT(irq);
+    /* Using |= here is detrimental to your health */
+    /* Applicable for SPI and PPIs */
+    if (irq < SPI_START) {
+        gic_rdist_sgi_ppi_map[CURRENT_CPU_INDEX()]->icpendr0 = BIT(bit);
+    } else {
+        gic_dist->icpendrn[word] = BIT(bit);
+    }
+}
+
 static inline void gic_enable_clr(word_t irq)
 {
     int word = IRQ_REG(irq);
@@ -309,8 +337,12 @@ static inline void maskInterrupt(bool_t disable, irq_t irq)
 
 static inline void ackInterrupt(irq_t irq)
 {
-    assert(IS_IRQ_VALID(active_irq[CURRENT_CPU_INDEX()])
-           && (active_irq[CURRENT_CPU_INDEX()] & IRQ_MASK) == IRQT_TO_IRQ(irq));
+    word_t hw_irq = IRQT_TO_IRQ(irq);
+    assert(IS_IRQ_VALID(active_irq[CURRENT_CPU_INDEX()]) && (active_irq[CURRENT_CPU_INDEX()] & IRQ_MASK) == hw_irq);
+
+    if (is_irq_edge_triggered(hw_irq)) {
+        gic_pending_clr(hw_irq);
+    }
 
     /* Set End of Interrupt for active IRQ: ICC_EOIR1_EL1 */
     SYSTEM_WRITE_WORD(ICC_EOIR1_EL1, active_irq[CURRENT_CPU_INDEX()]);
@@ -320,7 +352,7 @@ static inline void ackInterrupt(irq_t irq)
 
 #ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
 
-extern word_t gic_vcpu_num_list_regs;
+extern unsigned int gic_vcpu_num_list_regs;
 
 static inline uint32_t get_gic_vcpu_ctrl_hcr(void)
 {
@@ -354,13 +386,13 @@ static inline void set_gic_vcpu_ctrl_vmcr(uint32_t vmcr)
 static inline uint32_t get_gic_vcpu_ctrl_apr(void)
 {
     uint32_t reg;
-    MRS(ICH_AP1R0_EL2, reg);
+    MRS(ICH_AP0R0_EL2, reg);
     return reg;
 }
 
 static inline void set_gic_vcpu_ctrl_apr(uint32_t apr)
 {
-    MSR(ICH_AP1R0_EL2, apr);
+    MSR(ICH_AP0R0_EL2, apr);
 }
 
 static inline uint32_t get_gic_vcpu_ctrl_vtr(void)
diff --git a/include/arch/arm/arch/machine/hardware.h b/include/arch/arm/arch/machine/hardware.h
index 029ea51c..a59be78f 100644
--- a/include/arch/arm/arch/machine/hardware.h
+++ b/include/arch/arm/arch/machine/hardware.h
@@ -18,14 +18,11 @@ typedef word_t vm_fault_type_t;
 
 #define PAGE_BASE(_p, _s)        ((_p) & ~MASK(pageBitsForSize((_s))))
 #define PAGE_OFFSET(_p, _s)      ((_p) & MASK(pageBitsForSize((_s))))
+#define IS_PAGE_ALIGNED(_p, _s)  (((_p) & MASK(pageBitsForSize((_s)))) == 0)
 
 #define IPI_MEM_BARRIER \
   do { \
-     /* This can be relaxed for GICv2 but for GICv3 dmb() no longer works */ \
-     /* since the way IPI is triggered is different (memory-mapped or MSR inst.) */ \
-     /* and dmb() is not able to avoid re-ordering between memory accesses and */ \
-     /* instructions. In order to support both GICv2 and v3 dsb() is required. */ \
-     dsb_ishst(); \
+     dmb(); \
   } while (0)
 
 #endif /* __ASSEMBLER__ */
diff --git a/include/arch/arm/arch/machine/timer.h b/include/arch/arm/arch/machine/timer.h
index 553d8529..0df9b80d 100644
--- a/include/arch/arm/arch/machine/timer.h
+++ b/include/arch/arm/arch/machine/timer.h
@@ -26,9 +26,7 @@
 
 void initTimer(void);
 
-/* Get the max. time_t value (time in us) that can be expressed in ticks_t. This
- * is the max. value usToTicks() can be passed without overflowing.
- */
+/* get the max value usToTicks can be passed without overflowing */
 static inline CONST time_t getMaxUsToTicks(void)
 {
 #if USE_KHZ
@@ -42,7 +40,7 @@ static inline CONST ticks_t usToTicks(time_t us)
 {
 #if USE_KHZ
     /* reciprocal division overflows too quickly for dividing by KHZ_IN_MHZ.
-     * This operation isn't used frequently or on many platforms, so use manual
+     * This operation isn't  used frequently or on many platforms, so use manual
      * division here */
     return div64(us * TIMER_CLOCK_KHZ, KHZ_IN_MHZ);
 #else
diff --git a/include/arch/arm/arch/object/smc.h b/include/arch/arm/arch/object/smc.h
deleted file mode 100644
index cd898274..00000000
--- a/include/arch/arm/arch/object/smc.h
+++ /dev/null
@@ -1,12 +0,0 @@
-/*
- * Copyright 2021, DornerWorks Ltd.
- *
- * SPDX-License-Identifier: GPL-2.0-only
- */
-
-#pragma once
-
-#define NUM_SMC_REGS 8
-
-exception_t decodeARMSMCInvocation(word_t label, word_t length, cptr_t cptr,
-                                   cte_t *srcSlot, cap_t cap, bool_t call, word_t *buffer);
diff --git a/include/arch/arm/arch/object/smmu.h b/include/arch/arm/arch/object/smmu.h
index 81d31b14..b6da97c6 100644
--- a/include/arch/arm/arch/object/smmu.h
+++ b/include/arch/arm/arch/object/smmu.h
@@ -14,18 +14,18 @@
 #define ASID_INVALID     nASIDs
 
 
-exception_t decodeARMSIDControlInvocation(word_t label, word_t length, cptr_t cptr,
+exception_t decodeARMSIDControlInvocation(word_t label, unsigned int length, cptr_t cptr,
                                           cte_t *srcSlot, cap_t cap,
                                           bool_t call, word_t *buffer);
 
-exception_t decodeARMSIDInvocation(word_t label, word_t length, cptr_t cptr,
+exception_t decodeARMSIDInvocation(word_t label, unsigned int length, cptr_t cptr,
                                    cte_t *srcSlot, cap_t cap, bool_t call, word_t *buffer);
 
-exception_t decodeARMCBControlInvocation(word_t label, word_t length, cptr_t cptr,
+exception_t decodeARMCBControlInvocation(word_t label, unsigned int length, cptr_t cptr,
                                          cte_t *srcSlot, cap_t cap,
                                          bool_t call, word_t *buffer);
 
-exception_t decodeARMCBInvocation(word_t label, word_t length, cptr_t cptr,
+exception_t decodeARMCBInvocation(word_t label, unsigned int length, cptr_t cptr,
                                   cte_t *srcSlot, cap_t cap, bool_t call, word_t *buffer);
 exception_t smmu_delete_cb(cap_t cap);
 exception_t smmu_delete_sid(cap_t cap);
diff --git a/include/arch/arm/arch/object/structures.h b/include/arch/arm/arch/object/structures.h
index 88e8782d..7da1e116 100644
--- a/include/arch/arm/arch/object/structures.h
+++ b/include/arch/arm/arch/object/structures.h
@@ -12,16 +12,6 @@
 
 static inline bool_t CONST Arch_isCapRevocable(cap_t derivedCap, cap_t srcCap)
 {
-#ifdef CONFIG_ALLOW_SMC_CALLS
-    switch (cap_get_capType(derivedCap)) {
-    case cap_smc_cap:
-        return (cap_smc_cap_get_capSMCBadge(derivedCap) !=
-                cap_smc_cap_get_capSMCBadge(srcCap));
-
-    default:
-        return false;
-    }
-#endif
     return false;
 }
 
diff --git a/include/arch/arm/arch/object/vcpu.h b/include/arch/arm/arch/object/vcpu.h
index 886bf723..7d9037cd 100644
--- a/include/arch/arm/arch/object/vcpu.h
+++ b/include/arch/arm/arch/object/vcpu.h
@@ -52,9 +52,6 @@ struct gicVCpuIface {
     uint32_t hcr;
     uint32_t vmcr;
     uint32_t apr;
-    /* virq_t[] requires word-size alignment; add extra padding for
-     * 64-bit platforms to make this struct packed. */
-    uint32_t gicVCpuIface_padding;
     virq_t lr[GIC_VCPU_MAX_NUM_LR];
 };
 
@@ -78,13 +75,11 @@ struct vcpu {
     word_t regs[seL4_VCPUReg_Num];
     bool_t vppi_masked[n_VPPIEventIRQ];
 #ifdef CONFIG_VTIMER_UPDATE_VOFFSET
+    /* vTimer is 8-bytes wide and has same alignment requirement.
+     * To keep the struct packed on 32-bit platforms when accompanied by an
+     * odd number of 32-bit words, we need to add a padding word.
+     * */
     word_t vcpu_padding;
-    /* vTimer is 8-bytes wide and has the same 8-byte alignment requirement.
-     * If the sum of n_VPPIEventIRQ and seL4_VCPUReg_Num is even, we do not need
-     * extra padding. If the sum is odd we do. It currently is odd, so the extra
-     * padding above is necessary for the struct to remain packed on 32 bit
-     * platforms.
-     */
     struct vTimer virtTimer;
 #endif
 };
@@ -109,7 +104,7 @@ void dissociateVCPUTCB(vcpu_t *vcpu, tcb_t *tcb);
 
 exception_t decodeARMVCPUInvocation(
     word_t label,
-    word_t length,
+    unsigned int length,
     cptr_t cptr,
     cte_t *slot,
     cap_t cap,
@@ -123,11 +118,11 @@ void vcpu_switch(vcpu_t *cpu);
 void handleVCPUInjectInterruptIPI(vcpu_t *vcpu, unsigned long index, virq_t virq);
 #endif /* ENABLE_SMP_SUPPORT */
 
-exception_t decodeVCPUWriteReg(cap_t cap, word_t length, word_t *buffer);
-exception_t decodeVCPUReadReg(cap_t cap, word_t length, bool_t call, word_t *buffer);
-exception_t decodeVCPUInjectIRQ(cap_t cap, word_t length, word_t *buffer);
+exception_t decodeVCPUWriteReg(cap_t cap, unsigned int length, word_t *buffer);
+exception_t decodeVCPUReadReg(cap_t cap, unsigned int length, bool_t call, word_t *buffer);
+exception_t decodeVCPUInjectIRQ(cap_t cap, unsigned int length, word_t *buffer);
 exception_t decodeVCPUSetTCB(cap_t cap);
-exception_t decodeVCPUAckVPPI(cap_t cap, word_t length, word_t *buffer);
+exception_t decodeVCPUAckVPPI(cap_t cap, unsigned int length, word_t *buffer);
 
 exception_t invokeVCPUWriteReg(vcpu_t *vcpu, word_t field, word_t value);
 exception_t invokeVCPUReadReg(vcpu_t *vcpu, word_t field, bool_t call);


diff --git a/include/arch/arm/armv/armv8-a/64/armv/machine.h b/include/arch/arm/armv/armv8-a/64/armv/machine.h
index 9bc506c2..4744d231 100644
--- a/include/arch/arm/armv/armv8-a/64/armv/machine.h
+++ b/include/arch/arm/armv/armv8-a/64/armv/machine.h
@@ -16,11 +16,6 @@ static inline void dsb(void)
     asm volatile("dsb sy" ::: "memory");
 }
 
-static inline void dsb_ishst(void)
-{
-    asm volatile("dsb ishst" ::: "memory");
-}
-
 static inline void dmb(void)
 {
     asm volatile("dmb sy" ::: "memory");
diff --git a/include/arch/arm/armv/armv8-a/64/armv/vcpu.h b/include/arch/arm/armv/armv8-a/64/armv/vcpu.h
index da3d360a..82a7536d 100644
--- a/include/arch/arm/armv/armv8-a/64/armv/vcpu.h
+++ b/include/arch/arm/armv/armv8-a/64/armv/vcpu.h
@@ -14,41 +14,35 @@
 #include <drivers/timer/arm_generic.h>
 
 /* Note that the HCR_DC for ARMv8 disables S1 translation if enabled */
+#ifdef CONFIG_DISABLE_WFI_WFE_TRAPS
 /* Trap SMC and override CPSR.AIF */
-#define HCR_COMMON ( HCR_VM | HCR_RW | HCR_AMO | HCR_IMO | HCR_FMO | HCR_TSC)
+#define HCR_COMMON ( HCR_VM | HCR_RW | HCR_AMO | HCR_IMO | HCR_FMO )
+#else
+/* Trap WFI/WFE/SMC and override CPSR.AIF */
+#define HCR_COMMON ( HCR_TWI | HCR_TWE | HCR_VM | HCR_RW | HCR_AMO | HCR_IMO | HCR_FMO )
+#endif
 
 /* Allow native tasks to run at EL0, but restrict access */
 #define HCR_NATIVE ( HCR_COMMON | HCR_TGE | HCR_TVM | HCR_TTLB | HCR_DC \
                    | HCR_TAC | HCR_SWIO |  HCR_TSC )
-
-#ifdef CONFIG_DISABLE_WFI_WFE_TRAPS
-#define HCR_VCPU   ( HCR_COMMON)
-#else
-#define HCR_VCPU   ( HCR_COMMON | HCR_TWE | HCR_TWI)
-#endif
+#define HCR_VCPU   ( HCR_COMMON | HCR_TSC)
 
 #define SCTLR_EL1_UCI       BIT(26)     /* Enable EL0 access to DC CVAU, DC CIVAC, DC CVAC,
                                            and IC IVAU in AArch64 state   */
 #define SCTLR_EL1_C         BIT(2)      /* Enable data and unified caches */
 #define SCTLR_EL1_I         BIT(12)     /* Enable instruction cache       */
 #define SCTLR_EL1_CP15BEN   BIT(5)      /* AArch32 CP15 barrier enable    */
-#define SCTLR_EL1_UCT       BIT(15)     /* Enable EL0 access to CTR_EL0   */
+#define SCTLR_EL1_UTC       BIT(15)     /* Enable EL0 access to CTR_EL0   */
 #define SCTLR_EL1_NTWI      BIT(16)     /* WFI executed as normal         */
 #define SCTLR_EL1_NTWE      BIT(18)     /* WFE executed as normal         */
 
-#ifdef CONFIG_AARCH64_USER_CACHE_ENABLE
-#define SCTLR_NATIVE_USER_CACHE_OPS (SCTLR_EL1_UCI | SCTLR_EL1_UCT)
-#else
-#define SCTLR_NATIVE_USER_CACHE_OPS
-#endif
-
 /* Disable MMU, SP alignment check, and alignment check */
 /* A57 default value */
 #define SCTLR_EL1_RES      0x30d00800   /* Reserved value */
-#define SCTLR_EL1          ( SCTLR_EL1_RES | SCTLR_EL1_CP15BEN | \
-                             SCTLR_EL1_NTWI | SCTLR_EL1_NTWE )
-#define SCTLR_EL1_NATIVE   (SCTLR_EL1 | SCTLR_EL1_C | SCTLR_EL1_I | SCTLR_NATIVE_USER_CACHE_OPS)
-#define SCTLR_EL1_VM       (SCTLR_EL1 | SCTLR_EL1_UCT | SCTLR_EL1_UCI)
+#define SCTLR_EL1          ( SCTLR_EL1_RES | SCTLR_EL1_CP15BEN | SCTLR_EL1_UTC \
+                           | SCTLR_EL1_NTWI | SCTLR_EL1_NTWE )
+#define SCTLR_EL1_NATIVE   (SCTLR_EL1 | SCTLR_EL1_C | SCTLR_EL1_I | SCTLR_EL1_UCI)
+#define SCTLR_EL1_VM       (SCTLR_EL1 | SCTLR_EL1_UCI)
 #define SCTLR_DEFAULT      SCTLR_EL1_NATIVE
 
 #define UNKNOWN_FAULT       0x2000000
@@ -123,7 +117,6 @@
 #define REG_HCR_EL2         "hcr_el2"
 #define REG_VTCR_EL2        "vtcr_el2"
 #define REG_VMPIDR_EL2      "vmpidr_el2"
-#define REG_MPIDR_EL1       "mpidr_el1"
 #define REG_ID_AA64MMFR0_EL1 "id_aa64mmfr0_el1"
 
 /* for EL1 SCTLR */
@@ -197,8 +190,6 @@ static inline void writeAMAIR(word_t reg)
     MSR(REG_AMAIR_EL1, reg);
 }
 
-/** MODIFIES: */
-/** DONT_TRANSLATE */
 static inline word_t readCIDR(void)
 {
     uint32_t reg;
@@ -206,8 +197,6 @@ static inline word_t readCIDR(void)
     return (word_t)reg;
 }
 
-/** MODIFIES: phantom_machine_state */
-/** DONT_TRANSLATE */
 static inline void writeCIDR(word_t reg)
 {
     MSR(REG_CONTEXTIDR_EL1, (uint32_t)reg);
@@ -225,8 +214,6 @@ static inline void writeACTLR(word_t reg)
     MSR(REG_ACTLR_EL1, reg);
 }
 
-/** MODIFIES: */
-/** DONT_TRANSLATE */
 static inline word_t readAFSR0(void)
 {
     uint32_t reg;
@@ -234,15 +221,11 @@ static inline word_t readAFSR0(void)
     return (word_t)reg;
 }
 
-/** MODIFIES: phantom_machine_state */
-/** DONT_TRANSLATE */
 static inline void writeAFSR0(word_t reg)
 {
     MSR(REG_AFSR0_EL1, (uint32_t)reg);
 }
 
-/** MODIFIES: */
-/** DONT_TRANSLATE */
 static inline word_t readAFSR1(void)
 {
     uint32_t reg;
@@ -250,15 +233,11 @@ static inline word_t readAFSR1(void)
     return (word_t)reg;
 }
 
-/** MODIFIES: phantom_machine_state */
-/** DONT_TRANSLATE */
 static inline void writeAFSR1(word_t reg)
 {
     MSR(REG_AFSR1_EL1, (uint32_t)reg);
 }
 
-/** MODIFIES: */
-/** DONT_TRANSLATE */
 static inline word_t readESR(void)
 {
     uint32_t reg;
@@ -266,8 +245,6 @@ static inline word_t readESR(void)
     return (word_t)reg;
 }
 
-/** MODIFIES: phantom_machine_state */
-/** DONT_TRANSLATE */
 static inline void writeESR(word_t reg)
 {
     MSR(REG_ESR_EL1, (uint32_t)reg);
@@ -286,8 +263,6 @@ static inline void writeFAR(word_t reg)
 }
 
 /* ISR is read-only */
-/** MODIFIES: */
-/** DONT_TRANSLATE */
 static inline word_t readISR(void)
 {
     uint32_t reg;
@@ -482,8 +457,10 @@ static word_t vcpu_hw_read_reg(word_t reg_index)
         return readCNTVOFF_EL2();
     case seL4_VCPUReg_CNTKCTL_EL1:
         return readCNTKCTL_EL1();
+#ifdef ENABLE_SMP_SUPPORT
     case seL4_VCPUReg_VMPIDR_EL2:
         return readVMPIDR_EL2();
+#endif /* ENABLE_SMP_SUPPORT */
     default:
         fail("ARM/HYP: Invalid register index");
     }
@@ -495,83 +472,63 @@ static void vcpu_hw_write_reg(word_t reg_index, word_t reg)
 {
     switch (reg_index) {
     case seL4_VCPUReg_SCTLR:
-        setSCTLR(reg);
-        break;
+        return setSCTLR(reg);
     case seL4_VCPUReg_TTBR0:
-        writeTTBR0(reg);
-        break;
+        return writeTTBR0(reg);
     case seL4_VCPUReg_TTBR1:
-        writeTTBR1(reg);
-        break;
+        return writeTTBR1(reg);
     case seL4_VCPUReg_TCR:
-        writeTCR(reg);
-        break;
+        return writeTCR(reg);
     case seL4_VCPUReg_MAIR:
-        writeMAIR(reg);
-        break;
+        return writeMAIR(reg);
     case seL4_VCPUReg_AMAIR:
-        writeAMAIR(reg);
-        break;
+        return writeAMAIR(reg);
     case seL4_VCPUReg_CIDR:
-        writeCIDR(reg);
-        break;
+        return writeCIDR(reg);
     case seL4_VCPUReg_ACTLR:
-        writeACTLR(reg);
-        break;
+        return writeACTLR(reg);
     case seL4_VCPUReg_CPACR:
-        writeCPACR_EL1(reg);
-        break;
+        return writeCPACR_EL1(reg);
     case seL4_VCPUReg_AFSR0:
-        writeAFSR0(reg);
-        break;
+        return writeAFSR0(reg);
     case seL4_VCPUReg_AFSR1:
-        writeAFSR1(reg);
-        break;
+        return writeAFSR1(reg);
     case seL4_VCPUReg_ESR:
-        writeESR(reg);
-        break;
+        return writeESR(reg);
     case seL4_VCPUReg_FAR:
-        writeFAR(reg);
-        break;
+        return writeFAR(reg);
     case seL4_VCPUReg_ISR:
         /* ISR is read-only */
-        break;
+        return;
     case seL4_VCPUReg_VBAR:
-        writeVBAR(reg);
-        break;
+        return writeVBAR(reg);
     case seL4_VCPUReg_TPIDR_EL1:
-        writeTPIDR_EL1(reg);
-        break;
+        return writeTPIDR_EL1(reg);
     case seL4_VCPUReg_SP_EL1:
-        writeSP_EL1(reg);
-        break;
+        return writeSP_EL1(reg);
     case seL4_VCPUReg_ELR_EL1:
-        writeELR_EL1(reg);
-        break;
+        return writeELR_EL1(reg);
     case seL4_VCPUReg_SPSR_EL1:
-        writeSPSR_EL1(reg);
-        break;
+        return writeSPSR_EL1(reg);
     case seL4_VCPUReg_CNTV_CTL:
-        writeCNTV_CTL_EL0(reg);
-        break;
+        return writeCNTV_CTL_EL0(reg);
     case seL4_VCPUReg_CNTV_CVAL:
-        writeCNTV_CVAL_EL0(reg);
-        break;
+        return writeCNTV_CVAL_EL0(reg);
     case seL4_VCPUReg_CNTVOFF:
-        writeCNTVOFF_EL2(reg);
-        break;
+        return writeCNTVOFF_EL2(reg);
     case seL4_VCPUReg_CNTKCTL_EL1:
-        writeCNTKCTL_EL1(reg);
-        break;
+        return writeCNTKCTL_EL1(reg);
+#ifdef ENABLE_SMP_SUPPORT
     case seL4_VCPUReg_VMPIDR_EL2:
-        writeVMPIDR_EL2(reg);
-        break;
+        return writeVMPIDR_EL2(reg);
+#endif /* ENABLE_SMP_SUPPORT */
     default:
         fail("ARM/HYP: Invalid register index");
     }
+
+    return;
 }
 
-/** DONT_TRANSLATE */
 static inline void vcpu_init_vtcr(void)
 {
 

 
diff --git a/include/arch/riscv/arch/64/mode/hardware.h b/include/arch/riscv/arch/64/mode/hardware.h
index 9f0882dc..d76cdab3 100644
--- a/include/arch/riscv/arch/64/mode/hardware.h
+++ b/include/arch/riscv/arch/64/mode/hardware.h
@@ -70,7 +70,7 @@
  *    = sign extension bit for canonical addresses
  *    (= 47 on x64, 38 on RISCV64 sv39, 47 on RISCV64 sv48)
  *  b = The number of bits used by kernel mapping.
- *    = 38 (half of the 1 level page table) on RISCV64 sv39
+ *    = 38 (half of the 1 level page table) on RISCV64 sc39
  *    = 39 (entire second level page table) on aarch64 / X64 / sv48
  */
 
diff --git a/include/arch/riscv/arch/64/mode/machine.h b/include/arch/riscv/arch/64/mode/machine.h
index be4d5dc9..2c2502db 100644
--- a/include/arch/riscv/arch/64/mode/machine.h
+++ b/include/arch/riscv/arch/64/mode/machine.h
@@ -9,24 +9,11 @@
 #include <util.h>
 #include <arch/model/smp.h>
 #include <stdint.h>
-#include <plat/machine/devices_gen.h>
-
-#ifdef CONFIG_RISCV_USE_CLINT_MTIME
-/*
- * Currently all RISC-V 64-bit platforms supported have the mtime register
- * mapped at the same offset of the base address of the CLINT.
- */
-#define CLINT_MTIME_OFFSET 0xbff8
-#endif
 
 static inline uint64_t riscv_read_time(void)
 {
     word_t n;
-#ifdef CONFIG_RISCV_USE_CLINT_MTIME
-    n = *(volatile word_t *)(CLINT_PPTR + CLINT_MTIME_OFFSET);
-#else
     asm volatile("rdtime %0" : "=r"(n));
-#endif
     return n;
 }
 
diff --git a/include/arch/riscv/arch/fastpath/fastpath.h b/include/arch/riscv/arch/fastpath/fastpath.h
index 17ca48a8..83128101 100644
--- a/include/arch/riscv/arch/fastpath/fastpath.h
+++ b/include/arch/riscv/arch/fastpath/fastpath.h
@@ -101,7 +101,8 @@ static inline void NORETURN FORCE_INLINE fastpath_restore(word_t badge, word_t m
     word_t cur_thread_regs = (word_t)cur_thread->tcbArch.tcbContext.registers;
 
 #ifdef ENABLE_SMP_SUPPORT
-    word_t sp = read_sscratch();
+    word_t sp;
+    asm volatile("csrr %0, sscratch" : "=r"(sp));
     sp -= sizeof(word_t);
     *((word_t *)sp) = cur_thread_regs;
 #endif
@@ -121,11 +122,11 @@ static inline void NORETURN FORCE_INLINE fastpath_restore(word_t badge, word_t m
         LOAD_S "  ra, (0*%[REGSIZE])(t0)  \n"
         LOAD_S "  sp, (1*%[REGSIZE])(t0)  \n"
         LOAD_S "  gp, (2*%[REGSIZE])(t0)  \n"
-        /* skip tp/x4, t0/x5, t1/x6, they are restored later */
+        /* skip tp */
+        /* skip x5/t0 */
         LOAD_S "  t2, (6*%[REGSIZE])(t0)  \n"
         LOAD_S "  s0, (7*%[REGSIZE])(t0)  \n"
         LOAD_S "  s1, (8*%[REGSIZE])(t0)  \n"
-        /* skip a0/x10, a1/x11, they have been restored already */
         LOAD_S "  a2, (11*%[REGSIZE])(t0) \n"
         LOAD_S "  a3, (12*%[REGSIZE])(t0) \n"
         LOAD_S "  a4, (13*%[REGSIZE])(t0) \n"
diff --git a/include/arch/riscv/arch/kernel/traps.h b/include/arch/riscv/arch/kernel/traps.h
index 493cca45..65c7aace 100644
--- a/include/arch/riscv/arch/kernel/traps.h
+++ b/include/arch/riscv/arch/kernel/traps.h
@@ -20,29 +20,27 @@ static inline void arch_c_exit_hook(void)
     /* Nothing architecture specific to be done. */
 }
 
-/* ASM trap entry. */
-void NORETURN trap_entry(void);
-
 #ifdef CONFIG_KERNEL_MCS
 void c_handle_fastpath_reply_recv(word_t cptr, word_t msgInfo, word_t reply)
 #else
 void c_handle_fastpath_reply_recv(word_t cptr, word_t msgInfo)
 #endif
-VISIBLE NORETURN SECTION(".text.fastpath");
+VISIBLE NORETURN;
 
 void c_handle_fastpath_call(word_t cptr, word_t msgInfo)
-VISIBLE NORETURN SECTION(".text.fastpath");
+VISIBLE NORETURN;
 
 void c_handle_syscall(word_t cptr, word_t msgInfo, syscall_t syscall)
-VISIBLE NORETURN SECTION(".text.traps");
+VISIBLE NORETURN;
 
 void c_handle_interrupt(void)
-VISIBLE NORETURN SECTION(".text.traps");
+VISIBLE NORETURN;
 
 void c_handle_exception(void)
-VISIBLE NORETURN SECTION(".text.traps");
+VISIBLE NORETURN;
 
 void restore_user_context(void)
-VISIBLE NORETURN SECTION(".text.traps");
+VISIBLE NORETURN;
 
 void handle_exception(void);
+
diff --git a/include/arch/riscv/arch/machine.h b/include/arch/riscv/arch/machine.h
index 568d35e3..43850247 100644
--- a/include/arch/riscv/arch/machine.h
+++ b/include/arch/riscv/arch/machine.h
@@ -81,6 +81,11 @@ static inline void fence_r_rw(void)
     asm volatile("fence r,rw" ::: "memory");
 }
 
+static inline void fence_w_r(void)
+{
+    asm volatile("fence w,r" ::: "memory");
+}
+
 static inline void ifence_local(void)
 {
     asm volatile("fence.i":::"memory");
@@ -221,13 +226,6 @@ static inline void clear_sie_mask(word_t mask_low)
     asm volatile("csrrc %0, sie, %1" : "=r"(temp) : "rK"(mask_low));
 }
 
-static inline word_t read_sscratch(void)
-{
-    word_t temp;
-    asm volatile("csrr %0, sscratch" : "=r"(temp));
-    return temp;
-}
-
 #ifdef CONFIG_HAVE_FPU
 static inline uint32_t read_fcsr(void)
 {
diff --git a/include/arch/riscv/arch/machine/timer.h b/include/arch/riscv/arch/machine/timer.h
index eb287c0c..de473b6a 100644
--- a/include/arch/riscv/arch/machine/timer.h
+++ b/include/arch/riscv/arch/machine/timer.h
@@ -37,17 +37,11 @@ static inline PURE ticks_t getTimerPrecision(void)
     return usToTicks(1);
 }
 
-/* Get the max. ticks_t value that can be expressed in time_t (time in us). This
- * is the max. value ticksToUs() can be passed without overflowing.
- */
 static inline CONST ticks_t getMaxTicksToUs(void)
 {
     return UINT64_MAX;
 }
 
-/* Get the max. time_t value (time in us) that can be expressed in ticks_t. This
- * is the max. value usToTicks() can be passed without overflowing.
- */
 static inline CONST time_t getMaxUsToTicks(void)
 {
     return UINT64_MAX / TICKS_IN_US;
@@ -62,6 +56,7 @@ static inline ticks_t getCurrentTime(void)
 /* set the next deadline irq - deadline is absolute */
 static inline void setDeadline(ticks_t deadline)
 {
+    assert(deadline > NODE_STATE(ksCurTime));
     /* Setting the timer acknowledges any existing IRQs */
     sbi_set_timer(deadline);
 }


diff --git a/include/assert.h b/include/assert.h
index 579b8810..0f3a9032 100644
--- a/include/assert.h
+++ b/include/assert.h
@@ -30,7 +30,7 @@ void _assert_fail(
 #define assert(expr) \
     do { \
         if (!(expr)) { \
-            _assert_fail(#expr, __FILE__, __LINE__, __func__); \
+            _assert_fail(#expr, __FILE__, __LINE__, __FUNCTION__); \
         } \
     } while(0)
 
diff --git a/include/bootinfo.h b/include/bootinfo.h
index 4b6fbb29..c11d1206 100644
--- a/include/bootinfo.h
+++ b/include/bootinfo.h
@@ -15,3 +15,11 @@
 #define BI_REF(p) ((word_t)(p))
 
 #define S_REG_EMPTY (seL4_SlotRegion){ .start = 0, .end = 0 }
+
+/* The boot info frame takes at least one page, it must be big enough to hold
+ * the seL4_BootInfo data structure. Due to internal restrictions, the boot info
+ * frame size must be of the form 2^n. Furthermore, there might still be code
+ * that makes the hard-coded assumption the boot info frame is always one page.
+ */
+#define BI_FRAME_SIZE_BITS PAGE_BITS
+compile_assert(bi_size, sizeof(seL4_BootInfo) <= BIT(BI_FRAME_SIZE_BITS))
diff --git a/include/drivers/irq/riscv_plic0.h b/include/drivers/irq/riscv_plic0.h
index 0f5ab066..d379ca0c 100644
--- a/include/drivers/irq/riscv_plic0.h
+++ b/include/drivers/irq/riscv_plic0.h
@@ -5,7 +5,7 @@
  * SPDX-License-Identifier: GPL-2.0-only
  *
  * SiFive U54/U74 PLIC handling (HiFive Unleashed/Unmatched, Polarfire,
- * QEMU RISC-V virt, Star64)
+ * QEMU RISC-V virt)
  */
 
 #pragma once
@@ -15,8 +15,7 @@
 #if !defined(CONFIG_PLAT_HIFIVE) && \
     !defined(CONFIG_PLAT_POLARFIRE) && \
     !defined(CONFIG_PLAT_QEMU_RISCV_VIRT) && \
-    !defined(CONFIG_PLAT_ROCKETCHIP_ZCU102) && \
-    !defined(CONFIG_PLAT_STAR64)
+    !defined(CONFIG_PLAT_ROCKETCHIP_ZCU102)
 #error "Check if this platform suppots a PLIC."
 #endif
 
@@ -53,12 +52,11 @@
 
 #define PLIC_NUM_INTERRUPTS PLIC_MAX_IRQ
 
-#if defined(CONFIG_PLAT_HIFIVE) || \
-    defined(CONFIG_PLAT_POLARFIRE) || \
-    defined(CONFIG_PLAT_STAR64)
+#if defined(CONFIG_PLAT_HIFIVE) || defined(CONFIG_PLAT_POLARFIRE)
 
-/* SiFive U54-MC and U74-MC have 5 cores, and the first core does not have
- * supervisor mode. Therefore, we need to compensate for the addresses.
+/* SiFive U54-MC has 5 cores, and the first core does not
+ * have supervisor mode. Therefore, we need to compensate
+ * for the addresses.
  */
 #define PLAT_PLIC_THRES_ADJUST(x) ((x) - PLIC_THRES_PER_CONTEXT)
 #define PLAT_PLIC_EN_ADJUST(x)    ((x) - PLIC_EN_PER_CONTEXT)
diff --git a/include/drivers/timer/arm_generic.h b/include/drivers/timer/arm_generic.h
index c95d7947..fb055f21 100644
--- a/include/drivers/timer/arm_generic.h
+++ b/include/drivers/timer/arm_generic.h
@@ -24,6 +24,7 @@ static inline ticks_t getCurrentTime(void)
 /** DONT_TRANSLATE **/
 static inline void setDeadline(ticks_t deadline)
 {
+    assert(deadline >= NODE_STATE(ksCurTime));
     SYSTEM_WRITE_64(CNT_CVAL, deadline);
 }
 
@@ -31,22 +32,13 @@ static inline void ackDeadlineIRQ(void)
 {
     ticks_t deadline = UINT64_MAX;
     setDeadline(deadline);
-    /* Ensure that the timer deasserts the IRQ before GIC EOIR/DIR.
-     * This is sufficient to remove the pending state from the GICR
-     * and avoid the interrupt happening twice because of the level
-     * sensitive configuration. */
-    isb();
 }
 #else /* CONFIG_KERNEL_MCS */
 #include <arch/machine/timer.h>
 static inline void resetTimer(void)
 {
     SYSTEM_WRITE_WORD(CNT_TVAL, TIMER_RELOAD);
-    /* Ensure that the timer deasserts the IRQ before GIC EOIR/DIR.
-     * This is sufficient to remove the pending state from the GICR
-     * and avoid the interrupt happening twice because of the level
-     * sensitive configuration. */
-    isb();
+    SYSTEM_WRITE_WORD(CNT_CTL, BIT(0));
 }
 #endif /* !CONFIG_KERNEL_MCS */
 
diff --git a/include/drivers/timer/arm_global.h b/include/drivers/timer/arm_global.h
index 6d780ac1..f5721a9c 100644
--- a/include/drivers/timer/arm_global.h
+++ b/include/drivers/timer/arm_global.h
@@ -70,9 +70,7 @@ static inline void setDeadline(ticks_t deadline)
     globalTimer->comparatorUpper = (uint32_t)(deadline >> 32llu);
     /* enable cmp */
     globalTimer->control |= BIT(COMP_ENABLE);
-    /* Assert that either the deadline is in the future or that the IRQ has already been raised.
-       This should be guaranteed by hardware from r2p0 on of the Cortex-A9 MPCore Technical
-       Reference Manual (r2p0 published in 2009) */
+    /* if this fails PRECISION is too low */
     assert(getCurrentTime() < deadline || globalTimer->isr == 1u);
 }
 
diff --git a/include/hardware.h b/include/hardware.h
index 6cfa8953..4474ff36 100644
--- a/include/hardware.h
+++ b/include/hardware.h
@@ -49,6 +49,11 @@
 extern char ki_end[1];
 #define KERNEL_ELF_TOP ((paddr_t)ki_end)
 
+/* This symbol is generated by the linker and marks the end of boot
+ * code/data in kernel image */
+extern char ki_boot_end[1];
+
+
 #endif /* __ASSEMBLER__ */
 
 #include <mode/hardware.h>
diff --git a/include/kernel/boot.h b/include/kernel/boot.h
index 480f48fe..25f2c3fb 100644
--- a/include/kernel/boot.h
+++ b/include/kernel/boot.h
@@ -39,8 +39,6 @@ static inline bool_t is_reg_empty(region_t reg)
     return reg.start == reg.end;
 }
 
-p_region_t get_p_reg_kernel_img_boot(void);
-p_region_t get_p_reg_kernel_img(void);
 bool_t init_freemem(word_t n_available, const p_region_t *available,
                     word_t n_reserved, const region_t *reserved,
                     v_region_t it_v_reg, word_t extra_bi_size_bits);
@@ -51,7 +49,7 @@ bool_t provide_cap(cap_t root_cnode_cap, cap_t cap);
 cap_t create_it_asid_pool(cap_t root_cnode_cap);
 void write_it_pd_pts(cap_t root_cnode_cap, cap_t it_pd_cap);
 void create_idle_thread(void);
-bool_t create_untypeds(cap_t root_cnode_cap);
+bool_t create_untypeds(cap_t root_cnode_cap, region_t boot_mem_reuse_reg);
 void bi_finalise(void);
 void create_domain_cap(cap_t root_cnode_cap);
 
@@ -138,7 +136,7 @@ static inline BOOT_CODE pptr_t it_alloc_paging(void)
 /* return the amount of paging structures required to cover v_reg */
 word_t arch_get_n_paging(v_region_t it_veg);
 
-#if defined(CONFIG_DEBUG_BUILD) && defined(ENABLE_SMP_SUPPORT) && defined(CONFIG_KERNEL_MCS) && !defined(CONFIG_PLAT_QEMU_ARM_VIRT)
+#if defined(CONFIG_DEBUG_BUILD) && defined(ENABLE_SMP_SUPPORT) && defined(CONFIG_KERNEL_MCS)
 /* Test whether clocks are synchronised across nodes */
 #define ENABLE_SMP_CLOCK_SYNC_TEST_ON_BOOT
 #endif
diff --git a/include/kernel/sporadic.h b/include/kernel/sporadic.h
index 35518b7b..ddde253c 100644
--- a/include/kernel/sporadic.h
+++ b/include/kernel/sporadic.h
@@ -101,12 +101,11 @@ static inline bool_t refill_single(sched_context_t *sc)
  * has available if usage is charged to it. */
 static inline ticks_t refill_capacity(sched_context_t *sc, ticks_t usage)
 {
-    ticks_t head_amount = refill_head(sc)->rAmount;
-    if (unlikely(usage > head_amount)) {
+    if (unlikely(usage > refill_head(sc)->rAmount)) {
         return 0;
     }
 
-    return head_amount - usage;
+    return refill_head(sc)->rAmount - usage;
 }
 
 /*
@@ -126,7 +125,7 @@ static inline bool_t refill_sufficient(sched_context_t *sc, ticks_t usage)
  */
 static inline bool_t refill_ready(sched_context_t *sc)
 {
-    return refill_head(sc)->rTime <= NODE_STATE(ksCurTime);
+    return refill_head(sc)->rTime <= (NODE_STATE(ksCurTime) + getKernelWcetTicks());
 }
 
 /*
@@ -160,10 +159,7 @@ static inline bool_t sc_released(sched_context_t *sc)
  */
 static inline bool_t sc_sporadic(sched_context_t *sc)
 {
-    /* asserting sc != NULL --> sc->scSporadic --> sc_active(sc). That means, when
-       this function returns true, we also know that sc_active(sc) is true */
-    assert(sc == NULL || !sc->scSporadic || sc_active(sc));
-    return sc != NULL && sc->scSporadic;
+    return sc != NULL && sc_active(sc) && sc->scSporadic;
 }
 
 /*
diff --git a/include/kernel/thread.h b/include/kernel/thread.h
index 55ae8b3d..22eec986 100644
--- a/include/kernel/thread.h
+++ b/include/kernel/thread.h
@@ -153,7 +153,7 @@ static inline bool_t PURE isSchedulable(const tcb_t *thread)
 {
     return isRunnable(thread) &&
            thread->tcbSchedContext != NULL &&
-           sc_active(thread->tcbSchedContext) &&
+           thread->tcbSchedContext->scRefillMax > 0 &&
            !thread_state_get_tcbInReleaseQueue(thread->tcbState);
 }
 #else
@@ -230,10 +230,10 @@ void chargeBudget(ticks_t consumed, bool_t canTimeoutFault);
  */
 static inline void updateTimestamp(void)
 {
-    ticks_t prev = NODE_STATE(ksCurTime);
+    time_t prev = NODE_STATE(ksCurTime);
     NODE_STATE(ksCurTime) = getCurrentTime();
     assert(NODE_STATE(ksCurTime) < MAX_RELEASE_TIME);
-    ticks_t consumed = (NODE_STATE(ksCurTime) - prev);
+    time_t consumed = (NODE_STATE(ksCurTime) - prev);
     NODE_STATE(ksConsumed) += consumed;
     if (numDomains > 1) {
         if ((consumed + MIN_BUDGET) >= ksDomainTime) {
diff --git a/include/machine/interrupt.h b/include/machine/interrupt.h
index a3b71749..4746ddb1 100644
--- a/include/machine/interrupt.h
+++ b/include/machine/interrupt.h
@@ -80,7 +80,7 @@ static inline bool_t isIRQPending(void);
  * When an IRQ is disabled, it should not raise an interrupt on the processor.
  *
  * @param[in]  disable  True to disable IRQ, False to enable IRQ
- * @param[in]  irq      The IRQ to modify
+ * @param[in]  irq      The irq to modify
  */
 static inline void maskInterrupt(bool_t disable, irq_t irq);
 
@@ -93,7 +93,7 @@ static inline void maskInterrupt(bool_t disable, irq_t irq);
  * but before user level has handled the cause and does not imply that the cause
  * of the interrupt has been handled.
  *
- * @param[in]  irq   IRQ to ack
+ * @param[in]  irq   irq to ack
  */
 static inline void ackInterrupt(irq_t irq);
 
diff --git a/include/machine/timer.h b/include/machine/timer.h
index 70f412b6..047534fe 100644
--- a/include/machine/timer.h
+++ b/include/machine/timer.h
@@ -16,9 +16,7 @@
 /* Read the current time from the timer. */
 /** MODIFIES: [*] */
 static inline ticks_t getCurrentTime(void);
-/* Set the next deadline irq - deadline is absolute and may be slightly in
-   the past. If it is set in the past, we expect an interrupt to be raised
-   immediately after we leave the kernel. */
+/* set the next deadline irq - deadline is absolute */
 /** MODIFIES: [*] */
 static inline void setDeadline(ticks_t deadline);
 /* ack previous deadline irq */
diff --git a/include/model/statedata.h b/include/model/statedata.h
index 7f40545a..3f86b8ba 100644
--- a/include/model/statedata.h
+++ b/include/model/statedata.h
@@ -64,9 +64,9 @@ NODE_STATE_DECLARE(tcb_t, *ksIdleThread);
 NODE_STATE_DECLARE(tcb_t, *ksSchedulerAction);
 
 #ifdef CONFIG_KERNEL_MCS
-NODE_STATE_DECLARE(tcb_queue_t, ksReleaseQueue);
-NODE_STATE_DECLARE(ticks_t, ksConsumed);
-NODE_STATE_DECLARE(ticks_t, ksCurTime);
+NODE_STATE_DECLARE(tcb_t, *ksReleaseHead);
+NODE_STATE_DECLARE(time_t, ksConsumed);
+NODE_STATE_DECLARE(time_t, ksCurTime);
 NODE_STATE_DECLARE(bool_t, ksReprogram);
 NODE_STATE_DECLARE(sched_context_t, *ksCurSC);
 NODE_STATE_DECLARE(sched_context_t, *ksIdleSC);
diff --git a/include/object/schedcontext.h b/include/object/schedcontext.h
index 3f2a0948..719d61b6 100644
--- a/include/object/schedcontext.h
+++ b/include/object/schedcontext.h
@@ -23,16 +23,17 @@ exception_t decodeSchedContextInvocation(word_t label, cap_t cap, word_t *buffer
  */
 void schedContext_bindTCB(sched_context_t *sc, tcb_t *tcb);
 
-/* Unbind the tcb from a scheduling context. If the tcb is runnable,
+/* Unbind a specific tcb from a scheduling context. If the tcb is runnable,
  * remove from the scheduler.
  *
- * @param sc the scheduling context whose tcb will be unbound
+ * @param sc  scheduling context to unbind
+ * @param tcb the tcb to unbind
  *
- * @pre   the sc is not NULL and is bound to a tcb
- *        (sc != NULL && sc->scTcb != NULL);
- * @post  (sc->scTcb == NULL && sc->scTcb->tcbSchedContext == NULL)
+ * @pre   the tcb is bound to the sc,
+ *        (sc->scTcb == tcb && tcb->tcbSchedContext == sc);
+ * @post  (tcb->tcbSchedContext == NULL && sc->scTcb == NULL)
  */
-void schedContext_unbindTCB(sched_context_t *sc);
+void schedContext_unbindTCB(sched_context_t *sc, tcb_t *tcb);
 
 /*
  * Unbind any tcb from a scheduling context. If the tcb bound to the scheduling
diff --git a/include/object/tcb.h b/include/object/tcb.h
index 24ef46c6..da34d6bd 100644
--- a/include/object/tcb.h
+++ b/include/object/tcb.h
@@ -44,57 +44,6 @@ static inline unsigned int setMR(tcb_t *receiver, word_t *receiveIPCBuffer,
 void tcbSchedEnqueue(tcb_t *tcb);
 void tcbSchedAppend(tcb_t *tcb);
 void tcbSchedDequeue(tcb_t *tcb);
-tcb_queue_t tcb_queue_remove(tcb_queue_t queue, tcb_t *tcb);
-
-static inline bool_t PURE tcb_queue_empty(tcb_queue_t queue)
-{
-    return queue.head == NULL;
-}
-
-static inline tcb_queue_t tcb_queue_prepend(tcb_queue_t queue, tcb_t *tcb)
-{
-    if (tcb_queue_empty(queue)) {
-        queue.end = tcb;
-    } else {
-        tcb->tcbSchedNext = queue.head;
-        queue.head->tcbSchedPrev = tcb;
-    }
-
-    queue.head = tcb;
-
-    return queue;
-}
-
-static inline tcb_queue_t tcb_queue_append(tcb_queue_t queue, tcb_t *tcb)
-{
-    if (tcb_queue_empty(queue)) {
-        queue.head = tcb;
-    } else {
-        tcb->tcbSchedPrev = queue.end;
-        queue.end->tcbSchedNext = tcb;
-    }
-
-    queue.end = tcb;
-
-    return queue;
-}
-
-/* Insert a TCB into a queue immediately before another item in the queue
-   (the queue must initially contain at least two items) */
-static inline void tcb_queue_insert(tcb_t *tcb, tcb_t *after)
-{
-    tcb_t *before;
-    before = after->tcbSchedPrev;
-
-    assert(before != NULL);
-    assert(before != after);
-
-    tcb->tcbSchedPrev = before;
-    tcb->tcbSchedNext = after;
-
-    after->tcbSchedPrev = tcb;
-    before->tcbSchedNext = tcb;
-}
 
 #ifdef CONFIG_DEBUG_BUILD
 void tcbDebugAppend(tcb_t *tcb);
@@ -103,6 +52,7 @@ void tcbDebugRemove(tcb_t *tcb);
 #ifdef CONFIG_KERNEL_MCS
 void tcbReleaseRemove(tcb_t *tcb);
 void tcbReleaseEnqueue(tcb_t *tcb);
+tcb_t *tcbReleaseDequeue(void);
 #endif
 
 #ifdef ENABLE_SMP_SUPPORT
@@ -273,6 +223,10 @@ word_t CONST Arch_decodeTransfer(word_t flags);
 exception_t CONST Arch_performTransfer(word_t arch, tcb_t *tcb_src,
                                        tcb_t *tcb_dest);
 
+#ifdef ENABLE_SMP_SUPPORT
+void Arch_migrateTCB(tcb_t *thread);
+#endif /* ENABLE_SMP_SUPPORT */
+
 #ifdef CONFIG_DEBUG_BUILD
 void setThreadName(tcb_t *thread, const char *name);
 #endif /* CONFIG_DEBUG_BUILD */


 
diff --git a/libsel4/include/sel4/bootinfo_types.h b/libsel4/include/sel4/bootinfo_types.h
index 66c97aa5..a7845634 100644
--- a/libsel4/include/sel4/bootinfo_types.h
+++ b/libsel4/include/sel4/bootinfo_types.h
@@ -6,12 +6,11 @@
 
 #pragma once
 
-#include <sel4/config.h>
+#include <autoconf.h>
 #include <sel4/macros.h>
-#include <sel4/sel4_arch/constants.h>
 
 /* caps with fixed slot positions in the root CNode */
-enum seL4_RootCNodeCapSlots {
+enum {
     seL4_CapNull                =  0, /* null cap */
     seL4_CapInitThreadTCB       =  1, /* initial thread's TCB cap */
     seL4_CapInitThreadCNode     =  2, /* initial thread's root CNode cap */
@@ -24,11 +23,14 @@ enum seL4_RootCNodeCapSlots {
     seL4_CapBootInfoFrame       =  9, /* bootinfo frame cap */
     seL4_CapInitThreadIPCBuffer = 10, /* initial thread's IPC buffer frame cap */
     seL4_CapDomain              = 11, /* global domain controller cap */
-    seL4_CapSMMUSIDControl      = 12, /* global SMMU SID controller cap, null cap if not supported */
-    seL4_CapSMMUCBControl       = 13, /* global SMMU CB controller cap, null cap if not supported */
-    seL4_CapInitThreadSC        = 14, /* initial thread's scheduling context cap, null cap if not supported */
-    seL4_CapSMC                 = 15, /* global SMC cap, null cap if not supported */
-    seL4_NumInitialCaps         = 16
+    seL4_CapSMMUSIDControl      = 12,  /*global SMMU SID controller cap, null cap if not supported*/
+    seL4_CapSMMUCBControl       = 13,  /*global SMMU CB controller cap, null cap if not supported*/
+#ifdef CONFIG_KERNEL_MCS
+    seL4_CapInitThreadSC        = 14, /* initial thread's scheduling context cap */
+    seL4_NumInitialCaps         = 15
+#else
+    seL4_NumInitialCaps         = 14
+#endif /* !CONFIG_KERNEL_MCS */
 };
 
 /* Legacy code will have assumptions on the vspace root being a Page Directory
@@ -77,27 +79,12 @@ typedef struct seL4_BootInfo {
      * to make this struct easier to represent in other languages */
 } seL4_BootInfo;
 
-/* The boot info frame must be large enough to hold the seL4_BootInfo data
- * structure. Due to internal restrictions, the size must be of the form 2^n and
- * the minimum is one page.
- */
-#define seL4_BootInfoFrameBits  seL4_PageBits
-#define seL4_BootInfoFrameSize  LIBSEL4_BIT(seL4_BootInfoFrameBits)
-
-SEL4_COMPILE_ASSERT(
-    invalid_seL4_BootInfoFrameSize,
-    sizeof(seL4_BootInfo) <= seL4_BootInfoFrameSize)
-
-/* If seL4_BootInfo.extraLen > 0, this indicate the presence of additional boot
- * information chunks starting at the offset seL4_BootInfoFrameSize. Userland
- * code often contains the hard-coded assumption that the offset is 4 KiByte,
- * because the boot info frame usually is one page, which is 4 KiByte on x86,
- * Arm and RISC-V.
- * The additional boot info chunks are arch/platform specific, they may or may
- * not exist in any given execution. Each chunk has a header that contains an ID
- * to describe the chunk. All IDs share a global namespace to ensure uniqueness.
+/* If extraLen > 0, then 4K after the start of bootinfo there is a region of the
+ * size extraLen that contains additional boot info data chunks. They are
+ * arch/platform specific and may or may not exist in any given execution. Each
+ * chunk has a header that contains an ID to describe the chunk. All IDs share a
+ * global namespace to ensure uniqueness.
  */
-
 typedef enum {
     SEL4_BOOTINFO_HEADER_PADDING            = 0,
     SEL4_BOOTINFO_HEADER_X86_VBE            = 1,
diff --git a/libsel4/include/sel4/constants.h b/libsel4/include/sel4/constants.h
index c5ec6978..29b1157b 100644
--- a/libsel4/include/sel4/constants.h
+++ b/libsel4/include/sel4/constants.h
@@ -6,7 +6,7 @@
 
 #pragma once
 
-#include <sel4/config.h>
+#include <autoconf.h>
 #include <sel4/macros.h>
 
 #ifndef __ASSEMBLER__
@@ -51,10 +51,12 @@ enum priorityConstants {
 
 enum seL4_MsgLimits {
     seL4_MsgLengthBits = 7,
-    seL4_MsgExtraCapBits = 2,
-    seL4_MsgMaxLength = 120
+    seL4_MsgExtraCapBits = 2
 };
 
+enum {
+    seL4_MsgMaxLength = 120,
+};
 #define seL4_MsgMaxExtraCaps (LIBSEL4_BIT(seL4_MsgExtraCapBits)-1)
 
 /* seL4_CapRights_t defined in shared_types_*.bf */
 
 
diff --git a/libsel4/include/sel4/syscalls.h b/libsel4/include/sel4/syscalls.h
index 9125dd6b..ade4432b 100644
--- a/libsel4/include/sel4/syscalls.h
+++ b/libsel4/include/sel4/syscalls.h
@@ -5,7 +5,7 @@
  */
 
 #pragma once
-#include <sel4/config.h>
+#include <autoconf.h>
 
 #include <sel4/functions.h>
 
@@ -44,7 +44,7 @@ LIBSEL4_INLINE_FUNC void
 seL4_DebugPutChar(char c);
 
 /**
- * @xmlonly <manual name="Dump Scheduler" label="sel4_dumpscheduler"/> @endxmlonly
+ * @xmlonly <manual name="Dump scheduler" label="sel4_dumpscheduler"/> @endxmlonly
  * @brief Output the contents of the kernel scheduler.
  *
  * Dump the state of the all TCB objects to kernel serial output. This system call
@@ -87,14 +87,14 @@ seL4_DebugSnapshot(void);
 
 /**
  * @xmlonly <manual name="Cap Identify" label="sel4_debugcapidentify"/> @endxmlonly
- * @brief Identify the type of a capability in the current CSpace.
+ * @brief Identify the type of a capability in the current cspace.
  *
  * This debugging system call returns the type of capability in a capability
- * slot in the current CSpace. The type returned is not a libsel4 type, but
+ * slot in the current cspace. The type returned is not a libsel4 type, but
  * refers to an internal seL4 type. This can be looked up in a built kernel by
  * looking for the (generated) `enum cap_tag`, type `cap_tag_t`.
  *
- * @param cap A capability slot in the current CSpace.
+ * @param cap A capability slot in the current cspace.
  * @return The type of capability passed in.
  *
  */
@@ -118,7 +118,7 @@ seL4_DebugCapIdentify(seL4_CPtr cap);
  */
 LIBSEL4_INLINE_FUNC void
 seL4_DebugNameThread(seL4_CPtr tcb, const char *name);
-#if defined(CONFIG_ENABLE_SMP_SUPPORT) && defined(CONFIG_ARCH_ARM)
+#if CONFIG_MAX_NUM_NODES > 1 && defined CONFIG_ARCH_ARM
 /**
  * @xmlonly <manual name="Send SGI 0-15" label="sel4_debugsendipi"/> @endxmlonly
  * @brief Sends arbitrary SGI.
@@ -215,7 +215,7 @@ seL4_BenchmarkFinalizeLog(void);
  * @brief Set log buffer.
  *
  * Provide a large frame object for the kernel to use as a log-buffer.
- * The object must not be device memory, and must be `seL4_LargePageBits` in size.
+ * The object must not be device memory, and must be seL4_LargePageBits in size.
  *
  * @param[in] frame_cptr A capability pointer to a user allocated frame of seL4_LargePage size.
  * @return A `seL4_IllegalOperation` error if `frame_cptr` is not valid and couldn't set the buffer.
@@ -317,7 +317,7 @@ seL4_BenchmarkResetAllThreadsUtilisation(void);
 
 #ifdef CONFIG_VTX
 /**
- * @xmlonly <manual name="VM Enter" label="sel4_vmenter"/> @endxmlonly
+ * @xmlonly <manual name="VMEnter" label="sel4_vmenter"/> @endxmlonly
  * @brief Change current thread to execute from its bound VCPU
  *
  * Changes the execution mode of the current thread from normal TCB execution, to
@@ -338,7 +338,7 @@ seL4_BenchmarkResetAllThreadsUtilisation(void);
  *      - `SEL4_VMENTER_CALL_CONTROL_ENTRY_MR` New value for the VM Entry Controls
  *
  * On return these same three message registers will be filled with the values at the point
- * that the privileged mode ceased executing. If this function returns with `SEL4_VMENTER_RESULT_FAULT`
+ * that the privlidged mode ceased executing. If this function returns with `SEL4_VMENTER_RESULT_FAULT`
  * then the following additional message registers will be filled out
  *      - `SEL4_VMENTER_FAULT_REASON_MR`
  *      - `SEL4_VMENTER_FAULT_QUALIFICATION_MR`
@@ -374,7 +374,7 @@ seL4_VMEnter(seL4_Word *sender);
 
 #ifdef CONFIG_SET_TLS_BASE_SELF
 /**
- * @xmlonly <manual name="Set TLS Base" label="sel4_settlsbase"/> @endxmlonly
+ * @xmlonly <manual name="SetTLSBase" label="sel4_settlsbase"/> @endxmlonly
  * @brief Set the TLS base address and register of the currently executing thread.
  *
  * This stores the base address of the TLS region in the register
diff --git a/libsel4/include/sel4/syscalls_master.h b/libsel4/include/sel4/syscalls_master.h
index ffb16f13..56986d2b 100644
--- a/libsel4/include/sel4/syscalls_master.h
+++ b/libsel4/include/sel4/syscalls_master.h
@@ -5,7 +5,7 @@
  */
 
 #pragma once
-#include <sel4/config.h>
+#include <autoconf.h>
 
 /**
  * @defgroup GeneralSystemCalls System Calls (non-MCS)
@@ -125,7 +125,7 @@ LIBSEL4_INLINE_FUNC seL4_MessageInfo_t
 seL4_ReplyRecv(seL4_CPtr dest, seL4_MessageInfo_t msgInfo, seL4_Word *sender);
 
 /**
- * @xmlonly <manual name="Non-Blocking Recv" label="sel4_nbrecv"/> @endxmlonly
+ * @xmlonly <manual name="NBRecv" label="sel4_nbrecv"/> @endxmlonly
  * @brief Receive a message from an endpoint but do not block
  *        in the case that no messages are pending
  *
@@ -201,7 +201,7 @@ seL4_Wait(seL4_CPtr src, seL4_Word *sender);
 
 /**
  * @xmlonly <manual name="Poll" label="sel4_poll"/> @endxmlonly
- * @brief Perform a non-blocking receive on a notification object
+ * @brief Perform a non-blocking recv on a notification object
  *
  * This is not a proper system call known by the kernel. Rather, it is a
  * convenience wrapper which calls seL4_NBRecv().
diff --git a/libsel4/include/sel4/syscalls_mcs.h b/libsel4/include/sel4/syscalls_mcs.h
index 5602de5f..80808b75 100644
--- a/libsel4/include/sel4/syscalls_mcs.h
+++ b/libsel4/include/sel4/syscalls_mcs.h
@@ -5,7 +5,7 @@
  */
 
 #pragma once
-#include <sel4/config.h>
+#include <autoconf.h>
 
 /**
  * @defgroup MCSSystemCalls MCS System Calls
@@ -104,7 +104,7 @@ seL4_NBSend(seL4_CPtr dest, seL4_MessageInfo_t msgInfo);
  *               This parameter is ignored if `NULL`.
  *
  * @param[in] reply The capability to the reply object, which is first invoked and then used
- *                  for the receive phase to store a new reply capability.
+ *                  for the recv phase to store a new reply capability.
  * @return A `seL4_MessageInfo_t` structure
  * @xmlonly
  * <docref>as described in <autoref label="sec:messageinfo"/></docref>
@@ -114,7 +114,7 @@ LIBSEL4_INLINE_FUNC seL4_MessageInfo_t
 seL4_ReplyRecv(seL4_CPtr src, seL4_MessageInfo_t msgInfo, seL4_Word *sender, seL4_CPtr reply);
 
 /**
- * @xmlonly <manual name="Non-Blocking Recv" label="sel4_mcs_nbrecv"/> @endxmlonly
+ * @xmlonly <manual name="NBRecv" label="sel4_mcs_nbrecv"/> @endxmlonly
  * @brief Receive a message from an endpoint but do not block
  *        in the case that no messages are pending
  *
@@ -140,8 +140,8 @@ LIBSEL4_INLINE_FUNC seL4_MessageInfo_t
 seL4_NBRecv(seL4_CPtr src, seL4_Word *sender, seL4_CPtr reply);
 
 /**
- * @xmlonly <manual name="Non-Blocking Send Recv" label="sel4_nbsendrecv"/> @endxmlonly
- * @brief Non-blocking send on one capability, and a blocking receive on another in a single
+ * @xmlonly <manual name="NBSend Recv" label="sel4_nbsendrecv"/> @endxmlonly
+ * @brief Non-blocking send on one capability, and a blocking recieve on another in a single
  *        system call.
  *
  * @xmlonly
@@ -158,7 +158,7 @@ seL4_NBRecv(seL4_CPtr src, seL4_Word *sender, seL4_CPtr reply);
  *               This parameter is ignored if `NULL`.
  * @param[in] src The capability to receive on.
  * @param[in] reply The capability to the reply object, which is first invoked and then used
- *                  for the receive phase to store a new reply capability.
+ *                  for the recv phase to store a new reply capability.
  * @return A `seL4_MessageInfo_t` structure
  * @xmlonly
  * as described in <autoref label="sec:messageinfo"/>
@@ -168,7 +168,7 @@ LIBSEL4_INLINE_FUNC seL4_MessageInfo_t
 seL4_NBSendRecv(seL4_CPtr dest, seL4_MessageInfo_t msgInfo, seL4_CPtr src, seL4_Word *sender, seL4_CPtr reply);
 
 /**
- * @xmlonly <manual name="Non-Blocking Send Wait" label="sel4_nbsendwait"/> @endxmlonly
+ * @xmlonly <manual name="NBSend Wait" label="sel4_nbsendwait"/> @endxmlonly
  * @brief Non-blocking invoke of a capability and wait on another in one system call
  *
  * @xmlonly
@@ -210,7 +210,7 @@ seL4_Yield(void);
  *
  * Block on a notification or endpoint waiting for a message. No reply object is
  * required for a Wait. Wait should not be paired with Call, as it does not provide
- * a reply object. If Wait is paired with a Call the waiter will block after receiving
+ * a reply object. If Wait is paired with a Call the waiter will block after recieving
  * the message.
  *
  * @xmlonly
@@ -224,16 +224,12 @@ seL4_Yield(void);
  *               sender, or the notification word of the
  *               notification object that was signalled.
  *               This parameter is ignored if `NULL`.
- * @return A `seL4_MessageInfo_t` structure
- * @xmlonly
- * <docref>as described in <autoref label="sec:messageinfo"/></docref>
- * @endxmlonly
  */
 LIBSEL4_INLINE_FUNC seL4_MessageInfo_t
 seL4_Wait(seL4_CPtr src, seL4_Word *sender);
 
 /**
- * @xmlonly <manual name="Non-Blocking Wait" label="sel4_nbwait"/> @endxmlonly
+ * @xmlonly <manual name="NBWait" label="sel4_nbwait"/> @endxmlonly
  * @brief Perform a polling wait on an endpoint or notification object
  *
  * Poll a notification or endpoint waiting for a message. No reply object is
@@ -250,17 +246,13 @@ seL4_Wait(seL4_CPtr src, seL4_Word *sender);
  *               sender, or the notification word of the
  *               notification object that was signalled.
  *               This parameter is ignored if `NULL`.
- * @return A `seL4_MessageInfo_t` structure
- * @xmlonly
- * <docref>as described in <autoref label="sec:messageinfo"/></docref>
- * @endxmlonly
  */
 LIBSEL4_INLINE_FUNC seL4_MessageInfo_t
 seL4_NBWait(seL4_CPtr src, seL4_Word *sender);
 
 /**
  * @xmlonly <manual name="Poll" label="sel4_mcs_poll"/> @endxmlonly
- * @brief Perform a non-blocking receive on a notification object
+ * @brief Perform a non-blocking recv on a notification object
  *
  * This is not a proper system call known by the kernel. Rather, it is a
  * convenience wrapper which calls seL4_NBWait().
@@ -277,6 +269,7 @@ seL4_NBWait(seL4_CPtr src, seL4_Word *sender);
  *               sender, or the notification word of the
  *               notification object that was signalled.
  *               This parameter is ignored if `NULL`.
+ *
  * @return A `seL4_MessageInfo_t` structure
  * @xmlonly
  * <docref>as described in <autoref label="sec:messageinfo"/></docref>






 


 
 




diff --git a/libsel4/tools/sel4_idl.xsd b/libsel4/tools/sel4_idl.xsd
index 2118fb23..187a324c 100644
--- a/libsel4/tools/sel4_idl.xsd
+++ b/libsel4/tools/sel4_idl.xsd
@@ -128,6 +128,7 @@
     <xsd:complexType name="ParamType">
         <xsd:sequence>
             <xsd:element name="description" minOccurs="0" maxOccurs="1" type="DescriptionType"/>
+            <xsd:element name="error" minOccurs="0" maxOccurs="unbounded" type="ErrorType"/>
         </xsd:sequence>
         <xsd:attribute name="type" type="xsd:string" use="required"/>
         <xsd:attribute name="name" type="xsd:string" use="required"/>



diff --git a/src/arch/arm/64/head.S b/src/arch/arm/64/head.S
index 65b35d7a..c12d434e 100644
--- a/src/arch/arm/64/head.S
+++ b/src/arch/arm/64/head.S
@@ -23,14 +23,6 @@
 #define CR_ALIGN_CLEAR   0
 #endif
 
-#if !defined(CONFIG_ARM_HYPERVISOR_SUPPORT) && defined(CONFIG_AARCH64_USER_CACHE_ENABLE)
-#define CR_USER_CACHE_OPS_SET (BIT(CONTROL_UCT) | BIT(CONTROL_UCI))
-#define CR_USER_CACHE_OPS_CLEAR 0
-#else
-#define CR_USER_CACHE_OPS_SET 0
-#define CR_USER_CACHE_OPS_CLEAR (BIT(CONTROL_UCT) | BIT(CONTROL_UCI))
-#endif
-
 #ifndef CONFIG_DEBUG_DISABLE_L1_ICACHE
     #define CR_L1_ICACHE_SET   BIT(CONTROL_I)
     #define CR_L1_ICACHE_CLEAR 0
@@ -50,13 +42,11 @@
 #define CR_BITS_SET    (CR_ALIGN_SET | \
                         CR_L1_ICACHE_SET | \
                         CR_L1_DCACHE_SET | \
-                        CR_USER_CACHE_OPS_SET | \
                         BIT(CONTROL_M))
 
 #define CR_BITS_CLEAR  (CR_ALIGN_CLEAR | \
                         CR_L1_ICACHE_CLEAR | \
                         CR_L1_DCACHE_CLEAR | \
-                        CR_USER_CACHE_OPS_CLEAR | \
                         BIT(CONTROL_SA0) | \
                         BIT(CONTROL_EE) | \
                         BIT(CONTROL_E0E))
@@ -75,7 +65,7 @@
 #define SCTLR   sctlr_el1
 #endif
 
-.section .boot.text, "ax"
+.section .boot.text
 BEGIN_FUNC(_start)
     /* Save x4 and x5 so we don't clobber it */
     mov     x7, x4
diff --git a/src/arch/arm/64/kernel/vspace.c b/src/arch/arm/64/kernel/vspace.c
index aa3d565c..a81cf3bc 100644
--- a/src/arch/arm/64/kernel/vspace.c
+++ b/src/arch/arm/64/kernel/vspace.c
@@ -26,6 +26,7 @@
 #include <arch/object/iospace.h>
 #include <arch/object/vcpu.h>
 #include <arch/machine/tlb.h>
+#define RESERVED 3
 
 /*
  * Memory types are defined in Memory Attribute Indirection Register.
@@ -75,12 +76,37 @@ enum mair_s2_types {
 
 #define SMP_SHARE   3
 
+struct lookupPGDSlot_ret {
+    exception_t status;
+    pgde_t *pgdSlot;
+};
+typedef struct lookupPGDSlot_ret lookupPGDSlot_ret_t;
+
+struct lookupPUDSlot_ret {
+    exception_t status;
+    pude_t *pudSlot;
+};
+typedef struct lookupPUDSlot_ret lookupPUDSlot_ret_t;
+
+struct lookupPDSlot_ret {
+    exception_t status;
+    pde_t *pdSlot;
+};
+typedef struct lookupPDSlot_ret lookupPDSlot_ret_t;
+
 struct lookupPTSlot_ret {
+    exception_t status;
     pte_t *ptSlot;
-    word_t ptBitsLeft;
 };
 typedef struct lookupPTSlot_ret lookupPTSlot_ret_t;
 
+struct lookupFrame_ret {
+    paddr_t frameBase;
+    vm_page_size_t frameSize;
+    bool_t valid;
+};
+typedef struct lookupFrame_ret lookupFrame_ret_t;
+
 struct findVSpaceForASID_ret {
     exception_t status;
     vspace_root_t *vspace_root;
@@ -123,6 +149,14 @@ static word_t CONST APFromVMRights(vm_rights_t vm_rights)
             return 1;
         }
 
+    case VMKernelReadOnly:
+        if (config_set(CONFIG_ARM_HYPERVISOR_SUPPORT)) {
+            /* no corresponding AP for S2AP, return None */
+            return 0;
+        } else {
+            return 2;
+        }
+
     case VMReadOnly:
         if (config_set(CONFIG_ARM_HYPERVISOR_SUPPORT)) {
             return 1;
@@ -135,39 +169,6 @@ static word_t CONST APFromVMRights(vm_rights_t vm_rights)
     }
 }
 
-#ifndef CONFIG_ARM_HYPERVISOR_SUPPORT
-static inline CONST word_t pte_get_AP(pte_t pte)
-{
-    assert(pte_is_page_type(pte));
-    switch (pte_get_pte_type(pte)) {
-    case pte_pte_4k_page:
-        return pte_pte_4k_page_get_AP(pte);
-
-    case pte_pte_page:
-        return pte_pte_page_get_AP(pte);
-
-    default:
-        return 0;
-    }
-}
-
-static vm_rights_t CONST vmRightsFromPTE(pte_t pte)
-{
-    word_t access_perms = pte_get_AP(pte);
-    switch (access_perms) {
-    case 0:
-    case 2:
-        return VMKernelOnly;
-    case 1:
-        return VMReadWrite;
-    case 3:
-        return VMReadOnly;
-    default:
-        fail("Invalid AP bit");
-    }
-}
-#endif
-
 vm_rights_t CONST maskVMRights(vm_rights_t vm_rights, seL4_CapRights_t cap_rights_mask)
 {
     if (vm_rights == VMReadOnly &&
@@ -225,12 +226,13 @@ BOOT_CODE void map_kernel_frame(paddr_t paddr, pptr_t vaddr, vm_rights_t vm_righ
         attr_index = DEVICE_nGnRnE;
         shareable = 0;
     }
-    armKSGlobalKernelPT[GET_KPT_INDEX(vaddr, KLVL_FRM_ARM_PT_LVL(3))] = pte_pte_4k_page_new(uxn, paddr,
-                                                                                            0, /* global */
-                                                                                            1, /* access flag */
-                                                                                            shareable,
-                                                                                            APFromVMRights(vm_rights),
-                                                                                            attr_index);
+    armKSGlobalKernelPT[GET_PT_INDEX(vaddr)] = pte_new(uxn, paddr,
+                                                       0, /* global */
+                                                       1, /* access flag */
+                                                       shareable,
+                                                       APFromVMRights(vm_rights),
+                                                       attr_index,
+                                                       RESERVED);
 }
 
 BOOT_CODE void map_kernel_window(void)
@@ -242,24 +244,23 @@ BOOT_CODE void map_kernel_window(void)
 
 #ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
     /* verify that the kernel window as at the second entry of the PGD */
-    assert(GET_KPT_INDEX(PPTR_BASE, KLVL_FRM_ARM_PT_LVL(0)) == 1);
+    assert(GET_PGD_INDEX(PPTR_BASE) == 1);
 #else
     /* verify that the kernel window as at the last entry of the PGD */
-    assert(GET_KPT_INDEX(PPTR_BASE, KLVL_FRM_ARM_PT_LVL(0)) == BIT(PT_INDEX_BITS) - 1);
+    assert(GET_PGD_INDEX(PPTR_BASE) == BIT(PGD_INDEX_BITS) - 1);
 #endif
     assert(IS_ALIGNED(PPTR_BASE, seL4_LargePageBits));
     /* verify that the kernel device window is 1gb aligned and 1gb in size */
-    assert(GET_KPT_INDEX(PPTR_TOP, KLVL_FRM_ARM_PT_LVL(1)) == BIT(PT_INDEX_BITS) - 1);
+    assert(GET_PUD_INDEX(PPTR_TOP) == BIT(PUD_INDEX_BITS) - 1);
     assert(IS_ALIGNED(PPTR_TOP, seL4_HugePageBits));
 
     /* place the PUD into the PGD */
-    armKSGlobalKernelPGD[GET_KPT_INDEX(PPTR_BASE, KLVL_FRM_ARM_PT_LVL(0))] = pte_pte_table_new(
-                                                                                 addrFromKPPtr(armKSGlobalKernelPUD));
+    armKSGlobalKernelPGD[GET_PGD_INDEX(PPTR_BASE)] = pgde_pgde_pud_new(
+                                                         addrFromKPPtr(armKSGlobalKernelPUD));
 
     /* place all PDs except the last one in PUD */
-    for (idx = GET_KPT_INDEX(PPTR_BASE, KLVL_FRM_ARM_PT_LVL(1)); idx < GET_KPT_INDEX(PPTR_TOP, KLVL_FRM_ARM_PT_LVL(1));
-         idx++) {
-        armKSGlobalKernelPUD[idx] = pte_pte_table_new(
+    for (idx = GET_PUD_INDEX(PPTR_BASE); idx < GET_PUD_INDEX(PPTR_TOP); idx++) {
+        armKSGlobalKernelPUD[idx] = pude_pude_pd_new(
                                         addrFromKPPtr(&armKSGlobalKernelPDs[idx][0])
                                     );
     }
@@ -267,32 +268,31 @@ BOOT_CODE void map_kernel_window(void)
     /* map the kernel window using large pages */
     vaddr = PPTR_BASE;
     for (paddr = PADDR_BASE; paddr < PADDR_TOP; paddr += BIT(seL4_LargePageBits)) {
-        armKSGlobalKernelPDs[GET_KPT_INDEX(vaddr, KLVL_FRM_ARM_PT_LVL(1))][GET_KPT_INDEX(vaddr,
-                                                                                         KLVL_FRM_ARM_PT_LVL(2))] = pte_pte_page_new(
+        armKSGlobalKernelPDs[GET_PUD_INDEX(vaddr)][GET_PD_INDEX(vaddr)] = pde_pde_large_new(
 #ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
-                                                                                                                        0, // XN
+                                                                              0, // XN
 #else
-                                                                                                                        1, // UXN
+                                                                              1, // UXN
 #endif
-                                                                                                                        paddr,
-                                                                                                                        0,                        /* global */
-                                                                                                                        1,                        /* access flag */
-                                                                                                                        SMP_TERNARY(SMP_SHARE, 0),        /* Inner-shareable if SMP enabled, otherwise unshared */
-                                                                                                                        0,                        /* VMKernelOnly */
-                                                                                                                        NORMAL
-                                                                                                                    );
+                                                                              paddr,
+                                                                              0,                        /* global */
+                                                                              1,                        /* access flag */
+                                                                              SMP_TERNARY(SMP_SHARE, 0),        /* Inner-shareable if SMP enabled, otherwise unshared */
+                                                                              0,                        /* VMKernelOnly */
+                                                                              NORMAL
+                                                                          );
         vaddr += BIT(seL4_LargePageBits);
     }
 
     /* put the PD into the PUD for device window */
-    armKSGlobalKernelPUD[GET_KPT_INDEX(PPTR_TOP, KLVL_FRM_ARM_PT_LVL(1))] = pte_pte_table_new(
-                                                                                addrFromKPPtr(&armKSGlobalKernelPDs[BIT(PT_INDEX_BITS) - 1][0])
-                                                                            );
+    armKSGlobalKernelPUD[GET_PUD_INDEX(PPTR_TOP)] = pude_pude_pd_new(
+                                                        addrFromKPPtr(&armKSGlobalKernelPDs[BIT(PUD_INDEX_BITS) - 1][0])
+                                                    );
 
     /* put the PT into the PD for device window */
-    armKSGlobalKernelPDs[BIT(PT_INDEX_BITS) - 1][BIT(PT_INDEX_BITS) - 1] = pte_pte_table_new(
-                                                                               addrFromKPPtr(armKSGlobalKernelPT)
-                                                                           );
+    armKSGlobalKernelPDs[BIT(PUD_INDEX_BITS) - 1][BIT(PD_INDEX_BITS) - 1] = pde_pde_small_new(
+                                                                                addrFromKPPtr(armKSGlobalKernelPT)
+                                                                            );
 
     map_kernel_devices();
 }
@@ -305,8 +305,8 @@ BOOT_CODE void map_kernel_window(void)
 static BOOT_CODE void map_it_frame_cap(cap_t vspace_cap, cap_t frame_cap, bool_t executable)
 {
     vspace_root_t *vspaceRoot = VSPACE_PTR(pptr_of_cap(vspace_cap));
-    pte_t *pud;
-    pte_t *pd;
+    pude_t *pud;
+    pde_t *pd;
     pte_t *pt;
 
     vptr_t vptr = cap_frame_cap_get_capFMappedAddress(frame_cap);
@@ -317,33 +317,34 @@ static BOOT_CODE void map_it_frame_cap(cap_t vspace_cap, cap_t frame_cap, bool_t
 #ifdef AARCH64_VSPACE_S2_START_L1
     pud = vspaceRoot;
 #else
-    vspaceRoot += GET_UPT_INDEX(vptr, ULVL_FRM_ARM_PT_LVL(0));
-    assert(pte_pte_table_ptr_get_present(vspaceRoot));
-    pud = paddr_to_pptr(pte_pte_table_ptr_get_pt_base_address(vspaceRoot));
+    vspaceRoot += GET_PGD_INDEX(vptr);
+    assert(pgde_pgde_pud_ptr_get_present(vspaceRoot));
+    pud = paddr_to_pptr(pgde_pgde_pud_ptr_get_pud_base_address(vspaceRoot));
 #endif
-    pud += GET_UPT_INDEX(vptr, ULVL_FRM_ARM_PT_LVL(1));
-    assert(pte_pte_table_ptr_get_present(pud));
-    pd = paddr_to_pptr(pte_pte_table_ptr_get_pt_base_address(pud));
-    pd += GET_UPT_INDEX(vptr, ULVL_FRM_ARM_PT_LVL(2));
-    assert(pte_pte_table_ptr_get_present(pd));
-    pt = paddr_to_pptr(pte_pte_table_ptr_get_pt_base_address(pd));
-    *(pt + GET_UPT_INDEX(vptr, ULVL_FRM_ARM_PT_LVL(3))) = pte_pte_4k_page_new(
-                                                              !executable,                    /* unprivileged execute never */
-                                                              pptr_to_paddr(pptr),            /* page_base_address    */
+    pud += GET_UPUD_INDEX(vptr);
+    assert(pude_pude_pd_ptr_get_present(pud));
+    pd = paddr_to_pptr(pude_pude_pd_ptr_get_pd_base_address(pud));
+    pd += GET_PD_INDEX(vptr);
+    assert(pde_pde_small_ptr_get_present(pd));
+    pt = paddr_to_pptr(pde_pde_small_ptr_get_pt_base_address(pd));
+    *(pt + GET_PT_INDEX(vptr)) = pte_new(
+                                     !executable,                    /* unprivileged execute never */
+                                     pptr_to_paddr(pptr),            /* page_base_address    */
 #ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
-                                                              0,
+                                     0,
 #else
-                                                              1,                              /* not global */
+                                     1,                              /* not global */
 #endif
-                                                              1,                              /* access flag */
-                                                              SMP_TERNARY(SMP_SHARE, 0),              /* Inner-shareable if SMP enabled, otherwise unshared */
-                                                              APFromVMRights(VMReadWrite),
+                                     1,                              /* access flag */
+                                     SMP_TERNARY(SMP_SHARE, 0),              /* Inner-shareable if SMP enabled, otherwise unshared */
+                                     APFromVMRights(VMReadWrite),
 #ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
-                                                              S2_NORMAL
+                                     S2_NORMAL,
 #else
-                                                              NORMAL
+                                     NORMAL,
 #endif
-                                                          );
+                                     RESERVED
+                                 );
 }
 
 static BOOT_CODE cap_t create_it_frame_cap(pptr_t pptr, vptr_t vptr, asid_t asid, bool_t use_large)
@@ -368,8 +369,8 @@ static BOOT_CODE cap_t create_it_frame_cap(pptr_t pptr, vptr_t vptr, asid_t asid
 static BOOT_CODE void map_it_pt_cap(cap_t vspace_cap, cap_t pt_cap)
 {
     vspace_root_t *vspaceRoot = VSPACE_PTR(pptr_of_cap(vspace_cap));
-    pte_t *pud;
-    pte_t *pd;
+    pude_t *pud;
+    pde_t *pd;
     pte_t *pt = PT_PTR(cap_page_table_cap_get_capPTBasePtr(pt_cap));
     vptr_t vptr = cap_page_table_cap_get_capPTMappedAddress(pt_cap);
 
@@ -378,16 +379,16 @@ static BOOT_CODE void map_it_pt_cap(cap_t vspace_cap, cap_t pt_cap)
 #ifdef AARCH64_VSPACE_S2_START_L1
     pud = vspaceRoot;
 #else
-    vspaceRoot += GET_UPT_INDEX(vptr, ULVL_FRM_ARM_PT_LVL(0));
-    assert(pte_pte_table_ptr_get_present(vspaceRoot));
-    pud = paddr_to_pptr(pte_pte_table_ptr_get_pt_base_address(vspaceRoot));
+    vspaceRoot += GET_PGD_INDEX(vptr);
+    assert(pgde_pgde_pud_ptr_get_present(vspaceRoot));
+    pud = paddr_to_pptr(pgde_pgde_pud_ptr_get_pud_base_address(vspaceRoot));
 #endif
-    pud += GET_UPT_INDEX(vptr, ULVL_FRM_ARM_PT_LVL(1));
-    assert(pte_pte_table_ptr_get_present(pud));
-    pd = paddr_to_pptr(pte_pte_table_ptr_get_pt_base_address(pud));
-    *(pd + GET_UPT_INDEX(vptr, ULVL_FRM_ARM_PT_LVL(2))) = pte_pte_table_new(
-                                                              pptr_to_paddr(pt)
-                                                          );
+    pud += GET_UPUD_INDEX(vptr);
+    assert(pude_pude_pd_ptr_get_present(pud));
+    pd = paddr_to_pptr(pude_pude_pd_ptr_get_pd_base_address(pud));
+    *(pd + GET_PD_INDEX(vptr)) = pde_pde_small_new(
+                                     pptr_to_paddr(pt)
+                                 );
 }
 
 static BOOT_CODE cap_t create_it_pt_cap(cap_t vspace_cap, pptr_t pptr, vptr_t vptr, asid_t asid)
@@ -406,32 +407,32 @@ static BOOT_CODE cap_t create_it_pt_cap(cap_t vspace_cap, pptr_t pptr, vptr_t vp
 static BOOT_CODE void map_it_pd_cap(cap_t vspace_cap, cap_t pd_cap)
 {
     vspace_root_t *vspaceRoot = VSPACE_PTR(pptr_of_cap(vspace_cap));
-    pte_t *pud;
-    pte_t *pd = PT_PTR(cap_page_table_cap_get_capPTBasePtr(pd_cap));
-    vptr_t vptr = cap_page_table_cap_get_capPTMappedAddress(pd_cap);
+    pude_t *pud;
+    pde_t *pd = PD_PTR(cap_page_directory_cap_get_capPDBasePtr(pd_cap));
+    vptr_t vptr = cap_page_directory_cap_get_capPDMappedAddress(pd_cap);
 
-    assert(cap_page_table_cap_get_capPTIsMapped(pd_cap));
+    assert(cap_page_directory_cap_get_capPDIsMapped(pd_cap));
 
 #ifdef AARCH64_VSPACE_S2_START_L1
     pud = vspaceRoot;
 #else
-    vspaceRoot += GET_UPT_INDEX(vptr, ULVL_FRM_ARM_PT_LVL(0));
-    assert(pte_pte_table_ptr_get_present(vspaceRoot));
-    pud = paddr_to_pptr(pte_pte_table_ptr_get_pt_base_address(vspaceRoot));
+    vspaceRoot += GET_PGD_INDEX(vptr);
+    assert(pgde_pgde_pud_ptr_get_present(vspaceRoot));
+    pud = paddr_to_pptr(pgde_pgde_pud_ptr_get_pud_base_address(vspaceRoot));
 #endif
-    *(pud + GET_UPT_INDEX(vptr, ULVL_FRM_ARM_PT_LVL(1))) = pte_pte_table_new(
-                                                               pptr_to_paddr(pd)
-                                                           );
+    *(pud + GET_UPUD_INDEX(vptr)) = pude_pude_pd_new(
+                                        pptr_to_paddr(pd)
+                                    );
 }
 
 static BOOT_CODE cap_t create_it_pd_cap(cap_t vspace_cap, pptr_t pptr, vptr_t vptr, asid_t asid)
 {
     cap_t cap;
-    cap = cap_page_table_cap_new(
-              asid,                   /* capPTMappedASID */
-              pptr,                   /* capPTBasePtr */
-              1,                      /* capPTIsMapped */
-              vptr                    /* capPTMappedAddress */
+    cap = cap_page_directory_cap_new(
+              asid,                   /* capPDMappedASID */
+              pptr,                   /* capPDBasePtr */
+              1,                      /* capPDIsMapped */
+              vptr                    /* capPDMappedAddress */
           );
     map_it_pd_cap(vspace_cap, cap);
     return cap;
@@ -440,24 +441,24 @@ static BOOT_CODE cap_t create_it_pd_cap(cap_t vspace_cap, pptr_t pptr, vptr_t vp
 #ifndef AARCH64_VSPACE_S2_START_L1
 static BOOT_CODE void map_it_pud_cap(cap_t vspace_cap, cap_t pud_cap)
 {
-    pte_t *pgd = PT_PTR(pptr_of_cap(vspace_cap));
-    pte_t *pud = PT_PTR(cap_page_table_cap_get_capPTBasePtr(pud_cap));
-    vptr_t vptr = cap_page_table_cap_get_capPTMappedAddress(pud_cap);
+    pgde_t *pgd = PGD_PTR(pptr_of_cap(vspace_cap));
+    pude_t *pud = PUD_PTR(cap_page_upper_directory_cap_get_capPUDBasePtr(pud_cap));
+    vptr_t vptr = cap_page_upper_directory_cap_get_capPUDMappedAddress(pud_cap);
 
-    assert(cap_page_table_cap_get_capPTIsMapped(pud_cap));
+    assert(cap_page_upper_directory_cap_get_capPUDIsMapped(pud_cap));
 
-    *(pgd + GET_UPT_INDEX(vptr, ULVL_FRM_ARM_PT_LVL(0))) = pte_pte_table_new(
-                                                               pptr_to_paddr(pud));
+    *(pgd + GET_PGD_INDEX(vptr)) = pgde_pgde_pud_new(
+                                       pptr_to_paddr(pud));
 }
 
 static BOOT_CODE cap_t create_it_pud_cap(cap_t vspace_cap, pptr_t pptr, vptr_t vptr, asid_t asid)
 {
     cap_t cap;
-    cap = cap_page_table_cap_new(
-              asid,               /* capPTMappedASID */
-              pptr,               /* capPTBasePtr */
-              1,                  /* capPTIsMapped */
-              vptr                /* capPTMappedAddress */
+    cap = cap_page_upper_directory_cap_new(
+              asid,               /* capPUDMappedASID */
+              pptr,               /* capPUDBasePtr */
+              1,                  /* capPUDIsMapped */
+              vptr                /* capPUDMappedAddress */
           );
     map_it_pud_cap(vspace_cap, cap);
     return cap;
@@ -467,10 +468,10 @@ BOOT_CODE word_t arch_get_n_paging(v_region_t it_v_reg)
 {
     return
 #ifndef AARCH64_VSPACE_S2_START_L1
-        get_n_paging(it_v_reg, GET_ULVL_PGSIZE_BITS(ULVL_FRM_ARM_PT_LVL(0))) +
+        get_n_paging(it_v_reg, PGD_INDEX_OFFSET) +
 #endif
-        get_n_paging(it_v_reg, GET_ULVL_PGSIZE_BITS(ULVL_FRM_ARM_PT_LVL(1))) +
-        get_n_paging(it_v_reg, GET_ULVL_PGSIZE_BITS(ULVL_FRM_ARM_PT_LVL(2)));
+        get_n_paging(it_v_reg, PUD_INDEX_OFFSET) +
+        get_n_paging(it_v_reg, PD_INDEX_OFFSET);
 }
 
 BOOT_CODE cap_t create_it_address_space(cap_t root_cnode_cap, v_region_t it_v_reg)
@@ -481,40 +482,37 @@ BOOT_CODE cap_t create_it_address_space(cap_t root_cnode_cap, v_region_t it_v_re
     seL4_SlotPos slot_pos_after;
 
     /* create the PGD */
-    vspace_cap = cap_vspace_cap_new(
-                     IT_ASID,           /* capVSMappedASID */
-                     rootserver.vspace, /* capVSBasePtr    */
-                     1                  /* capVSIsMapped   */
-#ifdef CONFIG_ARM_SMMU
-                     , 0                /* capVSMappedCB   */
-#endif
+    vspace_cap = cap_vtable_cap_new(
+                     IT_ASID,        /* capPGDMappedASID */
+                     rootserver.vspace, /* capPGDBasePtr   */
+                     1               /* capPGDIsMapped   */
                  );
     slot_pos_before = ndks_boot.slot_pos_cur;
     write_slot(SLOT_PTR(pptr_of_cap(root_cnode_cap), seL4_CapInitThreadVSpace), vspace_cap);
 
 #ifndef AARCH64_VSPACE_S2_START_L1
     /* Create any PUDs needed for the user land image */
-    for (vptr = ROUND_DOWN(it_v_reg.start, GET_ULVL_PGSIZE_BITS(ULVL_FRM_ARM_PT_LVL(0)));
+    for (vptr = ROUND_DOWN(it_v_reg.start, PGD_INDEX_OFFSET);
          vptr < it_v_reg.end;
-         vptr += GET_ULVL_PGSIZE(ULVL_FRM_ARM_PT_LVL(0))) {
+         vptr += BIT(PGD_INDEX_OFFSET)) {
         if (!provide_cap(root_cnode_cap, create_it_pud_cap(vspace_cap, it_alloc_paging(), vptr, IT_ASID))) {
             return cap_null_cap_new();
         }
     }
 #endif
     /* Create any PDs needed for the user land image */
-    for (vptr = ROUND_DOWN(it_v_reg.start, GET_ULVL_PGSIZE_BITS(ULVL_FRM_ARM_PT_LVL(1)));
+    for (vptr = ROUND_DOWN(it_v_reg.start, PUD_INDEX_OFFSET);
          vptr < it_v_reg.end;
-         vptr += GET_ULVL_PGSIZE(ULVL_FRM_ARM_PT_LVL(1))) {
+         vptr += BIT(PUD_INDEX_OFFSET)) {
         if (!provide_cap(root_cnode_cap, create_it_pd_cap(vspace_cap, it_alloc_paging(), vptr, IT_ASID))) {
             return cap_null_cap_new();
         }
     }
 
     /* Create any PTs needed for the user land image */
-    for (vptr = ROUND_DOWN(it_v_reg.start, GET_ULVL_PGSIZE_BITS(ULVL_FRM_ARM_PT_LVL(2)));
+    for (vptr = ROUND_DOWN(it_v_reg.start, PD_INDEX_OFFSET);
          vptr < it_v_reg.end;
-         vptr += GET_ULVL_PGSIZE(ULVL_FRM_ARM_PT_LVL(2))) {
+         vptr += BIT(PD_INDEX_OFFSET)) {
         if (!provide_cap(root_cnode_cap, create_it_pt_cap(vspace_cap, it_alloc_paging(), vptr, IT_ASID))) {
             return cap_null_cap_new();
         }
@@ -561,7 +559,7 @@ BOOT_CODE void write_it_asid_pool(cap_t it_ap_cap, cap_t it_vspace_cap)
                               0,
 #endif
                               /* vspace_root: reference to vspace root page table object */
-                              (word_t)cap_vspace_cap_get_capVSBasePtr(it_vspace_cap)
+                              (word_t)cap_vtable_root_get_basePtr(it_vspace_cap)
 #ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
                               /* stored_hw_vmid, stored_vmid_valid: Assigned hardware VMID for TLB. */
                               , 0, false
@@ -657,56 +655,289 @@ exception_t checkValidIPCBuffer(vptr_t vptr, cap_t cap)
     return EXCEPTION_NONE;
 }
 
+static lookupPGDSlot_ret_t lookupPGDSlot(vspace_root_t *vspace, vptr_t vptr)
+{
+    lookupPGDSlot_ret_t ret;
+
+    pgde_t *pgd = PGDE_PTR(vspace);
+    word_t pgdIndex = GET_PGD_INDEX(vptr);
+    ret.status = EXCEPTION_NONE;
+    ret.pgdSlot = pgd + pgdIndex;
+    return ret;
+}
+
+static lookupPUDSlot_ret_t lookupPUDSlot(vspace_root_t *vspace, vptr_t vptr)
+{
+    lookupPUDSlot_ret_t ret;
+
+#ifdef AARCH64_VSPACE_S2_START_L1
+    pude_t *pud = PUDE_PTR(vspace);
+    word_t pudIndex = GET_UPUD_INDEX(vptr);
+    ret.status = EXCEPTION_NONE;
+    ret.pudSlot = pud + pudIndex;
+    return ret;
+#else
+    lookupPGDSlot_ret_t pgdSlot = lookupPGDSlot(vspace, vptr);
+
+    if (!pgde_pgde_pud_ptr_get_present(pgdSlot.pgdSlot)) {
+        current_lookup_fault = lookup_fault_missing_capability_new(PGD_INDEX_OFFSET);
+
+        ret.pudSlot = NULL;
+        ret.status = EXCEPTION_LOOKUP_FAULT;
+        return ret;
+    } else {
+        pude_t *pud;
+        pude_t *pudSlot;
+        word_t pudIndex = GET_UPUD_INDEX(vptr);
+        pud = paddr_to_pptr(pgde_pgde_pud_ptr_get_pud_base_address(pgdSlot.pgdSlot));
+        pudSlot = pud + pudIndex;
+
+        ret.status = EXCEPTION_NONE;
+        ret.pudSlot = pudSlot;
+        return ret;
+    }
+#endif
+}
+
+static lookupPDSlot_ret_t lookupPDSlot(vspace_root_t *vspace, vptr_t vptr)
+{
+    lookupPUDSlot_ret_t pudSlot;
+    lookupPDSlot_ret_t ret;
+
+    pudSlot = lookupPUDSlot(vspace, vptr);
+    if (pudSlot.status != EXCEPTION_NONE) {
+        ret.pdSlot = NULL;
+        ret.status = pudSlot.status;
+        return ret;
+    }
+    if (!pude_pude_pd_ptr_get_present(pudSlot.pudSlot)) {
+        current_lookup_fault = lookup_fault_missing_capability_new(PUD_INDEX_OFFSET);
+
+        ret.pdSlot = NULL;
+        ret.status = EXCEPTION_LOOKUP_FAULT;
+        return ret;
+    } else {
+        pde_t *pd;
+        pde_t *pdSlot;
+        word_t pdIndex = GET_PD_INDEX(vptr);
+        pd = paddr_to_pptr(pude_pude_pd_ptr_get_pd_base_address(pudSlot.pudSlot));
+        pdSlot = pd + pdIndex;
+
+        ret.status = EXCEPTION_NONE;
+        ret.pdSlot = pdSlot;
+        return ret;
+    }
+}
+
 static lookupPTSlot_ret_t lookupPTSlot(vspace_root_t *vspace, vptr_t vptr)
 {
     lookupPTSlot_ret_t ret;
+    lookupPDSlot_ret_t pdSlot;
 
-    word_t level = UPT_LEVELS - 1;
-    pte_t *pt = vspace;
+    pdSlot = lookupPDSlot(vspace, vptr);
+    if (pdSlot.status != EXCEPTION_NONE) {
+        ret.ptSlot = NULL;
+        ret.status = pdSlot.status;
+        return ret;
+    }
+    if (!pde_pde_small_ptr_get_present(pdSlot.pdSlot)) {
+        current_lookup_fault = lookup_fault_missing_capability_new(PD_INDEX_OFFSET);
 
-    /* this is how many bits we potentially have left to decode. Initially we have the
-     * full address space to decode, and every time we walk this will be reduced. The
-     * final value of this after the walk is the size of the frame that can be inserted,
-     * or already exists, in ret.ptSlot. The following formulation is an invariant of
-     * the loop: */
-    ret.ptBitsLeft = PT_INDEX_BITS * level + seL4_PageBits;
-    ret.ptSlot = pt + ((vptr >> ret.ptBitsLeft) & MASK(seL4_VSpaceIndexBits));
+        ret.ptSlot = NULL;
+        ret.status = EXCEPTION_LOOKUP_FAULT;
+        return ret;
+    } else {
+        pte_t *pt;
+        pte_t *ptSlot;
+        word_t ptIndex = GET_PT_INDEX(vptr);
+        pt = paddr_to_pptr(pde_pde_small_ptr_get_pt_base_address(pdSlot.pdSlot));
+        ptSlot = pt + ptIndex;
+
+        ret.ptSlot = ptSlot;
+        ret.status = EXCEPTION_NONE;
+        return ret;
+    }
+}
 
-    while (pte_pte_table_ptr_get_present(ret.ptSlot) && likely(level > 0)) {
-        level--;
-        ret.ptBitsLeft -= PT_INDEX_BITS;
-        pt = paddr_to_pptr(pte_pte_table_ptr_get_pt_base_address(ret.ptSlot));
-        ret.ptSlot = pt + ((vptr >> ret.ptBitsLeft) & MASK(PT_INDEX_BITS));
+static lookupFrame_ret_t lookupFrame(vspace_root_t *vspace, vptr_t vptr)
+{
+    lookupPUDSlot_ret_t pudSlot;
+    lookupFrame_ret_t ret;
+
+    pudSlot = lookupPUDSlot(vspace, vptr);
+    if (pudSlot.status != EXCEPTION_NONE) {
+        ret.valid = false;
+        return ret;
     }
 
+    switch (pude_ptr_get_pude_type(pudSlot.pudSlot)) {
+    case pude_pude_1g:
+        ret.frameBase = pude_pude_1g_ptr_get_page_base_address(pudSlot.pudSlot);
+        ret.frameSize = ARMHugePage;
+        ret.valid = true;
+        return ret;
+
+    case pude_pude_pd: {
+        pde_t *pd = paddr_to_pptr(pude_pude_pd_ptr_get_pd_base_address(pudSlot.pudSlot));
+        pde_t *pdSlot = pd + GET_PD_INDEX(vptr);
+
+        if (pde_ptr_get_pde_type(pdSlot) == pde_pde_large) {
+            ret.frameBase = pde_pde_large_ptr_get_page_base_address(pdSlot);
+            ret.frameSize = ARMLargePage;
+            ret.valid = true;
+            return ret;
+        }
+
+        if (pde_ptr_get_pde_type(pdSlot) == pde_pde_small) {
+            pte_t *pt = paddr_to_pptr(pde_pde_small_ptr_get_pt_base_address(pdSlot));
+            pte_t *ptSlot = pt + GET_PT_INDEX(vptr);
+
+            if (pte_ptr_get_present(ptSlot)) {
+                ret.frameBase = pte_ptr_get_page_base_address(ptSlot);
+                ret.frameSize = ARMSmallPage;
+                ret.valid = true;
+                return ret;
+            }
+        }
+    }
+    }
+
+    ret.valid = false;
     return ret;
 }
 
 /* Note that if the hypervisor support is enabled, the user page tables use
  * stage-2 translation format. Otherwise, they follow the stage-1 translation format.
  */
-static pte_t makeUserPagePTE(paddr_t paddr, vm_rights_t vm_rights, vm_attributes_t attributes, vm_page_size_t page_size)
+static pte_t makeUser3rdLevel(paddr_t paddr, vm_rights_t vm_rights, vm_attributes_t attributes)
 {
     bool_t nonexecutable = vm_attributes_get_armExecuteNever(attributes);
-    word_t cacheable = vm_attributes_get_armPageCacheable(attributes);
 
+    if (vm_attributes_get_armPageCacheable(attributes)) {
+        return pte_new(
+                   nonexecutable,              /* unprivileged execute never */
+                   paddr,
+#ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
+                   0,
+#else
+                   1,                          /* not global */
+#endif
+                   1,                          /* access flag */
+                   SMP_TERNARY(SMP_SHARE, 0),          /* Inner-shareable if SMP enabled, otherwise unshared */
+                   APFromVMRights(vm_rights),
+#ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
+                   S2_NORMAL,
+#else
+                   NORMAL,
+#endif
+                   RESERVED
+               );
+    } else {
+        return pte_new(
+                   nonexecutable,              /* unprivileged execute never */
+                   paddr,
 #ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
-    word_t nG = 0; /* not global */
-    word_t attridx = cacheable ? S2_NORMAL : S2_DEVICE_nGnRnE;
+                   0,
 #else
-    word_t nG = 1; /* not global */
-    word_t attridx = cacheable ? NORMAL : DEVICE_nGnRnE;
+                   1,                          /* not global */
 #endif
+                   1,                          /* access flag */
+                   0,                          /* Ignored - Outter shareable */
+                   APFromVMRights(vm_rights),
+#ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
+                   S2_DEVICE_nGnRnE,
+#else
+                   DEVICE_nGnRnE,
+#endif
+
+                   RESERVED
+               );
+    }
+}
 
-    /* Inner-shareable if SMP enabled, otherwise unshared (ignored for devices) */
-    word_t shareable = cacheable ? SMP_TERNARY(SMP_SHARE, 0) : 0;
+static pde_t makeUser2ndLevel(paddr_t paddr, vm_rights_t vm_rights, vm_attributes_t attributes)
+{
+    bool_t nonexecutable = vm_attributes_get_armExecuteNever(attributes);
 
-    if (page_size == ARMSmallPage) {
-        return pte_pte_4k_page_new(nonexecutable, paddr, nG, 1 /* access flag */,
-                                   shareable, APFromVMRights(vm_rights), attridx);
+    if (vm_attributes_get_armPageCacheable(attributes)) {
+        return pde_pde_large_new(
+                   nonexecutable,              /* unprivileged execute never */
+                   paddr,
+#ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
+                   0,
+#else
+                   1,                          /* not global */
+#endif
+                   1,                          /* access flag */
+                   SMP_TERNARY(SMP_SHARE, 0),          /* Inner-shareable if SMP enabled, otherwise unshared */
+                   APFromVMRights(vm_rights),
+#ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
+                   S2_NORMAL
+#else
+                   NORMAL
+#endif
+               );
     } else {
-        return pte_pte_page_new(nonexecutable, paddr, nG, 1 /* access flag */,
-                                shareable, APFromVMRights(vm_rights), attridx);
+        return pde_pde_large_new(
+                   nonexecutable,              /* unprivileged execute never */
+                   paddr,
+#ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
+                   0,
+#else
+                   1,                          /* not global */
+#endif
+                   1,                          /* access flag */
+                   0,                          /* Ignored - Outter shareable */
+                   APFromVMRights(vm_rights),
+#ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
+                   S2_DEVICE_nGnRnE
+#else
+                   DEVICE_nGnRnE
+#endif
+               );
+    }
+}
+
+static pude_t makeUser1stLevel(paddr_t paddr, vm_rights_t vm_rights, vm_attributes_t attributes)
+{
+    bool_t nonexecutable = vm_attributes_get_armExecuteNever(attributes);
+
+    if (vm_attributes_get_armPageCacheable(attributes)) {
+        return pude_pude_1g_new(
+                   nonexecutable,              /* unprivileged execute never */
+                   paddr,
+#ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
+                   0,
+#else
+                   1,                          /* not global */
+#endif
+                   1,                          /* access flag */
+                   SMP_TERNARY(SMP_SHARE, 0),          /* Inner-shareable if SMP enabled, otherwise unshared */
+                   APFromVMRights(vm_rights),
+#ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
+                   S2_NORMAL
+#else
+                   NORMAL
+#endif
+               );
+    } else {
+        return pude_pude_1g_new(
+                   nonexecutable,              /* unprivileged execute never */
+                   paddr,
+#ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
+                   0,
+#else
+                   1,                          /* not global */
+#endif
+                   1,                          /* access flag */
+                   0,                          /* Ignored - Outter shareable */
+                   APFromVMRights(vm_rights),
+#ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
+                   S2_DEVICE_nGnRnE
+#else
+                   DEVICE_nGnRnE
+#endif
+               );
     }
 }
 
@@ -751,13 +982,13 @@ exception_t handleVMFault(tcb_t *thread, vm_fault_type_t vm_faultType)
 
 bool_t CONST isVTableRoot(cap_t cap)
 {
-    return cap_get_capType(cap) == cap_vspace_cap;
+    return cap_get_capType(cap) == cap_vtable_root_cap;
 }
 
 bool_t CONST isValidNativeRoot(cap_t cap)
 {
     return isVTableRoot(cap) &&
-           cap_vspace_cap_get_capVSIsMapped(cap);
+           cap_vtable_root_isMapped(cap);
 }
 
 bool_t CONST isValidVTableRoot(cap_t cap)
@@ -779,8 +1010,8 @@ void setVMRoot(tcb_t *tcb)
         return;
     }
 
-    vspaceRoot = VSPACE_PTR(cap_vspace_cap_get_capVSBasePtr(threadRoot));
-    asid = cap_vspace_cap_get_capVSMappedASID(threadRoot);
+    vspaceRoot = VSPACE_PTR(cap_vtable_root_get_basePtr(threadRoot));
+    asid = cap_vtable_root_get_mappedASID(threadRoot);
     find_ret = findVSpaceForASID(asid);
     if (unlikely(find_ret.status != EXCEPTION_NONE || find_ret.vspace_root != vspaceRoot)) {
         setCurrentUserVSpaceRoot(ttbr_new(0, addrFromKPPtr(armKSGlobalUserVSpace)));
@@ -796,9 +1027,9 @@ static bool_t setVMRootForFlush(vspace_root_t *vspace, asid_t asid)
 
     threadRoot = TCB_PTR_CTE_PTR(NODE_STATE(ksCurThread), tcbVTable)->cap;
 
-    if (cap_get_capType(threadRoot) == cap_vspace_cap &&
-        cap_vspace_cap_get_capVSIsMapped(threadRoot) &&
-        VSPACE_PTR(cap_vspace_cap_get_capVSBasePtr(threadRoot)) == vspace) {
+    if (cap_get_capType(threadRoot) == cap_vtable_root_cap &&
+        cap_vtable_root_isMapped(threadRoot) &&
+        cap_vtable_root_get_basePtr(threadRoot) == vspace) {
         return false;
     }
 
@@ -806,54 +1037,81 @@ static bool_t setVMRootForFlush(vspace_root_t *vspace, asid_t asid)
     return true;
 }
 
+pgde_t *pageUpperDirectoryMapped(asid_t asid, vptr_t vaddr, pude_t *pud)
+{
+    findVSpaceForASID_ret_t find_ret;
+    lookupPGDSlot_ret_t lu_ret;
 
-#ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
+    find_ret = findVSpaceForASID(asid);
+    if (find_ret.status != EXCEPTION_NONE) {
+        return NULL;
+    }
 
-static inline asid_pool_t *getPoolPtr(asid_t asid)
-{
-    return armKSASIDTable[asid >> asidLowBits];
+    lu_ret = lookupPGDSlot(find_ret.vspace_root, vaddr);
+    if (pgde_pgde_pud_ptr_get_present(lu_ret.pgdSlot) &&
+        (pgde_pgde_pud_ptr_get_pud_base_address(lu_ret.pgdSlot) == pptr_to_paddr(pud))) {
+        return lu_ret.pgdSlot;
+    }
+
+    return NULL;
 }
 
-static inline asid_map_t getASIDMap(asid_pool_t *poolPtr, asid_t asid)
+pude_t *pageDirectoryMapped(asid_t asid, vptr_t vaddr, pde_t *pd)
 {
-    assert(poolPtr != NULL);
-    return poolPtr->array[asid & MASK(asidLowBits)];
+    findVSpaceForASID_ret_t find_ret;
+    lookupPUDSlot_ret_t lu_ret;
+
+    find_ret = findVSpaceForASID(asid);
+    if (find_ret.status != EXCEPTION_NONE) {
+        return NULL;
+    }
+
+    lu_ret = lookupPUDSlot(find_ret.vspace_root, vaddr);
+    if (lu_ret.status != EXCEPTION_NONE) {
+        return NULL;
+    }
+
+    if (pude_pude_pd_ptr_get_present(lu_ret.pudSlot) &&
+        (pude_pude_pd_ptr_get_pd_base_address(lu_ret.pudSlot) == pptr_to_paddr(pd))) {
+        return lu_ret.pudSlot;
+    }
+
+    return NULL;
 }
 
-static inline void setASIDMap(asid_pool_t *poolPtr, asid_t asid, asid_map_t asid_map)
+#ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
+
+static asid_map_t *getMapRefForASID(asid_t asid)
 {
+    asid_pool_t *poolPtr;
+
+    poolPtr = armKSASIDTable[asid >> asidLowBits];
     assert(poolPtr != NULL);
-    poolPtr->array[asid & MASK(asidLowBits)] = asid_map;
+
+    return &poolPtr->array[asid & MASK(asidLowBits)];
 }
 
 static void invalidateASID(asid_t asid)
 {
-    asid_pool_t *poolPtr;
-    asid_map_t asid_map;
+    asid_map_t *asid_map;
 
-    poolPtr = getPoolPtr(asid);
-    asid_map = getASIDMap(poolPtr, asid);
-    assert(asid_map_get_type(asid_map) == asid_map_asid_map_vspace);
-
-    asid_map = asid_map_asid_map_vspace_set_stored_hw_vmid(asid_map, 0);
-    asid_map = asid_map_asid_map_vspace_set_stored_vmid_valid(asid_map, false);
+    asid_map = getMapRefForASID(asid);
+    assert(asid_map_get_type(*asid_map) == asid_map_asid_map_vspace);
 
-    setASIDMap(poolPtr, asid, asid_map);
+    asid_map_asid_map_vspace_ptr_set_stored_hw_vmid(asid_map, 0);
+    asid_map_asid_map_vspace_ptr_set_stored_vmid_valid(asid_map, false);
 }
 
 static void storeHWASID(asid_t asid, hw_asid_t hw_asid)
 {
-    asid_pool_t *poolPtr;
-    asid_map_t asid_map;
+    asid_map_t *asid_map;
 
-    poolPtr = getPoolPtr(asid);
-    asid_map = getASIDMap(poolPtr, asid);
-    assert(asid_map_get_type(asid_map) == asid_map_asid_map_vspace);
+    asid_map = getMapRefForASID(asid);
+    assert(asid_map_get_type(*asid_map) == asid_map_asid_map_vspace);
 
-    asid_map = asid_map_asid_map_vspace_set_stored_hw_vmid(asid_map, hw_asid);
-    asid_map = asid_map_asid_map_vspace_set_stored_vmid_valid(asid_map, true);
+    asid_map_asid_map_vspace_ptr_set_stored_hw_vmid(asid_map, hw_asid);
+    asid_map_asid_map_vspace_ptr_set_stored_vmid_valid(asid_map, true);
 
-    setASIDMap(poolPtr, asid, asid_map);
     armKSHWASIDTable[hw_asid] = asid;
 }
 
@@ -1001,67 +1259,134 @@ static inline void invalidateTLBByASIDVA(asid_t asid, vptr_t vaddr)
 #endif
 }
 
+pde_t *pageTableMapped(asid_t asid, vptr_t vaddr, pte_t *pt)
+{
+    findVSpaceForASID_ret_t find_ret;
+    lookupPDSlot_ret_t lu_ret;
+
+    find_ret = findVSpaceForASID(asid);
+    if (find_ret.status != EXCEPTION_NONE) {
+        return NULL;
+    }
+
+    lu_ret = lookupPDSlot(find_ret.vspace_root, vaddr);
+    if (lu_ret.status != EXCEPTION_NONE) {
+        return NULL;
+    }
+
+    if (pde_pde_small_ptr_get_present(lu_ret.pdSlot) &&
+        (pde_pde_small_ptr_get_pt_base_address(lu_ret.pdSlot) == pptr_to_paddr(pt))) {
+        return lu_ret.pdSlot;
+    }
+
+    return NULL;
+}
 
-void unmapPageTable(asid_t asid, vptr_t vptr, pte_t *target_pt)
+void unmapPageUpperDirectory(asid_t asid, vptr_t vaddr, pude_t *pud)
 {
-    findVSpaceForASID_ret_t find_ret = findVSpaceForASID(asid);
-    if (unlikely(find_ret.status != EXCEPTION_NONE)) {
-        /* nothing to do */
-        return;
+    pgde_t *pgdSlot;
+
+    pgdSlot = pageUpperDirectoryMapped(asid, vaddr, pud);
+    if (likely(pgdSlot != NULL)) {
+        *pgdSlot = pgde_pgde_invalid_new();
+        cleanByVA_PoU((vptr_t)pgdSlot, pptr_to_paddr(pgdSlot));
+        invalidateTLBByASID(asid);
     }
-    pte_t *ptSlot = NULL;
-    pte_t *pt = (pte_t *)find_ret.vspace_root;
+}
 
-    for (word_t i = 0; i < UPT_LEVELS - 1 && pt != target_pt; i++) {
-        ptSlot = pt + GET_UPT_INDEX(vptr, i);
-        if (unlikely(!pte_pte_table_ptr_get_present(ptSlot))) {
-            /* couldn't find it */
-            return;
-        }
-        pt = paddr_to_pptr(pte_pte_table_ptr_get_pt_base_address(ptSlot));
+void unmapPageDirectory(asid_t asid, vptr_t vaddr, pde_t *pd)
+{
+    pude_t *pudSlot;
+
+    pudSlot = pageDirectoryMapped(asid, vaddr, pd);
+    if (likely(pudSlot != NULL)) {
+        *pudSlot = pude_invalid_new();
+
+        cleanByVA_PoU((vptr_t)pudSlot, pptr_to_paddr(pudSlot));
+        invalidateTLBByASID(asid);
     }
+}
 
-    if (pt != target_pt) {
-        /* didn't find it */
-        return;
+void unmapPageTable(asid_t asid, vptr_t vaddr, pte_t *pt)
+{
+    pde_t *pdSlot;
+
+    pdSlot = pageTableMapped(asid, vaddr, pt);
+    if (likely(pdSlot != NULL)) {
+        *pdSlot = pde_invalid_new();
+
+        cleanByVA_PoU((vptr_t)pdSlot, pptr_to_paddr(pdSlot));
+        invalidateTLBByASID(asid);
     }
-    /* If we found a pt then ptSlot won't be null */
-    assert(ptSlot != NULL);
-    *ptSlot = pte_pte_invalid_new();
-    cleanByVA_PoU((vptr_t)ptSlot, pptr_to_paddr(ptSlot));
-    invalidateTLBByASID(asid);
 }
 
 void unmapPage(vm_page_size_t page_size, asid_t asid, vptr_t vptr, pptr_t pptr)
 {
+    paddr_t addr;
     findVSpaceForASID_ret_t find_ret;
-    lookupPTSlot_ret_t  lu_ret;
-    pte_t pte;
 
+    addr = pptr_to_paddr((void *)pptr);
     find_ret = findVSpaceForASID(asid);
-    if (find_ret.status != EXCEPTION_NONE) {
+    if (unlikely(find_ret.status != EXCEPTION_NONE)) {
         return;
     }
 
-    lu_ret = lookupPTSlot(find_ret.vspace_root, vptr);
-    if (unlikely(lu_ret.ptBitsLeft != pageBitsForSize(page_size))) {
-        /* Do nothing if the wrong size object was returned */
-        return;
+    switch (page_size) {
+    case ARMSmallPage: {
+        lookupPTSlot_ret_t lu_ret;
+
+        lu_ret = lookupPTSlot(find_ret.vspace_root, vptr);
+        if (unlikely(lu_ret.status != EXCEPTION_NONE)) {
+            return;
+        }
+
+        if (pte_ptr_get_present(lu_ret.ptSlot) &&
+            pte_ptr_get_page_base_address(lu_ret.ptSlot) == addr) {
+            *(lu_ret.ptSlot) = pte_invalid_new();
+
+            cleanByVA_PoU((vptr_t)lu_ret.ptSlot, pptr_to_paddr(lu_ret.ptSlot));
+        }
+        break;
     }
 
-    pte = *(lu_ret.ptSlot);
-    if (!pte_is_page_type(pte)) {
-        /* Do nothing if no page is present */
-        return;
+    case ARMLargePage: {
+        lookupPDSlot_ret_t lu_ret;
+
+        lu_ret = lookupPDSlot(find_ret.vspace_root, vptr);
+        if (unlikely(lu_ret.status != EXCEPTION_NONE)) {
+            return;
+        }
+
+        if (pde_pde_large_ptr_get_present(lu_ret.pdSlot) &&
+            pde_pde_large_ptr_get_page_base_address(lu_ret.pdSlot) == addr) {
+            *(lu_ret.pdSlot) = pde_invalid_new();
+
+            cleanByVA_PoU((vptr_t)lu_ret.pdSlot, pptr_to_paddr(lu_ret.pdSlot));
+        }
+        break;
     }
 
-    if (pte_get_page_base_address(pte) != pptr_to_paddr((void *)pptr)) {
-        /* Do nothing if the mapped page is not the same physical frame */
-        return;
+    case ARMHugePage: {
+        lookupPUDSlot_ret_t lu_ret;
+
+        lu_ret = lookupPUDSlot(find_ret.vspace_root, vptr);
+        if (unlikely(lu_ret.status != EXCEPTION_NONE)) {
+            return;
+        }
+
+        if (pude_pude_1g_ptr_get_present(lu_ret.pudSlot) &&
+            pude_pude_1g_ptr_get_page_base_address(lu_ret.pudSlot) == addr) {
+            *(lu_ret.pudSlot) = pude_invalid_new();
+
+            cleanByVA_PoU((vptr_t)lu_ret.pudSlot, pptr_to_paddr(lu_ret.pudSlot));
+        }
+        break;
+    }
+
+    default:
+        fail("Invalid ARM page type");
     }
 
-    *(lu_ret.ptSlot) = pte_pte_invalid_new();
-    cleanByVA_PoU((vptr_t)lu_ret.ptSlot, pptr_to_paddr(lu_ret.ptSlot));
     assert(asid < BIT(16));
     invalidateTLBByASIDVA(asid, vptr);
 }
@@ -1107,7 +1432,7 @@ void deleteASIDPool(asid_t asid_base, asid_pool_t *pool)
     }
 }
 
-static void doFlush(word_t invLabel, vptr_t start, vptr_t end, paddr_t pstart)
+static void doFlush(int invLabel, vptr_t start, vptr_t end, paddr_t pstart)
 {
     switch (invLabel) {
     case ARMVSpaceClean_Data:
@@ -1144,7 +1469,7 @@ static void doFlush(word_t invLabel, vptr_t start, vptr_t end, paddr_t pstart)
 
 /* ================= INVOCATION HANDLING STARTS HERE ================== */
 
-static exception_t performVSpaceFlush(word_t invLabel, vspace_root_t *vspaceRoot, asid_t asid,
+static exception_t performVSpaceFlush(int invLabel, vspace_root_t *vspaceRoot, asid_t asid,
                                       vptr_t start, vptr_t end, paddr_t pstart)
 {
 
@@ -1170,12 +1495,57 @@ static exception_t performVSpaceFlush(word_t invLabel, vspace_root_t *vspaceRoot
     return EXCEPTION_NONE;
 }
 
+#ifndef AARCH64_VSPACE_S2_START_L1
+static exception_t performUpperPageDirectoryInvocationMap(cap_t cap, cte_t *ctSlot, pgde_t pgde, pgde_t *pgdSlot)
+{
+    ctSlot->cap = cap;
+    *pgdSlot = pgde;
+    cleanByVA_PoU((vptr_t)pgdSlot, pptr_to_paddr(pgdSlot));
+
+    return EXCEPTION_NONE;
+}
+
+static exception_t performUpperPageDirectoryInvocationUnmap(cap_t cap, cte_t *ctSlot)
+{
+    if (cap_page_upper_directory_cap_get_capPUDIsMapped(cap)) {
+        pude_t *pud = PUD_PTR(cap_page_upper_directory_cap_get_capPUDBasePtr(cap));
+        unmapPageUpperDirectory(cap_page_upper_directory_cap_get_capPUDMappedASID(cap),
+                                cap_page_upper_directory_cap_get_capPUDMappedAddress(cap), pud);
+        clearMemory_PT((void *)pud, cap_get_capSizeBits(cap));
+    }
+
+    cap_page_upper_directory_cap_ptr_set_capPUDIsMapped(&(ctSlot->cap), 0);
+    return EXCEPTION_NONE;
+}
+#endif
 
-static exception_t performPageTableInvocationMap(cap_t cap, cte_t *ctSlot, pte_t pte, pte_t *ptSlot)
+static exception_t performPageDirectoryInvocationMap(cap_t cap, cte_t *ctSlot, pude_t pude, pude_t *pudSlot)
 {
     ctSlot->cap = cap;
-    *ptSlot = pte;
-    cleanByVA_PoU((vptr_t)ptSlot, pptr_to_paddr(ptSlot));
+    *pudSlot = pude;
+    cleanByVA_PoU((vptr_t)pudSlot, pptr_to_paddr(pudSlot));
+
+    return EXCEPTION_NONE;
+}
+
+static exception_t performPageDirectoryInvocationUnmap(cap_t cap, cte_t *ctSlot)
+{
+    if (cap_page_directory_cap_get_capPDIsMapped(cap)) {
+        pde_t *pd = PD_PTR(cap_page_directory_cap_get_capPDBasePtr(cap));
+        unmapPageDirectory(cap_page_directory_cap_get_capPDMappedASID(cap),
+                           cap_page_directory_cap_get_capPDMappedAddress(cap), pd);
+        clearMemory_PT((void *)pd, cap_get_capSizeBits(cap));
+    }
+
+    cap_page_directory_cap_ptr_set_capPDIsMapped(&(ctSlot->cap), 0);
+    return EXCEPTION_NONE;
+}
+
+static exception_t performPageTableInvocationMap(cap_t cap, cte_t *ctSlot, pde_t pde, pde_t *pdSlot)
+{
+    ctSlot->cap = cap;
+    *pdSlot = pde;
+    cleanByVA_PoU((vptr_t)pdSlot, pptr_to_paddr(pdSlot));
 
     return EXCEPTION_NONE;
 }
@@ -1193,10 +1563,44 @@ static exception_t performPageTableInvocationUnmap(cap_t cap, cte_t *ctSlot)
     return EXCEPTION_NONE;
 }
 
-static exception_t performPageInvocationMap(asid_t asid, cap_t cap, cte_t *ctSlot,
-                                            pte_t pte, pte_t *ptSlot)
+static exception_t performHugePageInvocationMap(asid_t asid, cap_t cap, cte_t *ctSlot,
+                                                pude_t pude, pude_t *pudSlot)
 {
-    bool_t tlbflush_required = pte_ptr_get_valid(ptSlot);
+    bool_t tlbflush_required = pude_pude_1g_ptr_get_present(pudSlot);
+
+    ctSlot->cap = cap;
+    *pudSlot = pude;
+
+    cleanByVA_PoU((vptr_t)pudSlot, pptr_to_paddr(pudSlot));
+    if (unlikely(tlbflush_required)) {
+        assert(asid < BIT(16));
+        invalidateTLBByASIDVA(asid, cap_frame_cap_get_capFMappedAddress(cap));
+    }
+
+    return EXCEPTION_NONE;
+}
+
+static exception_t performLargePageInvocationMap(asid_t asid, cap_t cap, cte_t *ctSlot,
+                                                 pde_t pde, pde_t *pdSlot)
+{
+    bool_t tlbflush_required = pde_pde_large_ptr_get_present(pdSlot);
+
+    ctSlot->cap = cap;
+    *pdSlot = pde;
+
+    cleanByVA_PoU((vptr_t)pdSlot, pptr_to_paddr(pdSlot));
+    if (unlikely(tlbflush_required)) {
+        assert(asid < BIT(16));
+        invalidateTLBByASIDVA(asid, cap_frame_cap_get_capFMappedAddress(cap));
+    }
+
+    return EXCEPTION_NONE;
+}
+
+static exception_t performSmallPageInvocationMap(asid_t asid, cap_t cap, cte_t *ctSlot,
+                                                 pte_t pte, pte_t *ptSlot)
+{
+    bool_t tlbflush_required = pte_ptr_get_present(ptSlot);
 
     ctSlot->cap = cap;
     *ptSlot = pte;
@@ -1220,16 +1624,12 @@ static exception_t performPageInvocationUnmap(cap_t cap, cte_t *ctSlot)
                   cap_frame_cap_get_capFBasePtr(cap));
     }
 
-    cap_t slotCap = ctSlot->cap;
-    slotCap = cap_frame_cap_set_capFMappedAddress(slotCap, 0);
-    slotCap = cap_frame_cap_set_capFMappedASID(slotCap, asidInvalid);
-    ctSlot->cap = slotCap;
-
-
+    cap_frame_cap_ptr_set_capFMappedASID(&ctSlot->cap, asidInvalid);
+    cap_frame_cap_ptr_set_capFMappedAddress(&ctSlot->cap, 0);
     return EXCEPTION_NONE;
 }
 
-static exception_t performPageFlush(word_t invLabel, vspace_root_t *vspaceRoot, asid_t asid,
+static exception_t performPageFlush(int invLabel, vspace_root_t *vspaceRoot, asid_t asid,
                                     vptr_t start, vptr_t end, paddr_t pstart)
 {
     if (config_set(CONFIG_ARM_HYPERVISOR_SUPPORT)) {
@@ -1278,13 +1678,10 @@ static exception_t performPageGetAddress(pptr_t base_ptr, bool_t call)
 static exception_t performASIDControlInvocation(void *frame, cte_t *slot,
                                                 cte_t *parent, asid_t asid_base)
 {
-    /** AUXUPD: "(True, typ_region_bytes (ptr_val \<acute>frame) 12)" */
-    /** GHOSTUPD: "(True, gs_clear_region (ptr_val \<acute>frame) 12)" */
     cap_untyped_cap_ptr_set_capFreeIndex(&(parent->cap),
                                          MAX_FREE_INDEX(cap_untyped_cap_get_capBlockSize(parent->cap)));
 
     memzero(frame, BIT(seL4_ASIDPoolBits));
-    /** AUXUPD: "(True, ptr_retyps 1 (Ptr (ptr_val \<acute>frame) :: asid_pool_C ptr))" */
 
     cteInsert(
         cap_asid_pool_cap_new(
@@ -1298,16 +1695,15 @@ static exception_t performASIDControlInvocation(void *frame, cte_t *slot,
     return EXCEPTION_NONE;
 }
 
-static exception_t decodeARMVSpaceRootInvocation(word_t invLabel, word_t length,
+static exception_t decodeARMVSpaceRootInvocation(word_t invLabel, unsigned int length,
                                                  cte_t *cte, cap_t cap, word_t *buffer)
 {
     vptr_t start, end;
     paddr_t pstart;
     asid_t asid;
     vspace_root_t *vspaceRoot;
-    lookupPTSlot_ret_t resolve_ret;
+    lookupFrame_ret_t resolve_ret;
     findVSpaceForASID_ret_t find_ret;
-    pte_t pte;
 
     switch (invLabel) {
     case ARMVSpaceClean_Data:
@@ -1346,8 +1742,8 @@ static exception_t decodeARMVSpaceRootInvocation(word_t invLabel, word_t length,
         }
 
         /* Make sure that the supplied pgd is ok */
-        vspaceRoot = VSPACE_PTR(cap_vspace_cap_get_capVSBasePtr(cap));
-        asid = cap_vspace_cap_get_capVSMappedASID(cap);
+        vspaceRoot = cap_vtable_root_get_basePtr(cap);
+        asid = cap_vtable_root_get_mappedASID(cap);
 
         find_ret = findVSpaceForASID(asid);
         if (unlikely(find_ret.status != EXCEPTION_NONE)) {
@@ -1365,12 +1761,9 @@ static exception_t decodeARMVSpaceRootInvocation(word_t invLabel, word_t length,
         }
 
         /* Look up the frame containing 'start'. */
-        resolve_ret = lookupPTSlot(vspaceRoot, start);
-        pte = *resolve_ret.ptSlot;
-
-        /* Check that the returned slot is a page. */
-        if (!pte_is_page_type(pte)) {
+        resolve_ret = lookupFrame(vspaceRoot, start);
 
+        if (!resolve_ret.valid) {
             /* Fail silently, as there can't be any stale cached data (for the
              * given address space), and getting a syscall error because the
              * relevant page is non-resident would be 'astonishing'. */
@@ -1379,47 +1772,215 @@ static exception_t decodeARMVSpaceRootInvocation(word_t invLabel, word_t length,
         }
 
         /* Refuse to cross a page boundary. */
-        if (ROUND_DOWN(start, resolve_ret.ptBitsLeft) != ROUND_DOWN(end - 1, resolve_ret.ptBitsLeft)) {
+        if (PAGE_BASE(start, resolve_ret.frameSize) != PAGE_BASE(end - 1, resolve_ret.frameSize)) {
             current_syscall_error.type = seL4_RangeError;
             current_syscall_error.rangeErrorMin = start;
-            current_syscall_error.rangeErrorMax = ROUND_DOWN(start, resolve_ret.ptBitsLeft) +
-                                                  MASK(resolve_ret.ptBitsLeft);
+            current_syscall_error.rangeErrorMax = PAGE_BASE(start, resolve_ret.frameSize) +
+                                                  MASK(pageBitsForSize(resolve_ret.frameSize));
             return EXCEPTION_SYSCALL_ERROR;
         }
 
-#ifndef CONFIG_ARM_HYPERVISOR_SUPPORT
-        /* When in EL1, the mapping must be write-able for ARMVSpaceInvalidate_Data */
-        if (invLabel == ARMVSpaceInvalidate_Data && vmRightsFromPTE(pte) != VMReadWrite) {
-            userError("ARMVSpaceInvalidate_Data: Cannot call on mapping without write rights.");
-            current_syscall_error.type = seL4_IllegalOperation;
+        /* Calculate the physical start address. */
+        pstart = resolve_ret.frameBase + PAGE_OFFSET(start, resolve_ret.frameSize);
+
+        setThreadState(NODE_STATE(ksCurThread), ThreadState_Restart);
+        return performVSpaceFlush(invLabel, vspaceRoot, asid, start, end - 1, pstart);
+
+    default:
+        current_syscall_error.type = seL4_IllegalOperation;
+        return EXCEPTION_SYSCALL_ERROR;
+    }
+}
+
+#ifndef AARCH64_VSPACE_S2_START_L1
+static exception_t decodeARMPageUpperDirectoryInvocation(word_t invLabel, unsigned int length,
+                                                         cte_t *cte, cap_t cap, word_t *buffer)
+{
+    cap_t pgdCap;
+    vspace_root_t *pgd;
+    pgde_t pgde;
+    asid_t asid;
+    vptr_t vaddr;
+    lookupPGDSlot_ret_t pgdSlot;
+    findVSpaceForASID_ret_t find_ret;
+
+    if (invLabel == ARMPageUpperDirectoryUnmap) {
+        if (unlikely(!isFinalCapability(cte))) {
+            current_syscall_error.type = seL4_RevokeFirst;
             return EXCEPTION_SYSCALL_ERROR;
         }
+
+        setThreadState(NODE_STATE(ksCurThread), ThreadState_Restart);
+        return performUpperPageDirectoryInvocationUnmap(cap, cte);
+    }
+
+    if (unlikely(invLabel != ARMPageUpperDirectoryMap)) {
+        current_syscall_error.type = seL4_IllegalOperation;
+        return EXCEPTION_SYSCALL_ERROR;
+    }
+
+    if (unlikely(length < 2 || current_extra_caps.excaprefs[0] == NULL)) {
+        current_syscall_error.type = seL4_TruncatedMessage;
+        return EXCEPTION_SYSCALL_ERROR;
+    }
+
+    if (unlikely(cap_page_upper_directory_cap_get_capPUDIsMapped(cap))) {
+        current_syscall_error.type = seL4_InvalidCapability;
+        current_syscall_error.invalidCapNumber = 0;
+        return EXCEPTION_SYSCALL_ERROR;
+    }
+
+    vaddr = getSyscallArg(0, buffer) & (~MASK(PGD_INDEX_OFFSET));
+    pgdCap = current_extra_caps.excaprefs[0]->cap;
+
+    if (unlikely(!isValidNativeRoot(pgdCap))) {
+        current_syscall_error.type = seL4_InvalidCapability;
+        current_syscall_error.invalidCapNumber = 1;
+        return EXCEPTION_SYSCALL_ERROR;
+    }
+
+    pgd = cap_vtable_root_get_basePtr(pgdCap);
+    asid = cap_vtable_root_get_mappedASID(pgdCap);
+
+    if (unlikely(vaddr > USER_TOP)) {
+        current_syscall_error.type = seL4_InvalidArgument;
+        current_syscall_error.invalidArgumentNumber = 0;
+        return EXCEPTION_SYSCALL_ERROR;
+    }
+
+    find_ret = findVSpaceForASID(asid);
+    if (unlikely(find_ret.status != EXCEPTION_NONE)) {
+        current_syscall_error.type = seL4_FailedLookup;
+        current_syscall_error.failedLookupWasSource = false;
+        return EXCEPTION_SYSCALL_ERROR;
+    }
+
+    if (unlikely(find_ret.vspace_root != pgd)) {
+        current_syscall_error.type = seL4_InvalidCapability;
+        current_syscall_error.invalidCapNumber = 1;
+        return EXCEPTION_SYSCALL_ERROR;
+    }
+
+    pgdSlot = lookupPGDSlot(pgd, vaddr);
+
+    if (unlikely(pgde_pgde_pud_ptr_get_present(pgdSlot.pgdSlot))) {
+        current_syscall_error.type = seL4_DeleteFirst;
+        return EXCEPTION_SYSCALL_ERROR;
+    }
+
+    pgde = pgde_pgde_pud_new(
+               pptr_to_paddr(PUDE_PTR(cap_page_upper_directory_cap_get_capPUDBasePtr(cap))));
+
+    cap_page_upper_directory_cap_ptr_set_capPUDIsMapped(&cap, 1);
+    cap_page_upper_directory_cap_ptr_set_capPUDMappedASID(&cap, asid);
+    cap_page_upper_directory_cap_ptr_set_capPUDMappedAddress(&cap, vaddr);
+
+    setThreadState(NODE_STATE(ksCurThread), ThreadState_Restart);
+    return performUpperPageDirectoryInvocationMap(cap, cte, pgde, pgdSlot.pgdSlot);
+}
 #endif
 
-        /* Calculate the physical start address. */
-        paddr_t frame_base = pte_get_page_base_address(pte);
+static exception_t decodeARMPageDirectoryInvocation(word_t invLabel, unsigned int length,
+                                                    cte_t *cte, cap_t cap, word_t *buffer)
+{
+    cap_t vspaceRootCap;
+    vspace_root_t *vspaceRoot;
+    pude_t pude;
+    asid_t asid;
+    vptr_t vaddr;
+    lookupPUDSlot_ret_t pudSlot;
+    findVSpaceForASID_ret_t find_ret;
 
-        pstart = frame_base + (start & MASK(resolve_ret.ptBitsLeft));
+    if (invLabel == ARMPageDirectoryUnmap) {
+        if (unlikely(!isFinalCapability(cte))) {
+            current_syscall_error.type = seL4_RevokeFirst;
+            return EXCEPTION_SYSCALL_ERROR;
+        }
 
         setThreadState(NODE_STATE(ksCurThread), ThreadState_Restart);
-        return performVSpaceFlush(invLabel, vspaceRoot, asid, start, end - 1, pstart);
+        return performPageDirectoryInvocationUnmap(cap, cte);
+    }
 
-    default:
+    if (unlikely(invLabel != ARMPageDirectoryMap)) {
         current_syscall_error.type = seL4_IllegalOperation;
         return EXCEPTION_SYSCALL_ERROR;
     }
-}
 
+    if (unlikely(length < 2 || current_extra_caps.excaprefs[0] == NULL)) {
+        current_syscall_error.type = seL4_TruncatedMessage;
+        return EXCEPTION_SYSCALL_ERROR;
+    }
+
+    if (unlikely(cap_page_directory_cap_get_capPDIsMapped(cap))) {
+        current_syscall_error.type = seL4_InvalidCapability;
+        current_syscall_error.invalidCapNumber = 0;
+        return EXCEPTION_SYSCALL_ERROR;
+    }
+
+    vaddr = getSyscallArg(0, buffer) & (~MASK(PUD_INDEX_OFFSET));
+    vspaceRootCap = current_extra_caps.excaprefs[0]->cap;
+
+    if (unlikely(!isValidNativeRoot(vspaceRootCap))) {
+        current_syscall_error.type = seL4_InvalidCapability;
+        current_syscall_error.invalidCapNumber = 1;
+        return EXCEPTION_SYSCALL_ERROR;
+    }
+
+    vspaceRoot = cap_vtable_root_get_basePtr(vspaceRootCap);
+    asid = cap_vtable_root_get_mappedASID(vspaceRootCap);
+
+    if (unlikely(vaddr > USER_TOP)) {
+        current_syscall_error.type = seL4_InvalidArgument;
+        current_syscall_error.invalidArgumentNumber = 0;
+        return EXCEPTION_SYSCALL_ERROR;
+    }
+
+    find_ret = findVSpaceForASID(asid);
+    if (unlikely(find_ret.status != EXCEPTION_NONE)) {
+        current_syscall_error.type = seL4_FailedLookup;
+        current_syscall_error.failedLookupWasSource = false;
+        return EXCEPTION_SYSCALL_ERROR;
+    }
+
+    if (unlikely(find_ret.vspace_root != vspaceRoot)) {
+        current_syscall_error.type = seL4_InvalidCapability;
+        current_syscall_error.invalidCapNumber = 1;
+        return EXCEPTION_SYSCALL_ERROR;
+    }
+
+    pudSlot = lookupPUDSlot(vspaceRoot, vaddr);
 
-static exception_t decodeARMPageTableInvocation(word_t invLabel, word_t length,
+    if (pudSlot.status != EXCEPTION_NONE) {
+        current_syscall_error.type = seL4_FailedLookup;
+        current_syscall_error.failedLookupWasSource = false;
+        return EXCEPTION_SYSCALL_ERROR;
+    }
+
+    if (unlikely(pude_pude_pd_ptr_get_present(pudSlot.pudSlot) ||
+                 pude_pude_1g_ptr_get_present(pudSlot.pudSlot))) {
+        current_syscall_error.type = seL4_DeleteFirst;
+        return EXCEPTION_SYSCALL_ERROR;
+    }
+
+    pude = pude_pude_pd_new(pptr_to_paddr(PDE_PTR(cap_page_directory_cap_get_capPDBasePtr(cap))));
+
+    cap_page_directory_cap_ptr_set_capPDIsMapped(&cap, 1);
+    cap_page_directory_cap_ptr_set_capPDMappedASID(&cap, asid);
+    cap_page_directory_cap_ptr_set_capPDMappedAddress(&cap, vaddr);
+
+    setThreadState(NODE_STATE(ksCurThread), ThreadState_Restart);
+    return performPageDirectoryInvocationMap(cap, cte, pude, pudSlot.pudSlot);
+}
+
+static exception_t decodeARMPageTableInvocation(word_t invLabel, unsigned int length,
                                                 cte_t *cte, cap_t cap, word_t *buffer)
 {
     cap_t vspaceRootCap;
     vspace_root_t *vspaceRoot;
-    pte_t pte;
+    pde_t pde;
     asid_t asid;
     vptr_t vaddr;
-    lookupPTSlot_ret_t ptSlot;
+    lookupPDSlot_ret_t pdSlot;
     findVSpaceForASID_ret_t find_ret;
 
     if (invLabel == ARMPageTableUnmap) {
@@ -1448,7 +2009,7 @@ static exception_t decodeARMPageTableInvocation(word_t invLabel, word_t length,
         return EXCEPTION_SYSCALL_ERROR;
     }
 
-    vaddr = getSyscallArg(0, buffer);
+    vaddr = getSyscallArg(0, buffer) & (~MASK(PD_INDEX_OFFSET));
     vspaceRootCap = current_extra_caps.excaprefs[0]->cap;
 
     if (unlikely(!isValidNativeRoot(vspaceRootCap))) {
@@ -1457,8 +2018,8 @@ static exception_t decodeARMPageTableInvocation(word_t invLabel, word_t length,
         return EXCEPTION_SYSCALL_ERROR;
     }
 
-    vspaceRoot = VSPACE_PTR(cap_vspace_cap_get_capVSBasePtr(vspaceRootCap));
-    asid = cap_vspace_cap_get_capVSMappedASID(vspaceRootCap);
+    vspaceRoot = cap_vtable_root_get_basePtr(vspaceRootCap);
+    asid = cap_vtable_root_get_mappedASID(vspaceRootCap);
 
     if (unlikely(vaddr > USER_TOP)) {
         current_syscall_error.type = seL4_InvalidArgument;
@@ -1479,29 +2040,31 @@ static exception_t decodeARMPageTableInvocation(word_t invLabel, word_t length,
         return EXCEPTION_SYSCALL_ERROR;
     }
 
-    ptSlot = lookupPTSlot(vspaceRoot, vaddr);
+    pdSlot = lookupPDSlot(vspaceRoot, vaddr);
+
+    if (pdSlot.status != EXCEPTION_NONE) {
+        current_syscall_error.type = seL4_FailedLookup;
+        current_syscall_error.failedLookupWasSource = false;
+        return EXCEPTION_SYSCALL_ERROR;
+    }
 
-    if (unlikely(ptSlot.ptBitsLeft == seL4_PageBits || pte_ptr_get_valid(ptSlot.ptSlot))) {
+    if (unlikely(pde_pde_small_ptr_get_present(pdSlot.pdSlot) ||
+                 pde_pde_large_ptr_get_present(pdSlot.pdSlot))) {
         current_syscall_error.type = seL4_DeleteFirst;
         return EXCEPTION_SYSCALL_ERROR;
     }
 
-    pte = pte_pte_table_new(pptr_to_paddr(PTE_PTR(cap_page_table_cap_get_capPTBasePtr(cap))));
+    pde = pde_pde_small_new(pptr_to_paddr(PTE_PTR(cap_page_table_cap_get_capPTBasePtr(cap))));
 
-    cap = cap_page_table_cap_set_capPTIsMapped(cap, 1);
-    cap = cap_page_table_cap_set_capPTMappedASID(cap, asid);
-    cap = cap_page_table_cap_set_capPTMappedAddress(cap, (vaddr & ~MASK(ptSlot.ptBitsLeft)));
+    cap_page_table_cap_ptr_set_capPTIsMapped(&cap, 1);
+    cap_page_table_cap_ptr_set_capPTMappedASID(&cap, asid);
+    cap_page_table_cap_ptr_set_capPTMappedAddress(&cap, vaddr);
 
     setThreadState(NODE_STATE(ksCurThread), ThreadState_Restart);
-    return performPageTableInvocationMap(cap, cte, pte, ptSlot.ptSlot);
+    return performPageTableInvocationMap(cap, cte, pde, pdSlot.pdSlot);
 }
 
-static inline bool_t CONST checkVPAlignment(vm_page_size_t sz, word_t w)
-{
-    return (w & MASK(pageBitsForSize(sz))) == 0;
-}
-
-static exception_t decodeARMFrameInvocation(word_t invLabel, word_t length,
+static exception_t decodeARMFrameInvocation(word_t invLabel, unsigned int length,
                                             cte_t *cte, cap_t cap, bool_t call, word_t *buffer)
 {
     switch (invLabel) {
@@ -1535,8 +2098,8 @@ static exception_t decodeARMFrameInvocation(word_t invLabel, word_t length,
             return EXCEPTION_SYSCALL_ERROR;
         }
 
-        vspaceRoot = VSPACE_PTR(cap_vspace_cap_get_capVSBasePtr(vspaceRootCap));
-        asid = cap_vspace_cap_get_capVSMappedASID(vspaceRootCap);
+        vspaceRoot = cap_vtable_root_get_basePtr(vspaceRootCap);
+        asid = cap_vtable_root_get_mappedASID(vspaceRootCap);
 
         find_ret = findVSpaceForASID(asid);
         if (unlikely(find_ret.status != EXCEPTION_NONE)) {
@@ -1551,25 +2114,25 @@ static exception_t decodeARMFrameInvocation(word_t invLabel, word_t length,
             return EXCEPTION_SYSCALL_ERROR;
         }
 
-        if (unlikely(!checkVPAlignment(frameSize, vaddr))) {
+        if (unlikely(!IS_PAGE_ALIGNED(vaddr, frameSize))) {
             current_syscall_error.type = seL4_AlignmentError;
             return EXCEPTION_SYSCALL_ERROR;
         }
 
         /* In the case of remap, the cap should have a valid asid */
-        frame_asid = cap_frame_cap_get_capFMappedASID(cap);
+        frame_asid = cap_frame_cap_ptr_get_capFMappedASID(&cap);
 
         if (frame_asid != asidInvalid) {
             if (frame_asid != asid) {
                 userError("ARMPageMap: Attempting to remap a frame that does not belong to the passed address space");
                 current_syscall_error.type = seL4_InvalidCapability;
-                current_syscall_error.invalidCapNumber = 1;
+                current_syscall_error.invalidArgumentNumber = 0;
                 return EXCEPTION_SYSCALL_ERROR;
 
             } else if (cap_frame_cap_get_capFMappedAddress(cap) != vaddr) {
                 userError("ARMPageMap: Attempting to map frame into multiple addresses");
                 current_syscall_error.type = seL4_InvalidArgument;
-                current_syscall_error.invalidArgumentNumber = 0;
+                current_syscall_error.invalidArgumentNumber = 2;
                 return EXCEPTION_SYSCALL_ERROR;
             }
         } else {
@@ -1585,17 +2148,45 @@ static exception_t decodeARMFrameInvocation(word_t invLabel, word_t length,
 
         base = pptr_to_paddr((void *)cap_frame_cap_get_capFBasePtr(cap));
 
-        lookupPTSlot_ret_t lu_ret = lookupPTSlot(vspaceRoot, vaddr);
-        if (unlikely(lu_ret.ptBitsLeft != pageBitsForSize(frameSize))) {
-            current_lookup_fault = lookup_fault_missing_capability_new(lu_ret.ptBitsLeft);
-            current_syscall_error.type = seL4_FailedLookup;
-            current_syscall_error.failedLookupWasSource = false;
-            return EXCEPTION_SYSCALL_ERROR;
-        }
+        if (frameSize == ARMSmallPage) {
+            lookupPTSlot_ret_t lu_ret = lookupPTSlot(vspaceRoot, vaddr);
 
-        setThreadState(NODE_STATE(ksCurThread), ThreadState_Restart);
-        return performPageInvocationMap(asid, cap, cte,
-                                        makeUserPagePTE(base, vmRights, attributes, frameSize), lu_ret.ptSlot);
+            if (unlikely(lu_ret.status != EXCEPTION_NONE)) {
+                current_syscall_error.type = seL4_FailedLookup;
+                current_syscall_error.failedLookupWasSource = false;
+                return EXCEPTION_SYSCALL_ERROR;
+            }
+
+            setThreadState(NODE_STATE(ksCurThread), ThreadState_Restart);
+            return performSmallPageInvocationMap(asid, cap, cte,
+                                                 makeUser3rdLevel(base, vmRights, attributes), lu_ret.ptSlot);
+
+        } else if (frameSize == ARMLargePage) {
+            lookupPDSlot_ret_t lu_ret = lookupPDSlot(vspaceRoot, vaddr);
+
+            if (unlikely(lu_ret.status != EXCEPTION_NONE)) {
+                current_syscall_error.type = seL4_FailedLookup;
+                current_syscall_error.failedLookupWasSource = false;
+                return EXCEPTION_SYSCALL_ERROR;
+            }
+
+            setThreadState(NODE_STATE(ksCurThread), ThreadState_Restart);
+            return performLargePageInvocationMap(asid, cap, cte,
+                                                 makeUser2ndLevel(base, vmRights, attributes), lu_ret.pdSlot);
+
+        } else {
+            lookupPUDSlot_ret_t lu_ret = lookupPUDSlot(vspaceRoot, vaddr);
+
+            if (unlikely(lu_ret.status != EXCEPTION_NONE)) {
+                current_syscall_error.type = seL4_FailedLookup;
+                current_syscall_error.failedLookupWasSource = false;
+                return EXCEPTION_SYSCALL_ERROR;
+            }
+
+            setThreadState(NODE_STATE(ksCurThread), ThreadState_Restart);
+            return performHugePageInvocationMap(asid, cap, cte,
+                                                makeUser1stLevel(base, vmRights, attributes), lu_ret.pudSlot);
+        }
     }
 
     case ARMPageUnmap:
@@ -1663,28 +2254,8 @@ static exception_t decodeARMFrameInvocation(word_t invLabel, word_t length,
             current_syscall_error.type = seL4_IllegalOperation;
             return EXCEPTION_SYSCALL_ERROR;
         }
-#else
-        /* When in EL1, we are using the user page table for flushing and need to make sure
-           the mapping info in the cap is not stale. */
-        lookupPTSlot_ret_t lu_ret = lookupPTSlot(find_ret.vspace_root, vaddr);
-        pte_t pte = *lu_ret.ptSlot;
-        void *base_ptr = (void *) cap_frame_cap_get_capFBasePtr(cap);
-        if (unlikely(lu_ret.ptBitsLeft != pageBitsForSize(cap_frame_cap_get_capFSize(cap)) ||
-                     !pte_is_page_type(pte) ||
-                     pte_get_page_base_address(pte) != pptr_to_paddr(base_ptr))) {
-            userError("Page Flush: Attempting to use cap with stale mapping information.");
-            current_syscall_error.type = seL4_InvalidCapability;
-            current_syscall_error.invalidCapNumber = 0;
-            return EXCEPTION_SYSCALL_ERROR;
-        }
-
-        /* When in EL1, the mapping must be writeable for DC IVAC */
-        if (invLabel == ARMPageInvalidate_Data && vmRightsFromPTE(pte) != VMReadWrite) {
-            userError("ARMPageInvalidate_Data: Cannot call on mapping without write rights.");
-            current_syscall_error.type = seL4_IllegalOperation;
-            return EXCEPTION_SYSCALL_ERROR;
-        }
 #endif
+
         setThreadState(NODE_STATE(ksCurThread), ThreadState_Restart);
         return performPageFlush(invLabel, find_ret.vspace_root, asid, vaddr + start, vaddr + end - 1,
                                 pstart);
@@ -1704,8 +2275,15 @@ exception_t decodeARMMMUInvocation(word_t invLabel, word_t length, cptr_t cptr,
                                    cte_t *cte, cap_t cap, bool_t call, word_t *buffer)
 {
     switch (cap_get_capType(cap)) {
-    case cap_vspace_cap:
+    case cap_vtable_root_cap:
         return decodeARMVSpaceRootInvocation(invLabel, length, cte, cap, buffer);
+#ifndef AARCH64_VSPACE_S2_START_L1
+    case cap_page_upper_directory_cap:
+        return decodeARMPageUpperDirectoryInvocation(invLabel, length, cte, cap, buffer);
+#endif
+    case cap_page_directory_cap:
+        return decodeARMPageDirectoryInvocation(invLabel, length, cte, cap, buffer);
+
     case cap_page_table_cap:
         return decodeARMPageTableInvocation(invLabel, length, cte, cap, buffer);
 
@@ -1713,7 +2291,7 @@ exception_t decodeARMMMUInvocation(word_t invLabel, word_t length, cptr_t cptr,
         return decodeARMFrameInvocation(invLabel, length, cte, cap, call, buffer);
 
     case cap_asid_control_cap: {
-        word_t i;
+        unsigned int i;
         asid_t asid_base;
         word_t index, depth;
         cap_t untyped, root;
@@ -1788,7 +2366,7 @@ exception_t decodeARMMMUInvocation(word_t invLabel, word_t length, cptr_t cptr,
         cap_t vspaceCap;
         cte_t *vspaceCapSlot;
         asid_pool_t *pool;
-        word_t i;
+        unsigned int i;
         asid_t asid;
 
         if (unlikely(invLabel != ARMASIDPoolAssign)) {
@@ -1806,7 +2384,7 @@ exception_t decodeARMMMUInvocation(word_t invLabel, word_t length, cptr_t cptr,
         vspaceCapSlot = current_extra_caps.excaprefs[0];
         vspaceCap = vspaceCapSlot->cap;
 
-        if (unlikely(!isVTableRoot(vspaceCap) || cap_vspace_cap_get_capVSIsMapped(vspaceCap))) {
+        if (unlikely(!isVTableRoot(vspaceCap) || cap_vtable_root_isMapped(vspaceCap))) {
             current_syscall_error.type = seL4_InvalidCapability;
             current_syscall_error.invalidCapNumber = 1;
 
@@ -1882,22 +2460,21 @@ typedef struct readWordFromVSpace_ret {
 
 static readWordFromVSpace_ret_t readWordFromVSpace(vspace_root_t *pd, word_t vaddr)
 {
+    lookupFrame_ret_t lookup_frame_ret;
     readWordFromVSpace_ret_t ret;
     word_t offset;
     pptr_t kernel_vaddr;
     word_t *value;
 
-    lookupPTSlot_ret_t lookup_ret = lookupPTSlot(pd, vaddr);
+    lookup_frame_ret = lookupFrame(pd, vaddr);
 
-    /* Check that the returned slot is a page. */
-    if (!pte_ptr_get_valid(lookup_ret.ptSlot) ||
-        (pte_pte_table_ptr_get_present(lookup_ret.ptSlot) && lookup_ret.ptBitsLeft > PAGE_BITS)) {
+    if (!lookup_frame_ret.valid) {
         ret.status = EXCEPTION_LOOKUP_FAULT;
         return ret;
     }
 
-    offset = vaddr & MASK(lookup_ret.ptBitsLeft);
-    kernel_vaddr = (word_t)paddr_to_pptr(pte_page_ptr_get_page_base_address(lookup_ret.ptSlot));
+    offset = vaddr & MASK(pageBitsForSize(lookup_frame_ret.frameSize));
+    kernel_vaddr = (word_t)paddr_to_pptr(lookup_frame_ret.frameBase);
     value = (word_t *)(kernel_vaddr + offset);
 
     ret.status = EXCEPTION_NONE;
@@ -1914,12 +2491,12 @@ void Arch_userStackTrace(tcb_t *tptr)
     threadRoot = TCB_PTR_CTE_PTR(tptr, tcbVTable)->cap;
 
     /* lookup the vspace root */
-    if (cap_get_capType(threadRoot) != cap_vspace_cap) {
+    if (cap_get_capType(threadRoot) != cap_vtable_root_cap) {
         printf("Invalid vspace\n");
         return;
     }
 
-    vspaceRoot = VSPACE_PTR(cap_vspace_cap_get_capVSBasePtr(threadRoot));
+    vspaceRoot = cap_vtable_root_get_basePtr(threadRoot);
     sp = getRegister(tptr, SP_EL0);
 
     /* check for alignment so we don't have to worry about accessing
@@ -1980,7 +2557,7 @@ exception_t benchmark_arch_map_logBuffer(word_t frame_cptr)
 
     ksUserLogBuffer = pptr_to_paddr((void *) frame_pptr);
 
-    *armKSGlobalLogPTE = pte_pte_page_new(
+    *armKSGlobalLogPDE = pde_pde_large_new(
 #ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
                              0, // XN
 #else
@@ -1993,7 +2570,7 @@ exception_t benchmark_arch_map_logBuffer(word_t frame_cptr)
                              0,                         /* VMKernelOnly */
                              NORMAL_WT);
 
-    cleanByVA_PoU((vptr_t)armKSGlobalLogPTE, addrFromKPPtr(armKSGlobalLogPTE));
+    cleanByVA_PoU((vptr_t)armKSGlobalLogPDE, addrFromKPPtr(armKSGlobalLogPDE));
     invalidateTranslationSingle(KS_LOG_PPTR);
     return EXCEPTION_NONE;
 }
diff --git a/src/arch/arm/64/machine/capdl.c b/src/arch/arm/64/machine/capdl.c
index 8445adc6..d530ef5c 100644
--- a/src/arch/arm/64/machine/capdl.c
+++ b/src/arch/arm/64/machine/capdl.c
@@ -19,19 +19,21 @@ word_t get_tcb_sp(tcb_t *tcb)
 
 #ifdef CONFIG_PRINTING
 
-static void obj_frame_print_attrs(vm_page_size_t frameSize, paddr_t frameBase);
+static void obj_frame_print_attrs(lookupFrame_ret_t ret);
+static void cap_frame_print_attrs_pud(pude_t *pudSlot);
+static void cap_frame_print_attrs_pd(pde_t *pdSlot);
 static void cap_frame_print_attrs_pt(pte_t *ptSlot);
 static void cap_frame_print_attrs_impl(word_t SH, word_t AP, word_t NXN);
 static void cap_frame_print_attrs_vptr(word_t vptr, cap_t vspace);
 
 static void _cap_frame_print_attrs_vptr(word_t vptr, vspace_root_t *vspaceRoot);
 
-static void arm64_obj_pt_print_slots(pte_t *pdSlot);
-static void arm64_obj_pd_print_slots(pte_t *pudSlot);
+static void arm64_obj_pt_print_slots(pde_t *pdSlot);
+static void arm64_obj_pd_print_slots(pude_t *pudSlot);
 static void arm64_obj_pud_print_slots(void *pgdSlot_or_vspace);
 
-static void arm64_cap_pt_print_slots(pte_t *pdSlot, vptr_t vptr);
-static void arm64_cap_pd_print_slots(pte_t *pudSlot, vptr_t vptr);
+static void arm64_cap_pt_print_slots(pde_t *pdSlot, vptr_t vptr);
+static void arm64_cap_pd_print_slots(pude_t *pudSlot, vptr_t vptr);
 static void arm64_cap_pud_print_slots(void *pgdSlot_or_vspace, vptr_t vptr);
 
 /* Stage-1 access permissions:
@@ -53,11 +55,25 @@ static void arm64_cap_pud_print_slots(void *pgdSlot_or_vspace, vptr_t vptr);
  *  EL2 still uses the Stage-1 AP format.
  */
 /* use when only have access to pte of frames */
+static void cap_frame_print_attrs_pud(pude_t *pudSlot)
+{
+    cap_frame_print_attrs_impl(pude_pude_1g_ptr_get_SH(pudSlot),
+                               pude_pude_1g_ptr_get_AP(pudSlot),
+                               pude_pude_1g_ptr_get_UXN(pudSlot));
+}
+
+static void cap_frame_print_attrs_pd(pde_t *pdSlot)
+{
+    cap_frame_print_attrs_impl(pde_pde_large_ptr_get_SH(pdSlot),
+                               pde_pde_large_ptr_get_AP(pdSlot),
+                               pde_pde_large_ptr_get_UXN(pdSlot));
+}
+
 static void cap_frame_print_attrs_pt(pte_t *ptSlot)
 {
-    cap_frame_print_attrs_impl(pte_pte_page_ptr_get_SH(ptSlot),
-                               pte_pte_page_ptr_get_AP(ptSlot),
-                               pte_pte_page_ptr_get_UXN(ptSlot));
+    cap_frame_print_attrs_impl(pte_ptr_get_SH(ptSlot),
+                               pte_ptr_get_AP(ptSlot),
+                               pte_ptr_get_UXN(ptSlot));
 }
 
 static void cap_frame_print_attrs_impl(word_t SH, word_t AP, word_t NXN)
@@ -107,32 +123,47 @@ static void cap_frame_print_attrs_impl(word_t SH, word_t AP, word_t NXN)
 /* use when only have access to vptr of frames */
 static void _cap_frame_print_attrs_vptr(word_t vptr, vspace_root_t *vspace)
 {
-    lookupPTSlot_ret_t ret = lookupPTSlot(vspace, vptr);
-
-    /* Check that the returned slot is a page. */
-    if (!pte_ptr_get_valid(ret.ptSlot) ||
-        (pte_pte_table_ptr_get_present(ret.ptSlot) && ret.ptBitsLeft > PAGE_BITS)) {
-        assert(0);
+    lookupPUDSlot_ret_t pudSlot = lookupPUDSlot(vspace, vptr);
+    if (pudSlot.status != EXCEPTION_NONE) {
+        return;
     }
 
-    word_t table_index;
-    switch (ret.ptBitsLeft) {
-
-    case ARMHugePage:
-        table_index = GET_UPT_INDEX(vptr, ULVL_FRM_ARM_PT_LVL(1));
-        break;
-    case ARMLargePage:
-        table_index = GET_UPT_INDEX(vptr, ULVL_FRM_ARM_PT_LVL(2));
+    switch (pude_ptr_get_pude_type(pudSlot.pudSlot)) {
+    case pude_pude_1g:
+        printf("frame_%p_%04lu ", pudSlot.pudSlot, GET_PUD_INDEX(vptr));
+        cap_frame_print_attrs_pud(pudSlot.pudSlot);
         break;
-    case ARMSmallPage:
-        table_index = GET_UPT_INDEX(vptr, ULVL_FRM_ARM_PT_LVL(3));
+
+    case pude_pude_pd: {
+        pde_t *pd = paddr_to_pptr(pude_pude_pd_ptr_get_pd_base_address(pudSlot.pudSlot));
+        pde_t *pdSlot = pd + GET_PD_INDEX(vptr);
+
+        switch (pde_ptr_get_pde_type(pdSlot)) {
+        case pde_pde_large:
+            printf("frame_%p_%04lu ", pdSlot, GET_PD_INDEX(vptr));
+            cap_frame_print_attrs_pd(pdSlot);
+            break;
+
+        case pde_pde_small: {
+            pte_t *pt = paddr_to_pptr(pde_pde_small_ptr_get_pt_base_address(pdSlot));
+            pte_t *ptSlot = pt + GET_PT_INDEX(vptr);
+
+            if (pte_ptr_get_present(ptSlot)) {
+                printf("frame_%p_%04lu ", ptSlot, GET_PT_INDEX(vptr));
+                cap_frame_print_attrs_pt(ptSlot);
+                break;
+            } else {
+                return;
+            }
+        }
+        default:
+            assert(0);
+        }
         break;
+    }
     default:
         assert(0);
-
     }
-    printf("frame_%p_%04lu ", ret.ptSlot, table_index);
-    cap_frame_print_attrs_pt(ret.ptSlot);
 }
 
 void cap_frame_print_attrs_vptr(word_t vptr, cap_t vspace)
@@ -143,50 +174,50 @@ void cap_frame_print_attrs_vptr(word_t vptr, cap_t vspace)
 /*
  * print object slots
  */
-static void arm64_cap_pt_print_slots(pte_t *pdSlot, vptr_t vptr)
+static void arm64_cap_pt_print_slots(pde_t *pdSlot, vptr_t vptr)
 {
-    pte_t *pt = paddr_to_pptr(pte_pte_table_ptr_get_pt_base_address(pdSlot));
-    printf("pt_%p_%04lu {\n", pdSlot, GET_UPT_INDEX(vptr, ULVL_FRM_ARM_PT_LVL(2)));
+    pte_t *pt = paddr_to_pptr(pde_pde_small_ptr_get_pt_base_address(pdSlot));
+    printf("pt_%p_%04lu {\n", pdSlot, GET_PD_INDEX(vptr));
 
-    for (word_t i = 0; i < BIT(PT_INDEX_BITS); i ++) {
-        pte_t *ptSlot = pt + i;
+    for (word_t i = 0; i < BIT(PT_INDEX_OFFSET + PT_INDEX_BITS); i += (1 << PT_INDEX_OFFSET)) {
+        pte_t *ptSlot = pt + GET_PT_INDEX(i);
 
-        if (pte_4k_page_ptr_get_present(ptSlot)) {
+        if (pte_ptr_get_present(ptSlot)) {
             // print pte entries
-            printf("0x%lx: frame_%p_%04lu", i, ptSlot, i);
+            printf("0x%lx: frame_%p_%04lu", GET_PT_INDEX(i), ptSlot, GET_PT_INDEX(i));
             cap_frame_print_attrs_pt(ptSlot);
         }
     }
     printf("}\n"); /* pt */
 }
 
-static void arm64_cap_pd_print_slots(pte_t *pudSlot, vptr_t vptr)
+static void arm64_cap_pd_print_slots(pude_t *pudSlot, vptr_t vptr)
 {
-    printf("pd_%p_%04lu {\n", pudSlot, GET_UPT_INDEX(vptr, ULVL_FRM_ARM_PT_LVL(1)));
-    pte_t *pd = paddr_to_pptr(pte_pte_table_ptr_get_pt_base_address(pudSlot));
+    printf("pd_%p_%04lu {\n", pudSlot, GET_PUD_INDEX(vptr));
+    pde_t *pd = paddr_to_pptr(pude_pude_pd_ptr_get_pd_base_address(pudSlot));
 
-    for (word_t i = 0; i < BIT(PT_INDEX_BITS); i++) {
-        pte_t *pdSlot = pd + i;
+    for (word_t i = 0; i < BIT(PD_INDEX_OFFSET + PD_INDEX_BITS); i += (1 << PD_INDEX_OFFSET)) {
+        pde_t *pdSlot = pd + GET_PD_INDEX(i);
 
-        switch (pte_ptr_get_pte_type(pdSlot)) {
+        switch (pde_ptr_get_pde_type(pdSlot)) {
 
-        case pte_pte_page:
-            printf("0x%lx: frame_%p_%04lu", i, pdSlot, i);
-            cap_frame_print_attrs_pt(pdSlot);
+        case pde_pde_large:
+            printf("0x%lx: frame_%p_%04lu", GET_PD_INDEX(i), pdSlot, GET_PD_INDEX(i));
+            cap_frame_print_attrs_pd(pdSlot);
             break;
 
-        case pte_pte_table:
-            printf("0x%lx: pt_%p_%04lu\n", i, pdSlot, i);
+        case pde_pde_small:
+            printf("0x%lx: pt_%p_%04lu\n", GET_PD_INDEX(i), pdSlot, GET_PD_INDEX(i));
             break;
         }
     }
 
     printf("}\n"); /* pd */
 
-    for (word_t i = 0; i < BIT(PT_INDEX_BITS); i++) {
-        pte_t *pdSlot = pd + i;
-        if (pte_ptr_get_pte_type(pdSlot) == pte_pte_table) {
-            arm64_cap_pt_print_slots(pdSlot, vptr + (i * GET_ULVL_PGSIZE(ULVL_FRM_ARM_PT_LVL(2))));
+    for (word_t i = 0; i < BIT(PD_INDEX_OFFSET + PD_INDEX_BITS); i += (1 << PD_INDEX_OFFSET)) {
+        pde_t *pdSlot = pd + GET_PD_INDEX(i);
+        if (pde_ptr_get_pde_type(pdSlot) == pde_pde_small) {
+            arm64_cap_pt_print_slots(pdSlot, i);
         }
     }
 }
@@ -194,28 +225,26 @@ static void arm64_cap_pd_print_slots(pte_t *pudSlot, vptr_t vptr)
 static void arm64_cap_pud_print_slots(void *pgdSlot_or_vspace, vptr_t vptr)
 {
 #ifdef AARCH64_VSPACE_S2_START_L1
-    pte_t *pud = pgdSlot_or_vspace;
-    word_t index_bits = seL4_VSpaceIndexBits;
+    pude_t *pud = pgdSlot_or_vspace;
     printf("%p_pd {\n", pgdSlot_or_vspace);
 #else
-    pte_t *pud = paddr_to_pptr(pte_pte_table_ptr_get_pt_base_address(pgdSlot_or_vspace));
-    word_t index_bits = seL4_PageTableIndexBits;
-    printf("pud_%p_%04lu {\n", pgdSlot_or_vspace, GET_UPT_INDEX(vptr, ULVL_FRM_ARM_PT_LVL(0)));
+    pude_t *pud = paddr_to_pptr(pgde_pgde_pud_ptr_get_pud_base_address(pgdSlot_or_vspace));
+    printf("pud_%p_%04lu {\n", pgdSlot_or_vspace, GET_PGD_INDEX(vptr));
 #endif
 
-    for (word_t i = 0; i < BIT(index_bits); i++) {
-        pte_t *pudSlot = pud + i;
-        if (pte_ptr_get_pte_type(pudSlot) == pte_pte_table) {
-            printf("0x%lx: pd_%p_%04lu\n", i, pudSlot, i);
+    for (word_t i = 0; i < BIT(PUD_INDEX_OFFSET + UPUD_INDEX_BITS); i += (1 << PUD_INDEX_OFFSET)) {
+        pude_t *pudSlot = pud + GET_PUD_INDEX(i);
+        if (pude_ptr_get_pude_type(pudSlot) == pude_pude_pd) {
+            printf("0x%lx: pd_%p_%04lu\n", GET_PUD_INDEX(i), pudSlot, GET_PUD_INDEX(i));
         }
     }
 
     printf("}\n"); /* pgd/pud */
 
-    for (word_t i = 0; i < BIT(index_bits); i++) {
-        pte_t *pudSlot = pud + GET_UPT_INDEX(i, ULVL_FRM_ARM_PT_LVL(1));
-        if (pte_ptr_get_pte_type(pudSlot) == pte_pte_table) {
-            arm64_cap_pd_print_slots(pudSlot, vptr + (i * GET_ULVL_PGSIZE(ULVL_FRM_ARM_PT_LVL(1))));
+    for (word_t i = 0; i < BIT(PUD_INDEX_OFFSET + UPUD_INDEX_BITS); i += (1 << PUD_INDEX_OFFSET)) {
+        pude_t *pudSlot = pud + GET_PUD_INDEX(i);
+        if (pude_ptr_get_pude_type(pudSlot) == pude_pude_pd) {
+            arm64_cap_pd_print_slots(pudSlot, i);
         }
     }
 }
@@ -224,7 +253,7 @@ void obj_vtable_print_slots(tcb_t *tcb)
 {
     if (isVTableRoot(TCB_PTR_CTE_PTR(tcb, tcbVTable)->cap) && !seen(TCB_PTR_CTE_PTR(tcb, tcbVTable)->cap)) {
         add_to_seen(TCB_PTR_CTE_PTR(tcb, tcbVTable)->cap);
-        vspace_root_t *vspace = VSPACE_PTR(cap_vspace_cap_get_capVSBasePtr(TCB_PTR_CTE_PTR(tcb, tcbVTable)->cap));
+        vspace_root_t *vspace = cap_vtable_root_get_basePtr(TCB_PTR_CTE_PTR(tcb, tcbVTable)->cap);
 
         /*
         * ARM hyp uses 3 level translation rather than the usual 4 level.
@@ -234,18 +263,18 @@ void obj_vtable_print_slots(tcb_t *tcb)
         arm64_cap_pud_print_slots(vspace, 0);
 #else
         printf("%p_pd {\n", vspace);
-        for (word_t i = 0; i < PT_INDEX_BITS; i++) {
-            pte_t *ptSlot = vspace + i;
-            if (pte_pte_table_ptr_get_present(ptSlot)) {
-                printf("0x%lx: pud_%p_%04lu\n", i, ptSlot, i);
+        for (word_t i = 0; i < BIT(PGD_INDEX_OFFSET + PGD_INDEX_BITS); i += (1UL << PGD_INDEX_OFFSET)) {
+            lookupPGDSlot_ret_t pgdSlot = lookupPGDSlot(vspace, i);
+            if (pgde_pgde_pud_ptr_get_present(pgdSlot.pgdSlot)) {
+                printf("0x%lx: pud_%p_%04lu\n", GET_PGD_INDEX(i), pgdSlot.pgdSlot, GET_PGD_INDEX(i));
             }
         }
         printf("}\n"); /* pd */
 
-        for (word_t i = 0; i < PT_INDEX_BITS; i++) {
-            pte_t *ptSlot = vspace + i;
-            if (pte_pte_table_ptr_get_present(ptSlot)) {
-                arm64_cap_pud_print_slots(ptSlot, i * GET_ULVL_PGSIZE(0));
+        for (word_t i = 0; i < BIT(PGD_INDEX_OFFSET + PGD_INDEX_BITS); i += (1UL << PGD_INDEX_OFFSET)) {
+            lookupPGDSlot_ret_t pgdSlot = lookupPGDSlot(vspace, i);
+            if (pgde_pgde_pud_ptr_get_present(pgdSlot.pgdSlot)) {
+                arm64_cap_pud_print_slots(pgdSlot.pgdSlot, i);
             }
         }
 #endif
@@ -265,37 +294,53 @@ void print_cap_arch(cap_t cap)
     switch (cap_get_capType(cap)) {
     case cap_page_table_cap: {
         asid_t asid = cap_page_table_cap_get_capPTMappedASID(cap);
-        vptr_t vptr = cap_page_table_cap_get_capPTMappedAddress(cap);
-        pte_t *target_pt = PT_PTR(cap_page_table_cap_get_capPTBasePtr(cap));
-
         findVSpaceForASID_ret_t find_ret = findVSpaceForASID(asid);
-        pte_t *ptSlot = NULL;
-        pte_t *pt = (pte_t *)find_ret.vspace_root;
-        word_t level;
-        for (level = 0; level < UPT_LEVELS - 1 && pt != target_pt; level++) {
-            ptSlot = pt + GET_UPT_INDEX(vptr, level);
-            if (unlikely(!pte_pte_table_ptr_get_present(ptSlot))) {
-                /* couldn't find it */
-                break;
-            }
-            pt = paddr_to_pptr(pte_pte_table_ptr_get_pt_base_address(ptSlot));
+        vptr_t vptr = cap_page_table_cap_get_capPTMappedAddress(cap);
+        if (asid) {
+            printf("pt_%p_%04lu (asid: %lu)\n",
+                   lookupPDSlot(find_ret.vspace_root, vptr).pdSlot, GET_PD_INDEX(vptr), (long unsigned int)asid);
+        } else {
+            printf("pt_%p_%04lu\n", lookupPDSlot(find_ret.vspace_root, vptr).pdSlot, GET_PD_INDEX(vptr));
         }
-        if (pt != target_pt) {
-            /* didn't find it */
-            break;
+        break;
+    }
+    case cap_page_directory_cap: {
+        asid_t asid = cap_page_directory_cap_get_capPDMappedASID(cap);
+        findVSpaceForASID_ret_t find_ret = findVSpaceForASID(asid);
+        vptr_t vptr = cap_page_directory_cap_get_capPDMappedAddress(cap);
+        if (asid) {
+            printf("pd_%p_%04lu (asid: %lu)\n",
+                   lookupPUDSlot(find_ret.vspace_root, vptr).pudSlot, GET_PUD_INDEX(vptr), (long unsigned int)asid);
+        } else {
+            printf("pd_%p_%04lu\n",
+                   lookupPUDSlot(find_ret.vspace_root, vptr).pudSlot, GET_PUD_INDEX(vptr));
         }
+        break;
+    }
+    case cap_page_upper_directory_cap: {
+        asid_t asid = cap_page_upper_directory_cap_get_capPUDMappedASID(cap);
+        findVSpaceForASID_ret_t find_ret = findVSpaceForASID(asid);
+        vptr_t vptr = cap_page_upper_directory_cap_get_capPUDMappedAddress(cap);
 
-
+#ifdef AARCH64_VSPACE_S2_START_L1
         if (asid) {
-            printf("pt_%p_%04lu (asid: %lu)\n",
-                   target_pt, GET_UPT_INDEX(vptr, level), (long unsigned int)asid);
+            printf("pud_%p_%04lu (asid: %lu)\n",
+                   find_ret.vspace_root, GET_PGD_INDEX(vptr), (long unsigned int)asid);
         } else {
-            printf("pt_%p_%04lu\n", target_pt, GET_UPT_INDEX(vptr, level));
+            printf("pud_%p_%04lu\n", find_ret.vspace_root, GET_PGD_INDEX(vptr));
         }
+#else
+        if (asid) {
+            printf("pud_%p_%04lu (asid: %lu)\n",
+                   lookupPGDSlot(find_ret.vspace_root, vptr).pgdSlot, GET_PGD_INDEX(vptr), (long unsigned int)asid);
+        } else {
+            printf("pud_%p_%04lu\n", lookupPGDSlot(find_ret.vspace_root, vptr).pgdSlot, GET_PGD_INDEX(vptr));
+        }
+#endif
         break;
     }
-    case cap_vspace_cap: {
-        asid_t asid = cap_vspace_cap_get_capVSMappedASID(cap);
+    case cap_page_global_directory_cap: {
+        asid_t asid = cap_page_global_directory_cap_get_capPGDMappedASID(cap);
         findVSpaceForASID_ret_t find_ret = findVSpaceForASID(asid);
         if (asid) {
             printf("%p_pd (asid: %lu)\n",
@@ -347,7 +392,9 @@ void print_object_arch(cap_t cap)
     switch (cap_get_capType(cap)) {
     case cap_frame_cap:
     case cap_page_table_cap:
-    case cap_vspace_cap:
+    case cap_page_directory_cap:
+    case cap_page_upper_directory_cap:
+    case cap_page_global_directory_cap:
         /* don't need to deal with these objects since they get handled from vtable */
         break;
 
@@ -378,12 +425,12 @@ void print_object_arch(cap_t cap)
     }
 }
 
-void obj_frame_print_attrs(vm_page_size_t frameSize, paddr_t frameBase)
+void obj_frame_print_attrs(lookupFrame_ret_t ret)
 {
     printf("(");
 
     /* VM size */
-    switch (frameSize) {
+    switch (ret.frameSize) {
     case ARMHugePage:
         printf("1G");
         break;
@@ -395,37 +442,44 @@ void obj_frame_print_attrs(vm_page_size_t frameSize, paddr_t frameBase)
         break;
     }
 
-    printf(", paddr: 0x%p)\n", (void *)frameBase);
+    printf(", paddr: 0x%p)\n", (void *)ret.frameBase);
 }
 
-void arm64_obj_pt_print_slots(pte_t *pdSlot)
+void arm64_obj_pt_print_slots(pde_t *pdSlot)
 {
-    pte_t *pt = paddr_to_pptr(pte_pte_table_ptr_get_pt_base_address(pdSlot));
+    lookupFrame_ret_t ret;
+    pte_t *pt = paddr_to_pptr(pde_pde_small_ptr_get_pt_base_address(pdSlot));
 
-    for (word_t i = 0; i < BIT(PT_INDEX_BITS); i++) {
-        pte_t *ptSlot = pt + i;
+    for (word_t i = 0; i < BIT(PT_INDEX_OFFSET + PT_INDEX_BITS); i += (1 << PT_INDEX_OFFSET)) {
+        pte_t *ptSlot = pt + GET_PT_INDEX(i);
 
-        if (pte_4k_page_ptr_get_present(ptSlot)) {
-            printf("frame_%p_%04lu = frame ", ptSlot, i);
-            obj_frame_print_attrs(ARMSmallPage, pte_page_ptr_get_page_base_address(ptSlot));
+        if (pte_ptr_get_present(ptSlot)) {
+            ret.frameBase = pte_ptr_get_page_base_address(ptSlot);
+            ret.frameSize = ARMSmallPage;
+            printf("frame_%p_%04lu = frame ", ptSlot, GET_PT_INDEX(i));
+            obj_frame_print_attrs(ret);
         }
     }
 }
 
-void arm64_obj_pd_print_slots(pte_t *pudSlot)
+void arm64_obj_pd_print_slots(pude_t *pudSlot)
 {
-    pte_t *pd = paddr_to_pptr(pte_pte_table_ptr_get_pt_base_address(pudSlot));
+    lookupFrame_ret_t ret;
+    pde_t *pd = paddr_to_pptr(pude_pude_pd_ptr_get_pd_base_address(pudSlot));
+
+    for (word_t i = 0; i < BIT(PD_INDEX_OFFSET + PD_INDEX_BITS); i += (1 << PD_INDEX_OFFSET)) {
+        pde_t *pdSlot = pd + GET_PD_INDEX(i);
 
-    for (word_t i = 0; i < BIT(PT_INDEX_BITS); i++) {
-        pte_t *pdSlot = pd + i;
+        if (pde_ptr_get_pde_type(pdSlot) == pde_pde_large) {
+            ret.frameBase = pde_pde_large_ptr_get_page_base_address(pdSlot);
+            ret.frameSize = ARMLargePage;
 
-        if (pte_ptr_get_pte_type(pdSlot) == pte_pte_page) {
-            printf("frame_%p_%04lu = frame ", pdSlot, i);
-            obj_frame_print_attrs(ARMLargePage, pte_page_ptr_get_page_base_address(pdSlot));
+            printf("frame_%p_%04lu = frame ", pdSlot, GET_PD_INDEX(i));
+            obj_frame_print_attrs(ret);
         }
 
-        if (pte_ptr_get_pte_type(pdSlot) == pte_pte_table) {
-            printf("pt_%p_%04lu = pt\n", pdSlot, i);
+        if (pde_ptr_get_pde_type(pdSlot) == pde_pde_small) {
+            printf("pt_%p_%04lu = pt\n", pdSlot, GET_PD_INDEX(i));
             arm64_obj_pt_print_slots(pdSlot);
         }
     }
@@ -433,23 +487,23 @@ void arm64_obj_pd_print_slots(pte_t *pudSlot)
 
 void arm64_obj_pud_print_slots(void *pgdSlot_or_vspace)
 {
-    pte_t *pud = paddr_to_pptr(pte_pte_table_ptr_get_pt_base_address(pgdSlot_or_vspace));
-#ifdef AARCH64_VSPACE_S2_START_L1
-    word_t index_bits = seL4_VSpaceIndexBits;
-#else
-    word_t index_bits = seL4_PageTableIndexBits;
-#endif
-    for (word_t i = 0; i < BIT(index_bits); i++) {
-        pte_t *pudSlot = pud + i;
+    lookupFrame_ret_t ret;
+    pude_t *pud = paddr_to_pptr(pgde_pgde_pud_ptr_get_pud_base_address(pgdSlot_or_vspace));
+
+    for (word_t i = 0; i < BIT(PUD_INDEX_OFFSET + UPUD_INDEX_BITS); i += (1 << PUD_INDEX_OFFSET)) {
+        pude_t *pudSlot = pud + GET_PUD_INDEX(i);
 
-        switch (pte_ptr_get_pte_type(pudSlot)) {
-        case pte_pte_page:
-            printf("frame_%p_%04lu = frame ", pudSlot, i);
-            obj_frame_print_attrs(ARMHugePage, pte_page_ptr_get_page_base_address(pudSlot));
+        switch (pude_ptr_get_pude_type(pudSlot)) {
+        case pude_pude_1g:
+            ret.frameBase = pude_pude_1g_ptr_get_page_base_address(pudSlot);
+            ret.frameSize = ARMHugePage;
+
+            printf("frame_%p_%04lu = frame ", pudSlot, GET_PUD_INDEX(i));
+            obj_frame_print_attrs(ret);
             break;
 
-        case pte_pte_table: {
-            printf("pd_%p_%04lu = pd\n", pudSlot, i);
+        case pude_pude_pd: {
+            printf("pd_%p_%04lu = pd\n", pudSlot, GET_PUD_INDEX(i));
             arm64_obj_pd_print_slots(pudSlot);
 
         }
@@ -461,7 +515,7 @@ void obj_tcb_print_vtable(tcb_t *tcb)
 {
     if (isVTableRoot(TCB_PTR_CTE_PTR(tcb, tcbVTable)->cap) && !seen(TCB_PTR_CTE_PTR(tcb, tcbVTable)->cap)) {
         add_to_seen(TCB_PTR_CTE_PTR(tcb, tcbVTable)->cap);
-        vspace_root_t *vspace = VSPACE_PTR(cap_vspace_cap_get_capVSBasePtr(TCB_PTR_CTE_PTR(tcb, tcbVTable)->cap));
+        vspace_root_t *vspace = cap_vtable_root_get_basePtr(TCB_PTR_CTE_PTR(tcb, tcbVTable)->cap);
 
         /*
          * ARM hyp uses 3 level translation rather than the usual 4 level.
@@ -472,11 +526,11 @@ void obj_tcb_print_vtable(tcb_t *tcb)
         arm64_obj_pud_print_slots(vspace);
 #else
         printf("%p_pd = pgd\n", vspace);
-        for (word_t i = 0; i < PT_INDEX_BITS; i++) {
-            pte_t *ptSlot = vspace + i;
-            if (pte_pte_table_ptr_get_present(ptSlot)) {
-                printf("pud_%p_%04lu = pud\n", ptSlot, i);
-                arm64_obj_pud_print_slots(ptSlot);
+        for (word_t i = 0; i < BIT(PGD_INDEX_OFFSET + PGD_INDEX_BITS); i += (1UL << PGD_INDEX_OFFSET)) {
+            lookupPGDSlot_ret_t pgdSlot = lookupPGDSlot(vspace, i);
+            if (pgde_pgde_pud_ptr_get_present(pgdSlot.pgdSlot)) {
+                printf("pud_%p_%04lu = pud\n", pgdSlot.pgdSlot, GET_PGD_INDEX(i));
+                arm64_obj_pud_print_slots(pgdSlot.pgdSlot);
             }
         }
 #endif
@@ -489,9 +543,7 @@ void debug_capDL(void)
 {
     printf("arch aarch64\n");
     printf("objects {\n");
-#ifdef CONFIG_PRINTING
     print_objects();
-#endif
     printf("}\n");
 
     printf("caps {\n");
diff --git a/src/arch/arm/64/model/statedata.c b/src/arch/arm/64/model/statedata.c
index ef24097f..fcbe0652 100644
--- a/src/arch/arm/64/model/statedata.c
+++ b/src/arch/arm/64/model/statedata.c
@@ -83,18 +83,18 @@ asid_pool_t *armKSASIDTable[BIT(asidHighBits)];
  */
 
 vspace_root_t armKSGlobalUserVSpace[BIT(seL4_VSpaceIndexBits)] ALIGN_BSS(BIT(seL4_VSpaceBits));
-pte_t armKSGlobalKernelPGD[BIT(PT_INDEX_BITS)] ALIGN_BSS(BIT(seL4_PageTableBits));
+pgde_t armKSGlobalKernelPGD[BIT(PGD_INDEX_BITS)] ALIGN_BSS(BIT(PGD_SIZE_BITS));
 
-pte_t armKSGlobalKernelPUD[BIT(PT_INDEX_BITS)] ALIGN_BSS(BIT(seL4_PageTableBits));
-pte_t armKSGlobalKernelPDs[BIT(PT_INDEX_BITS)][BIT(PT_INDEX_BITS)] ALIGN_BSS(BIT(seL4_PageTableBits));
+pude_t armKSGlobalKernelPUD[BIT(PUD_INDEX_BITS)] ALIGN_BSS(BIT(seL4_PUDBits));
+pde_t armKSGlobalKernelPDs[BIT(PUD_INDEX_BITS)][BIT(PD_INDEX_BITS)] ALIGN_BSS(BIT(seL4_PageDirBits));
 pte_t armKSGlobalKernelPT[BIT(PT_INDEX_BITS)] ALIGN_BSS(BIT(seL4_PageTableBits));
 
 #ifdef CONFIG_KERNEL_LOG_BUFFER
-pte_t *armKSGlobalLogPTE = &armKSGlobalKernelPDs[BIT(PT_INDEX_BITS) - 1][BIT(PT_INDEX_BITS) - 2];
+pde_t *armKSGlobalLogPDE = &armKSGlobalKernelPDs[BIT(PUD_INDEX_BITS) - 1][BIT(PD_INDEX_BITS) - 2];
 compile_assert(log_pude_is_correct_preallocated_pude,
-               GET_KPT_INDEX(KS_LOG_PPTR, KLVL_FRM_ARM_PT_LVL(1)) == BIT(PT_INDEX_BITS) - 1);
+               GET_PUD_INDEX(KS_LOG_PPTR) == BIT(PUD_INDEX_BITS) - 1);
 compile_assert(log_pde_is_correct_preallocated_pde,
-               GET_KPT_INDEX(KS_LOG_PPTR, KLVL_FRM_ARM_PT_LVL(2)) == BIT(PT_INDEX_BITS) - 2);
+               GET_PD_INDEX(KS_LOG_PPTR) == BIT(PD_INDEX_BITS) - 2);
 #endif
 
 #ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
diff --git a/src/arch/arm/64/object/objecttype.c b/src/arch/arm/64/object/objecttype.c
index bd733f71..fa930a60 100644
--- a/src/arch/arm/64/object/objecttype.c
+++ b/src/arch/arm/64/object/objecttype.c
@@ -32,12 +32,36 @@ deriveCap_ret_t Arch_deriveCap(cte_t *slot, cap_t cap)
     deriveCap_ret_t ret;
 
     switch (cap_get_capType(cap)) {
-    case cap_vspace_cap:
-        if (cap_vspace_cap_get_capVSIsMapped(cap)) {
+    case cap_page_global_directory_cap:
+        if (cap_page_global_directory_cap_get_capPGDIsMapped(cap)) {
             ret.cap = cap;
             ret.status = EXCEPTION_NONE;
         } else {
-            userError("Deriving a VSpace cap without an assigned ASID");
+            userError("Deriving a PDG cap without an assigned ASID");
+            current_syscall_error.type = seL4_IllegalOperation;
+            ret.cap = cap_null_cap_new();
+            ret.status = EXCEPTION_SYSCALL_ERROR;
+        }
+        return ret;
+
+    case cap_page_upper_directory_cap:
+        if (cap_page_upper_directory_cap_get_capPUDIsMapped(cap)) {
+            ret.cap = cap;
+            ret.status = EXCEPTION_NONE;
+        } else {
+            userError("Deriving a PUD cap without an assigned ASID");
+            current_syscall_error.type = seL4_IllegalOperation;
+            ret.cap = cap_null_cap_new();
+            ret.status = EXCEPTION_SYSCALL_ERROR;
+        }
+        return ret;
+
+    case cap_page_directory_cap:
+        if (cap_page_directory_cap_get_capPDIsMapped(cap)) {
+            ret.cap = cap;
+            ret.status = EXCEPTION_NONE;
+        } else {
+            userError("Deriving a PD cap without an assigned ASID");
             current_syscall_error.type = seL4_IllegalOperation;
             ret.cap = cap_null_cap_new();
             ret.status = EXCEPTION_SYSCALL_ERROR;
@@ -84,12 +108,6 @@ deriveCap_ret_t Arch_deriveCap(cte_t *slot, cap_t cap)
         ret.cap = cap;
         ret.status = EXCEPTION_NONE;
         return ret;
-#endif
-#ifdef CONFIG_ALLOW_SMC_CALLS
-    case cap_smc_cap:
-        ret.cap = cap;
-        ret.status = EXCEPTION_NONE;
-        return ret;
 #endif
     default:
         /* This assert has no equivalent in haskell,
@@ -100,19 +118,7 @@ deriveCap_ret_t Arch_deriveCap(cte_t *slot, cap_t cap)
 
 cap_t CONST Arch_updateCapData(bool_t preserve, word_t data, cap_t cap)
 {
-#ifdef CONFIG_ALLOW_SMC_CALLS
-    if (cap_get_capType(cap) == cap_smc_cap) {
-        if (!preserve && cap_smc_cap_get_capSMCBadge(cap) == 0) {
-            return cap_smc_cap_set_capSMCBadge(cap, data);
-        } else {
-            return cap_null_cap_new();
-        }
-    } else {
-#endif
-        return cap;
-#ifdef CONFIG_ALLOW_SMC_CALLS
-    }
-#endif
+    return cap;
 }
 
 cap_t CONST Arch_maskCapRights(seL4_CapRights_t cap_rights_mask, cap_t cap)
@@ -141,16 +147,46 @@ finaliseCap_ret_t Arch_finaliseCap(cap_t cap, bool_t final)
         }
         break;
 
-    case cap_vspace_cap:
+    case cap_page_global_directory_cap:
+#ifdef CONFIG_ARM_SMMU
+        if (cap_page_global_directory_cap_get_capPGDMappedCB(cap) != CB_INVALID) {
+            smmu_cb_delete_vspace(cap_page_global_directory_cap_get_capPGDMappedCB(cap),
+                                  cap_page_global_directory_cap_get_capPGDMappedASID(cap));
+        }
+#endif
+        if (final && cap_page_global_directory_cap_get_capPGDIsMapped(cap)) {
+            deleteASID(cap_page_global_directory_cap_get_capPGDMappedASID(cap),
+                       VSPACE_PTR(cap_page_global_directory_cap_get_capPGDBasePtr(cap)));
+        }
+        break;
+
+    case cap_page_upper_directory_cap:
+#ifdef AARCH64_VSPACE_S2_START_L1
 #ifdef CONFIG_ARM_SMMU
-        if (cap_vspace_cap_get_capVSMappedCB(cap) != CB_INVALID) {
-            smmu_cb_delete_vspace(cap_vspace_cap_get_capVSMappedCB(cap),
-                                  cap_vspace_cap_get_capVSMappedASID(cap));
+        if (cap_page_upper_directory_cap_get_capPGDMappedCB(cap) != CB_INVALID) {
+            smmu_cb_delete_vspace(cap_page_upper_directory_cap_get_capPUDMappedCB(cap),
+                                  cap_page_upper_directory_cap_get_capPUDMappedASID(cap));
+        }
+#endif
+        if (final && cap_page_upper_directory_cap_get_capPUDIsMapped(cap)) {
+            deleteASID(cap_page_upper_directory_cap_get_capPUDMappedASID(cap),
+                       PUDE_PTR(cap_page_upper_directory_cap_get_capPUDBasePtr(cap)));
+        }
+#else
+        if (final && cap_page_upper_directory_cap_get_capPUDIsMapped(cap)) {
+            unmapPageUpperDirectory(cap_page_upper_directory_cap_get_capPUDMappedASID(cap),
+                                    cap_page_upper_directory_cap_get_capPUDMappedAddress(cap),
+                                    PUDE_PTR(cap_page_upper_directory_cap_get_capPUDBasePtr(cap)));
         }
+
 #endif
-        if (final && cap_vspace_cap_get_capVSIsMapped(cap)) {
-            deleteASID(cap_vspace_cap_get_capVSMappedASID(cap),
-                       VSPACE_PTR(cap_vspace_cap_get_capVSBasePtr(cap)));
+        break;
+
+    case cap_page_directory_cap:
+        if (final && cap_page_directory_cap_get_capPDIsMapped(cap)) {
+            unmapPageDirectory(cap_page_directory_cap_get_capPDMappedASID(cap),
+                               cap_page_directory_cap_get_capPDMappedAddress(cap),
+                               PDE_PTR(cap_page_directory_cap_get_capPDBasePtr(cap)));
         }
         break;
 
@@ -221,10 +257,24 @@ bool_t CONST Arch_sameRegionAs(cap_t cap_a, cap_t cap_b)
         }
         break;
 
-    case cap_vspace_cap:
-        if (cap_get_capType(cap_b) == cap_vspace_cap) {
-            return cap_vspace_cap_get_capVSBasePtr(cap_a) ==
-                   cap_vspace_cap_get_capVSBasePtr(cap_b);
+    case cap_page_directory_cap:
+        if (cap_get_capType(cap_b) == cap_page_directory_cap) {
+            return cap_page_directory_cap_get_capPDBasePtr(cap_a) ==
+                   cap_page_directory_cap_get_capPDBasePtr(cap_b);
+        }
+        break;
+
+    case cap_page_upper_directory_cap:
+        if (cap_get_capType(cap_b) == cap_page_upper_directory_cap) {
+            return cap_page_upper_directory_cap_get_capPUDBasePtr(cap_a) ==
+                   cap_page_upper_directory_cap_get_capPUDBasePtr(cap_b);
+        }
+        break;
+
+    case cap_page_global_directory_cap:
+        if (cap_get_capType(cap_b) == cap_page_global_directory_cap) {
+            return cap_page_global_directory_cap_get_capPGDBasePtr(cap_a) ==
+                   cap_page_global_directory_cap_get_capPGDBasePtr(cap_b);
         }
         break;
 
@@ -274,13 +324,6 @@ bool_t CONST Arch_sameRegionAs(cap_t cap_a, cap_t cap_b)
                    cap_cb_cap_get_capCB(cap_b);
         }
         break;
-#endif
-#ifdef CONFIG_ALLOW_SMC_CALLS
-    case cap_smc_cap:
-        if (cap_get_capType(cap_b) == cap_smc_cap) {
-            return true;
-        }
-        break;
 #endif
     }
     return false;
@@ -322,8 +365,14 @@ word_t Arch_getObjectSize(word_t t)
         return ARMHugePageBits;
     case seL4_ARM_PageTableObject:
         return seL4_PageTableBits;
-    case seL4_ARM_VSpaceObject:
-        return seL4_VSpaceBits;
+    case seL4_ARM_PageDirectoryObject:
+        return seL4_PageDirBits;
+    case seL4_ARM_PageUpperDirectoryObject:
+        return seL4_PUDBits;
+#ifndef AARCH64_VSPACE_S2_START_L1
+    case seL4_ARM_PageGlobalDirectoryObject:
+        return seL4_PGDBits;
+#endif
 #ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
     case seL4_ARM_VCPUObject:
         return VCPU_SIZE_BITS;
@@ -338,19 +387,6 @@ cap_t Arch_createObject(object_t t, void *regionBase, word_t userSize, bool_t de
 {
     switch (t) {
     case seL4_ARM_SmallPageObject:
-        if (deviceMemory) {
-            /** AUXUPD: "(True, ptr_retyps 1
-                     (Ptr (ptr_val \<acute>regionBase) :: user_data_device_C ptr))" */
-            /** GHOSTUPD: "(True, gs_new_frames vmpage_size.ARMSmallPage
-                                                    (ptr_val \<acute>regionBase)
-                                                    (unat ARMSmallPageBits))" */
-        } else {
-            /** AUXUPD: "(True, ptr_retyps 1
-                     (Ptr (ptr_val \<acute>regionBase) :: user_data_C ptr))" */
-            /** GHOSTUPD: "(True, gs_new_frames vmpage_size.ARMSmallPage
-                                                    (ptr_val \<acute>regionBase)
-                                                    (unat ARMSmallPageBits))" */
-        }
         return cap_frame_cap_new(
                    asidInvalid,           /* capFMappedASID */
                    (word_t)regionBase,    /* capFBasePtr */
@@ -361,19 +397,6 @@ cap_t Arch_createObject(object_t t, void *regionBase, word_t userSize, bool_t de
                );
 
     case seL4_ARM_LargePageObject:
-        if (deviceMemory) {
-            /** AUXUPD: "(True, ptr_retyps (2^9)
-                     (Ptr (ptr_val \<acute>regionBase) :: user_data_device_C ptr))" */
-            /** GHOSTUPD: "(True, gs_new_frames vmpage_size.ARMLargePage
-                                                    (ptr_val \<acute>regionBase)
-                                                    (unat ARMLargePageBits))" */
-        } else {
-            /** AUXUPD: "(True, ptr_retyps (2^9)
-                     (Ptr (ptr_val \<acute>regionBase) :: user_data_C ptr))" */
-            /** GHOSTUPD: "(True, gs_new_frames vmpage_size.ARMLargePage
-                                                    (ptr_val \<acute>regionBase)
-                                                    (unat ARMLargePageBits))" */
-        }
         return cap_frame_cap_new(
                    asidInvalid,           /* capFMappedASID */
                    (word_t)regionBase,    /* capFBasePtr */
@@ -384,19 +407,6 @@ cap_t Arch_createObject(object_t t, void *regionBase, word_t userSize, bool_t de
                );
 
     case seL4_ARM_HugePageObject:
-        if (deviceMemory) {
-            /** AUXUPD: "(True, ptr_retyps (2^18)
-                     (Ptr (ptr_val \<acute>regionBase) :: user_data_device_C ptr))" */
-            /** GHOSTUPD: "(True, gs_new_frames vmpage_size.ARMHugePage
-                                                    (ptr_val \<acute>regionBase)
-                                                    (unat ARMHugePageBits))" */
-        } else {
-            /** AUXUPD: "(True, ptr_retyps (2^18)
-                     (Ptr (ptr_val \<acute>regionBase) :: user_data_C ptr))" */
-            /** GHOSTUPD: "(True, gs_new_frames vmpage_size.ARMHugePage
-                                                    (ptr_val \<acute>regionBase)
-                                                    (unat ARMHugePageBits))" */
-        }
         return cap_frame_cap_new(
                    asidInvalid,           /* capFMappedASID */
                    (word_t)regionBase,    /* capFBasePtr */
@@ -405,30 +415,42 @@ cap_t Arch_createObject(object_t t, void *regionBase, word_t userSize, bool_t de
                    VMReadWrite,           /* capFVMRights */
                    !!deviceMemory         /* capFIsDevice */
                );
-    case seL4_ARM_VSpaceObject:
+#ifndef AARCH64_VSPACE_S2_START_L1
+    case seL4_ARM_PageGlobalDirectoryObject:
 #ifdef CONFIG_ARM_SMMU
 
-        return cap_vspace_cap_new(
-                   asidInvalid,           /* capVSMappedASID */
-                   (word_t)regionBase,    /* capVSBasePtr    */
-                   0,                     /* capVSIsMapped   */
-                   CB_INVALID             /* capVSMappedCB   */
+        return cap_page_global_directory_cap_new(
+                   asidInvalid,           /* capPGDMappedASID   */
+                   (word_t)regionBase,    /* capPGDBasePtr      */
+                   0,                     /* capPGDIsMapped     */
+                   CB_INVALID             /* capPGDMappedCB     */
                );
 #else
 
-        /** AUXUPD: "(True, ptr_retyps 1
-              (Ptr (ptr_val \<acute>regionBase) :: (pte_C[vs_array_len]) ptr))" */
-        /** GHOSTUPD: "(True, gs_new_pt_t VSRootPT_T (ptr_val \<acute>regionBase))" */
-        return cap_vspace_cap_new(
-                   asidInvalid,           /* capVSMappedASID */
-                   (word_t)regionBase,    /* capVSBasePtr    */
-                   0                      /* capVSIsMapped   */
+        return cap_page_global_directory_cap_new(
+                   asidInvalid,           /* capPGDMappedASID   */
+                   (word_t)regionBase,    /* capPGDBasePtr      */
+                   0                      /* capPGDIsMapped     */
                );
 #endif /*!CONFIG_ARM_SMMU*/
+#endif /*!AARCH64_VSPACE_S2_START_L1*/
+    case seL4_ARM_PageUpperDirectoryObject:
+        return cap_page_upper_directory_cap_new(
+                   asidInvalid,           /* capPUDMappedASID    */
+                   (word_t)regionBase,    /* capPUDBasePtr       */
+                   0,                     /* capPUDIsMapped      */
+                   0                      /* capPUDMappedAddress */
+               );
+
+    case seL4_ARM_PageDirectoryObject:
+        return cap_page_directory_cap_new(
+                   asidInvalid,           /* capPDMappedASID    */
+                   (word_t)regionBase,    /* capPDBasePtr       */
+                   0,                     /* capPDIsMapped      */
+                   0                      /* capPDMappedAddress */
+               );
+
     case seL4_ARM_PageTableObject:
-        /** AUXUPD: "(True, ptr_retyps 1
-              (Ptr (ptr_val \<acute>regionBase) :: (pte_C[pt_array_len]) ptr))" */
-        /** GHOSTUPD: "(True, gs_new_pt_t NormalPT_T (ptr_val \<acute>regionBase))" */
         return cap_page_table_cap_new(
                    asidInvalid,           /* capPTMappedASID    */
                    (word_t)regionBase,    /* capPTBasePtr       */
@@ -457,7 +479,7 @@ exception_t Arch_decodeInvocation(word_t label, word_t length, cptr_t cptr,
     /* The C parser cannot handle a switch statement with only a default
      * case. So we need to do some gymnastics to remove the switch if
      * there are no other cases */
-#if defined(CONFIG_ARM_HYPERVISOR_SUPPORT) || defined(CONFIG_ARM_SMMU) || defined(CONFIG_ALLOW_SMC_CALLS)
+#if defined(CONFIG_ARM_HYPERVISOR_SUPPORT) || defined(CONFIG_ARM_SMMU)
     switch (cap_get_capType(cap)) {
 #ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
     case cap_vcpu_cap:
@@ -473,10 +495,6 @@ exception_t Arch_decodeInvocation(word_t label, word_t length, cptr_t cptr,
     case cap_cb_cap:
         return decodeARMCBInvocation(label, length, cptr, slot, cap, call, buffer);
 #endif /*CONFIG_ARM_SMMU*/
-#ifdef CONFIG_ALLOW_SMC_CALLS
-    case cap_smc_cap:
-        return decodeARMSMCInvocation(label, length, cptr, slot, cap, call, buffer);
-#endif
     default:
 #else
 {
diff --git a/src/arch/arm/64/traps.S b/src/arch/arm/64/traps.S
index 6320d54c..1cf55ace 100644
--- a/src/arch/arm/64/traps.S
+++ b/src/arch/arm/64/traps.S
@@ -32,7 +32,7 @@
 
 .macro lsp_i _tmp
     mrs     \_tmp, TPIDR
-#ifdef CONFIG_ENABLE_SMP_SUPPORT
+#if CONFIG_MAX_NUM_NODES > 1
     bic     \_tmp, \_tmp, #0xfff
 #endif
     mov     sp, \_tmp
@@ -43,7 +43,7 @@
     b       \label
 .endm
 
-.section .vectors, "ax"
+.section .vectors
 
 BEGIN_FUNC(arm_vector_table)
     ventry  invalid_vector_entry           // Synchronous EL1t/EL2t
@@ -67,7 +67,7 @@ BEGIN_FUNC(arm_vector_table)
     ventry  invalid_vector_entry           // SError 32-bit EL0/EL1
 END_FUNC(arm_vector_table)
 
-.section .vectors.text, "ax"
+.section .vectors.text
 
 .macro kernel_enter
     /* Storing thread's stack frame */


diff --git a/src/arch/arm/config.cmake b/src/arch/arm/config.cmake
index 03bff4de..ad81a240 100644
--- a/src/arch/arm/config.cmake
+++ b/src/arch/arm/config.cmake
@@ -6,7 +6,7 @@
 
 cmake_minimum_required(VERSION 3.7.2)
 
-if(KernelSel4ArchAarch32)
+if(KernelArchARM)
     set_property(TARGET kernel_config_target APPEND PROPERTY TOPLEVELTYPES pde_C)
 endif()
 
@@ -185,19 +185,10 @@ config_option(
         operations in a multithreading environment, instead of relying on \
         software emulation of FPU/VFP from the C library (e.g. mfloat-abi=soft)."
     DEFAULT ON
-    DEPENDS "KernelSel4ArchAarch32"
+    DEPENDS "KernelSel4ArchAarch32;NOT KernelVerificationBuild"
     DEFAULT_DISABLED OFF
 )
 
-config_option(
-    KernelAArch64UserCacheEnable AARCH64_USER_CACHE_ENABLE
-    "Enable any attempt to execute a DC CVAU, DC CIVAC, DC CVAC, or IC IVAU \
-    instruction or access to CTR_EL0 at EL0 using AArch64. \
-    When disabled, these operations will be trapped."
-    DEFAULT ON
-    DEPENDS "KernelSel4ArchAarch64"
-)
-
 config_option(
     KernelAArch64SErrorIgnore AARCH64_SERROR_IGNORE
     "By default any SError interrupt will halt the kernel. SErrors may \
@@ -208,16 +199,6 @@ config_option(
 )
 mark_as_advanced(KernelAArch64SErrorIgnore)
 
-config_option(
-    KernelAllowSMCCalls ALLOW_SMC_CALLS "Allow components to make SMC calls. \
-    WARNING: Allowing SMC calls causes a couple of issues. Since seL4 cannot \
-    pre-empt the secure monitor, the WCET is no longer guaranteed. Also, since the \
-    secure monitor is a higher privilege level and can make any change in the \
-    system, the proofs can no longer be guaranteed."
-    DEFAULT OFF
-    DEPENDS "NOT KernelVerificationBuild; KernelSel4ArchAarch64"
-)
-
 if(KernelAArch32FPUEnableContextSwitch OR KernelSel4ArchAarch64)
     set(KernelHaveFPU ON)
 endif()
@@ -263,7 +244,6 @@ add_sources(
         object/iospace.c
         object/vcpu.c
         object/smmu.c
-        object/smc.c
         smp/ipi.c
 )
 
diff --git a/src/arch/arm/kernel/boot.c b/src/arch/arm/kernel/boot.c
index 1c8decc7..33f73dcb 100644
--- a/src/arch/arm/kernel/boot.c
+++ b/src/arch/arm/kernel/boot.c
@@ -29,11 +29,8 @@
 #endif
 
 #ifdef ENABLE_SMP_SUPPORT
-/* SMP boot synchronization works based on a global variable with the initial
- * value 0, as the loader must zero all BSS variables. Secondary cores keep
- * spinning until the primary core has initialized all kernel structures and
- * then set it to 1.
- */
+/* sync variable to prevent other nodes from booting
+ * until kernel data structures initialized */
 BOOT_BSS static volatile int node_boot_lock;
 #endif /* ENABLE_SMP_SUPPORT */
 
@@ -45,7 +42,8 @@ BOOT_CODE static bool_t arch_init_freemem(p_region_t ui_p_reg,
                                           word_t extra_bi_size_bits)
 {
     /* reserve the kernel image region */
-    reserved[0] = paddr_to_pptr_reg(get_p_reg_kernel_img());
+    reserved[0].start = KERNEL_ELF_BASE;
+    reserved[0].end = (pptr_t)ki_end;
 
     int index = 1;
 
@@ -164,14 +162,6 @@ BOOT_CODE static void init_smmu(cap_t root_cnode_cap)
 
 #endif
 
-#ifdef CONFIG_ALLOW_SMC_CALLS
-BOOT_CODE static void init_smc(cap_t root_cnode_cap)
-{
-    /* Provide the SMC cap*/
-    write_slot(SLOT_PTR(pptr_of_cap(root_cnode_cap), seL4_CapSMC), cap_smc_cap_new(0));
-}
-#endif
-
 /** This and only this function initialises the CPU.
  *
  * It does NOT initialise any kernel state.
@@ -263,13 +253,15 @@ BOOT_CODE static void init_plat(void)
 #ifdef ENABLE_SMP_SUPPORT
 BOOT_CODE static bool_t try_init_kernel_secondary_core(void)
 {
+    unsigned i;
+
     /* need to first wait until some kernel init has been done */
     while (!node_boot_lock);
 
     /* Perform cpu init */
     init_cpu();
 
-    for (unsigned int i = 0; i < NUM_PPI; i++) {
+    for (i = 0; i < NUM_PPI; i++) {
         maskInterrupt(true, CORE_IRQ_TO_IRQT(getCurrentCPUIndex(), i));
     }
     setIRQState(IRQIPI, CORE_IRQ_TO_IRQT(getCurrentCPUIndex(), irq_remote_call_ipi));
@@ -292,23 +284,20 @@ BOOT_CODE static bool_t try_init_kernel_secondary_core(void)
 
 BOOT_CODE static void release_secondary_cpus(void)
 {
+
     /* release the cpus at the same time */
-    assert(0 == node_boot_lock); /* Sanity check for a proper lock state. */
     node_boot_lock = 1;
 
-    /*
-     * At this point in time the primary core (executing this code) already uses
-     * the seL4 MMU/cache setup. However, the secondary cores are still using
-     * the elfloader's MMU/cache setup, and thus any memory updates may not
-     * be visible there.
-     *
-     * On AARCH64, both elfloader and seL4 map memory inner shareable and have
-     * the caches enabled, so no explicit cache maintenance is necessary.
+#ifndef CONFIG_ARCH_AARCH64
+    /* At this point in time the other CPUs do *not* have the seL4 global pd set.
+     * However, they still have a PD from the elfloader (which is mapping memory
+     * as strongly ordered uncached, as a result we need to explicitly clean
+     * the cache for it to see the update of node_boot_lock
      *
-     * On AARCH32 the elfloader uses strongly ordered uncached memory, but seL4
-     * has caching enabled, thus explicit cache cleaning is required.
+     * For ARMv8, the elfloader sets the page table entries as inner shareable
+     * (so is the attribute of the seL4 global PD) when SMP is enabled, and
+     * turns on the cache. Thus, we do not need to clean and invalidate the cache.
      */
-#ifdef CONFIG_ARCH_AARCH32
     cleanInvalidateL1Caches();
     plat_cleanInvalidateL2Cache();
 #endif
@@ -359,7 +348,7 @@ static BOOT_CODE bool_t try_init_kernel(
 
     ipcbuf_vptr = ui_v_reg.end;
     bi_frame_vptr = ipcbuf_vptr + BIT(PAGE_BITS);
-    extra_bi_frame_vptr = bi_frame_vptr + BIT(seL4_BootInfoFrameBits);
+    extra_bi_frame_vptr = bi_frame_vptr + BIT(BI_FRAME_SIZE_BITS);
 
     /* setup virtual memory for the kernel */
     map_kernel_window();
@@ -446,10 +435,6 @@ static BOOT_CODE bool_t try_init_kernel(
     /* initialise the SMMU and provide the SMMU control caps*/
     init_smmu(root_cnode_cap);
 #endif
-#ifdef CONFIG_ALLOW_SMC_CALLS
-    init_smc(root_cnode_cap);
-#endif
-
     populate_bi_frame(0, CONFIG_MAX_NUM_NODES, ipcbuf_vptr, extra_bi_size);
 
     /* put DTB in the bootinfo block, if present. */
@@ -584,7 +569,12 @@ static BOOT_CODE bool_t try_init_kernel(
     init_core_state(initial);
 
     /* create all of the untypeds. Both devices and kernel window memory */
-    if (!create_untypeds(root_cnode_cap)) {
+    if (!create_untypeds(
+            root_cnode_cap,
+    (region_t) {
+    KERNEL_ELF_BASE, (pptr_t)ki_boot_end
+    } /* reusable boot code/data */
+        )) {
         printf("ERROR: could not create untypteds for kernel image boot memory\n");
         return false;
     }
diff --git a/src/arch/arm/machine/debug.c b/src/arch/arm/machine/debug.c
index b49f87a9..e1e0db92 100644
--- a/src/arch/arm/machine/debug.c
+++ b/src/arch/arm/machine/debug.c
@@ -671,13 +671,15 @@ bool_t configureSingleStepping(tcb_t *t,
         bp_num = convertBpNumToArch(bp_num);
     }
 
-    /* On ARM single-stepping is emulated using breakpoint mismatches. The aim
-     * of single stepping is to execute a single instruction. By setting an
-     * instruction mismatch breakpoint to the current LR of the target thread,
-     * the thread will be able to execute this instruction, but attempting to
-     * execute any other instruction will result in the generation of a debug
-     * exception that will be delivered to the kernel, allowing us to simulate
-     * single stepping.
+    /* On ARM single-stepping is emulated using breakpoint mismatches. So you
+     * would basically set the breakpoint to mismatch everything, and this will
+     * cause an exception to be triggered on every instruction.
+     *
+     * We use NULL as the mismatch address since no code should be trying to
+     * execute NULL, so it's a perfect address to use as the mismatch
+     * criterion. An alternative might be to use an address in the kernel's
+     * high vaddrspace, since that's an address that it's impossible for
+     * userspace to be executing at.
      */
     dbg_bcr_t bcr;
 
@@ -699,7 +701,7 @@ bool_t configureSingleStepping(tcb_t *t,
     bcr = dbg_bcr_set_byteAddressSelect(bcr, convertSizeToArch(1));
     bcr = Arch_setupBcr(bcr, false);
 
-    writeBvrContext(t, bp_num, t->tcbArch.tcbContext.registers[FaultIP]);
+    writeBvrContext(t, bp_num, 0);
     writeBcrContext(t, bp_num, bcr.words[0]);
 
     t->tcbArch.tcbContext.breakpointState.n_instructions = n_instr;
@@ -1070,7 +1072,7 @@ seL4_Fault_t handleUserLevelDebugException(word_t fault_vaddr)
 #endif
 
     word_t method_of_entry = getMethodOfEntry();
-    int active_bp;
+    int i, active_bp;
     seL4_Fault_t ret;
     word_t bp_reason, bp_vaddr;
 
@@ -1084,29 +1086,24 @@ seL4_Fault_t handleUserLevelDebugException(word_t fault_vaddr)
          *  2. A breakpoint configured in mismatch mode to emulate
          *  single-stepping.
          *
-         * We assume that the exception was triggered by a normal breakpoint
-         * unless the thread currently has single stepping enabled and the
-         * breakpoint value register used for this is mismatched with the
-         * current faultIP
+         * If the register is configured for mismatch, then it's a single-step
+         * exception. If the register is configured for match, then it's a
+         * normal breakpoint exception.
          */
-        tcb_t *curr_thread = NODE_STATE(ksCurThread);
-        user_context_t *context = &curr_thread->tcbArch.tcbContext;
-
-        if (context->breakpointState.single_step_enabled) {
-            word_t bvr;
-            word_t bp_num = context->breakpointState.single_step_hw_bp_num;
-
-            bvr = readBvrCp(bp_num);
-            if (bvr != context->registers[FaultIP]) {
-                bp_reason = seL4_SingleStep;
-                active_bp = bp_num;
-                /* Update the BVR so it doesn't fault on the same address
-                 * again (in the case of stepping through multiple instructions)
-                 */
-                writeBvrContext(curr_thread, active_bp, context->registers[FaultIP]);
+        for (i = 0; i < seL4_NumExclusiveBreakpoints; i++) {
+            dbg_bcr_t bcr;
+
+            bcr.words[0] = readBcrCp(i);
+            if (!dbg_bcr_get_enabled(bcr) || Arch_breakpointIsMismatch(bcr) != true) {
+                continue;
             }
+            /* Return the first BP enabled and configured for mismatch. */
+            bp_reason = seL4_SingleStep;
+            active_bp = i;
+            break;
         }
         break;
+
     case DEBUG_ENTRY_SYNC_WATCHPOINT:
         bp_reason = seL4_DataBreakpoint;
 #ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
diff --git a/src/arch/arm/machine/gic_v2.c b/src/arch/arm/machine/gic_v2.c
index a5b9282e..8d8ae789 100644
--- a/src/arch/arm/machine/gic_v2.c
+++ b/src/arch/arm/machine/gic_v2.c
@@ -228,6 +228,6 @@ volatile struct gich_vcpu_ctrl_map *gic_vcpu_ctrl =
     (volatile struct gich_vcpu_ctrl_map *)(GIC_V2_VCPUCTRL_PPTR);
 #endif /* GIC_PL400_GICVCPUCTRL_PPTR */
 
-word_t gic_vcpu_num_list_regs;
+unsigned int gic_vcpu_num_list_regs;
 
 #endif /* End of CONFIG_ARM_HYPERVISOR_SUPPORT */
diff --git a/src/arch/arm/machine/gic_v3.c b/src/arch/arm/machine/gic_v3.c
index 37aacef0..dbd1eafc 100644
--- a/src/arch/arm/machine/gic_v3.c
+++ b/src/arch/arm/machine/gic_v3.c
@@ -75,27 +75,26 @@ static inline uint64_t mpidr_to_gic_affinity(void)
 }
 
 /* Wait for completion of a distributor change */
-/** DONT_TRANSLATE */
 static uint32_t gicv3_do_wait_for_rwp(volatile uint32_t *ctlr_addr)
 {
     uint32_t val;
     bool_t waiting = true;
     uint32_t ret = 0;
 
-    uint64_t gpt_cnt_tval = 0;
+    uint32_t gpt_cnt_tval = 0;
     uint32_t deadline_ms =  GIC_DEADLINE_MS;
-    uint64_t gpt_cnt_ciel;
+    uint32_t gpt_cnt_ciel;
 
     /* Check the value before reading the generic timer */
     val = *ctlr_addr;
     if (!(val & GICD_CTLR_RWP)) {
         return 0;
     }
-    SYSTEM_READ_64(CNT_CT, gpt_cnt_tval);
+    SYSTEM_READ_WORD(CNTFRQ, gpt_cnt_tval);
     gpt_cnt_ciel = gpt_cnt_tval + (deadline_ms * TICKS_PER_MS);
 
     while (waiting) {
-        SYSTEM_READ_64(CNT_CT, gpt_cnt_tval);
+        SYSTEM_READ_WORD(CNTFRQ, gpt_cnt_tval);
         val = *ctlr_addr;
 
         if (gpt_cnt_tval >= gpt_cnt_ciel) {
@@ -389,6 +388,6 @@ void setIRQTarget(irq_t irq, seL4_Word target)
 
 #ifdef CONFIG_ARM_HYPERVISOR_SUPPORT
 
-word_t gic_vcpu_num_list_regs;
+unsigned int gic_vcpu_num_list_regs;
 
 #endif /* End of CONFIG_ARM_HYPERVISOR_SUPPORT */
diff --git a/src/arch/arm/object/smc.c b/src/arch/arm/object/smc.c
deleted file mode 100644
index 7f771ecb..00000000
--- a/src/arch/arm/object/smc.c
+++ /dev/null
@@ -1,94 +0,0 @@
-/*
- * Copyright 2021, DornerWorks Ltd.
- *
- * SPDX-License-Identifier: GPL-2.0-only
- */
-#include <config.h>
-
-#ifdef CONFIG_ALLOW_SMC_CALLS
-#include <arch/object/smc.h>
-
-compile_assert(n_msgRegisters_less_than_smc_regs, n_msgRegisters <= NUM_SMC_REGS);
-
-static exception_t invokeSMCCall(word_t *buffer, bool_t call)
-{
-    word_t i;
-    seL4_Word arg[NUM_SMC_REGS];
-    word_t *ipcBuffer;
-
-    for (i = 0; i < NUM_SMC_REGS; i++) {
-        arg[i] = getSyscallArg(i, buffer);
-    }
-
-    ipcBuffer = lookupIPCBuffer(true, NODE_STATE(ksCurThread));
-
-    register seL4_Word r0 asm("x0") = arg[0];
-    register seL4_Word r1 asm("x1") = arg[1];
-    register seL4_Word r2 asm("x2") = arg[2];
-    register seL4_Word r3 asm("x3") = arg[3];
-    register seL4_Word r4 asm("x4") = arg[4];
-    register seL4_Word r5 asm("x5") = arg[5];
-    register seL4_Word r6 asm("x6") = arg[6];
-    register seL4_Word r7 asm("x7") = arg[7];
-    asm volatile("smc #0\n"
-                 : "+r"(r0), "+r"(r1), "+r"(r2), "+r"(r3),
-                 "+r"(r4), "+r"(r5), "+r"(r6), "+r"(r7)
-                 :: "x8", "x9", "x10", "x11", "x12", "x13", "x14", "x15", "x16", "x17", "memory");
-
-    arg[0] = r0;
-    arg[1] = r1;
-    arg[2] = r2;
-    arg[3] = r3;
-    arg[4] = r4;
-    arg[5] = r5;
-    arg[6] = r6;
-    arg[7] = r7;
-
-    if (call) {
-        for (i = 0; i < n_msgRegisters; i++) {
-            setRegister(NODE_STATE(ksCurThread), msgRegisters[i], arg[i]);
-        }
-
-        if (ipcBuffer != NULL) {
-            for (; i < NUM_SMC_REGS; i++) {
-                ipcBuffer[i + 1] = arg[i];
-            }
-        }
-
-        setRegister(NODE_STATE(ksCurThread), badgeRegister, 0);
-        setRegister(NODE_STATE(ksCurThread), msgInfoRegister, wordFromMessageInfo(
-                        seL4_MessageInfo_new(0, 0, 0, i)));
-    }
-    setThreadState(NODE_STATE(ksCurThread), ThreadState_Running);
-    return EXCEPTION_NONE;
-}
-
-exception_t decodeARMSMCInvocation(word_t label, word_t length, cptr_t cptr,
-                                   cte_t *srcSlot, cap_t cap, bool_t call, word_t *buffer)
-{
-    if (label != ARMSMCCall) {
-        userError("ARMSMCInvocation: Illegal operation.");
-        current_syscall_error.type = seL4_IllegalOperation;
-        return EXCEPTION_SYSCALL_ERROR;
-    }
-
-    if (length < NUM_SMC_REGS) {
-        userError("ARMSMCCall: Truncated message.");
-        current_syscall_error.type = seL4_TruncatedMessage;
-        return EXCEPTION_SYSCALL_ERROR;
-    }
-
-    word_t badge = cap_smc_cap_get_capSMCBadge(cap);
-    word_t smc_func_id = getSyscallArg(0, buffer);
-
-    if (badge != 0 && badge != smc_func_id) {
-        userError("ARMSMCCall: Illegal operation.");
-        current_syscall_error.type = seL4_IllegalOperation;
-        return EXCEPTION_SYSCALL_ERROR;
-    }
-
-    setThreadState(NODE_STATE(ksCurThread), ThreadState_Restart);
-    return invokeSMCCall(buffer, call);
-}
-
-#endif
diff --git a/src/arch/arm/object/smmu.c b/src/arch/arm/object/smmu.c
index a1b488dd..6e47b01b 100644
--- a/src/arch/arm/object/smmu.c
+++ b/src/arch/arm/object/smmu.c
@@ -18,7 +18,7 @@ static exception_t checkARMCBVspace(cap_t cap)
     return EXCEPTION_NONE;
 }
 
-exception_t decodeARMSIDControlInvocation(word_t label, word_t length, cptr_t cptr,
+exception_t decodeARMSIDControlInvocation(word_t label, unsigned int length, cptr_t cptr,
                                           cte_t *srcSlot, cap_t cap, bool_t call, word_t *buffer)
 {
 
@@ -99,7 +99,7 @@ exception_t decodeARMSIDControlInvocation(word_t label, word_t length, cptr_t cp
     return EXCEPTION_NONE;
 }
 
-exception_t decodeARMSIDInvocation(word_t label, word_t length, cptr_t cptr,
+exception_t decodeARMSIDInvocation(word_t label, unsigned int length, cptr_t cptr,
                                    cte_t *srcSlot, cap_t cap, bool_t call, word_t *buffer)
 {
     cap_t cbCap;
@@ -185,7 +185,7 @@ exception_t smmu_delete_sid(cap_t cap)
     return status;
 }
 
-exception_t decodeARMCBControlInvocation(word_t label, word_t length, cptr_t cptr,
+exception_t decodeARMCBControlInvocation(word_t label, unsigned int length, cptr_t cptr,
                                          cte_t *srcSlot, cap_t cap, bool_t call, word_t *buffer)
 {
 
@@ -249,7 +249,7 @@ exception_t decodeARMCBControlInvocation(word_t label, word_t length, cptr_t cpt
     return EXCEPTION_NONE;
 }
 
-exception_t decodeARMCBInvocation(word_t label, word_t length, cptr_t cptr,
+exception_t decodeARMCBInvocation(word_t label, unsigned int length, cptr_t cptr,
                                   cte_t *srcSlot, cap_t cap, bool_t call, word_t *buffer)
 {
 
@@ -272,7 +272,7 @@ exception_t decodeARMCBInvocation(word_t label, word_t length, cptr_t cptr,
         cb = cap_cb_cap_get_capCB(cap);
         cbSlot = smmuStateCBNode + cb;
         setThreadState(NODE_STATE(ksCurThread), ThreadState_Restart);
-        smmu_tlb_invalidate_cb(cb, cap_vspace_cap_get_capVSMappedASID(cbSlot->cap));
+        smmu_tlb_invalidate_cb(cb, cap_vtable_root_get_mappedASID(cbSlot->cap));
         return EXCEPTION_NONE;
 
     case ARMCBAssignVspace:
@@ -284,7 +284,7 @@ exception_t decodeARMCBInvocation(word_t label, word_t length, cptr_t cptr,
         vspaceCapSlot = current_extra_caps.excaprefs[0];
         vspaceCap = vspaceCapSlot->cap;
 
-        if (unlikely(!isVTableRoot(vspaceCap) || !cap_vspace_cap_get_capVSIsMapped(vspaceCap))) {
+        if (unlikely(!isVTableRoot(vspaceCap) || !cap_vtable_root_isMapped(vspaceCap))) {
             userError("ARMCBAssignVspace: the vspace is invalid");
             current_syscall_error.type = seL4_InvalidCapability;
             current_syscall_error.invalidCapNumber = 1;
@@ -302,14 +302,14 @@ exception_t decodeARMCBInvocation(word_t label, word_t length, cptr_t cptr,
 
         setThreadState(NODE_STATE(ksCurThread), ThreadState_Restart);
         /*setting up vspace for the context bank in SMMU*/
-        smmu_cb_assign_vspace(cb, VSPACE_PTR(cap_vspace_cap_get_capVSBasePtr(vspaceCap)),
-                              cap_vspace_cap_get_capVSMappedASID(vspaceCap));
+        smmu_cb_assign_vspace(cb, cap_vtable_root_get_basePtr(vspaceCap),
+                              cap_vtable_root_get_mappedASID(vspaceCap));
         /*Connecting vspace cap to context bank*/
         cteInsert(vspaceCap, vspaceCapSlot, cbSlot);
-        cap_vspace_cap_ptr_set_capVSMappedCB(&(cbSlot->cap), cb);
+        cap_vtable_root_ptr_set_mappedCB(&(cbSlot->cap), cb);
         /*set relationship between CB and ASID*/
-        smmuStateCBAsidTable[cb] = cap_vspace_cap_get_capVSMappedASID(vspaceCap);
-        increaseASIDBindCB(cap_vspace_cap_get_capVSMappedASID(vspaceCap));
+        smmuStateCBAsidTable[cb] = cap_vtable_root_get_mappedASID(vspaceCap);
+        increaseASIDBindCB(cap_vtable_root_get_mappedASID(vspaceCap));
         return EXCEPTION_NONE;
 
     case ARMCBUnassignVspace:
diff --git a/src/arch/arm/object/tcb.c b/src/arch/arm/object/tcb.c
index 66cfe4da..0a5e0721 100644
--- a/src/arch/arm/object/tcb.c
+++ b/src/arch/arm/object/tcb.c
@@ -21,3 +21,15 @@ exception_t CONST Arch_performTransfer(word_t arch, tcb_t *tcb_src, tcb_t *tcb_d
 {
     return EXCEPTION_NONE;
 }
+
+#ifdef ENABLE_SMP_SUPPORT
+void Arch_migrateTCB(tcb_t *thread)
+{
+#ifdef CONFIG_HAVE_FPU
+    /* check if thread own its current core FPU */
+    if (nativeThreadUsingFPU(thread)) {
+        switchFpuOwner(NULL, thread->tcbAffinity);
+    }
+#endif /* CONFIG_HAVE_FPU */
+}
+#endif /* ENABLE_SMP_SUPPORT */
diff --git a/src/arch/arm/object/vcpu.c b/src/arch/arm/object/vcpu.c
index 0f7cdb20..f4c5922b 100644
--- a/src/arch/arm/object/vcpu.c
+++ b/src/arch/arm/object/vcpu.c
@@ -32,7 +32,7 @@ BOOT_CODE void vcpu_boot_init(void)
 static void vcpu_save(vcpu_t *vcpu, bool_t active)
 {
     word_t i;
-    word_t lr_num;
+    unsigned int lr_num;
 
     assert(vcpu);
     dsb();
@@ -85,7 +85,7 @@ void vcpu_restore(vcpu_t *vcpu)
 {
     assert(vcpu);
     word_t i;
-    word_t lr_num;
+    unsigned int lr_num;
     /* Turn off the VGIC */
     set_gic_vcpu_ctrl_hcr(0);
     isb();
@@ -314,7 +314,7 @@ exception_t invokeVCPUWriteReg(vcpu_t *vcpu, word_t field, word_t value)
     return EXCEPTION_NONE;
 }
 
-exception_t decodeVCPUWriteReg(cap_t cap, word_t length, word_t *buffer)
+exception_t decodeVCPUWriteReg(cap_t cap, unsigned int length, word_t *buffer)
 {
     word_t field;
     word_t value;
@@ -351,7 +351,7 @@ exception_t invokeVCPUReadReg(vcpu_t *vcpu, word_t field, bool_t call)
     return EXCEPTION_NONE;
 }
 
-exception_t decodeVCPUReadReg(cap_t cap, word_t length, bool_t call, word_t *buffer)
+exception_t decodeVCPUReadReg(cap_t cap, unsigned int length, bool_t call, word_t *buffer)
 {
     word_t field;
     if (length < 1) {
@@ -378,10 +378,8 @@ exception_t invokeVCPUInjectIRQ(vcpu_t *vcpu, unsigned long index, virq_t virq)
     if (likely(ARCH_NODE_STATE(armHSCurVCPU) == vcpu)) {
         set_gic_vcpu_ctrl_lr(index, virq);
 #ifdef ENABLE_SMP_SUPPORT
-    } else if (vcpu->vcpuTCB != NULL && vcpu->vcpuTCB->tcbAffinity != getCurrentCPUIndex()) {
-        doRemoteOp3Arg(IpiRemoteCall_VCPUInjectInterrupt,
-                       (word_t)vcpu, index, virq.words[0],
-                       vcpu->vcpuTCB->tcbAffinity);
+    } else if (vcpu->vcpuTCB->tcbAffinity != getCurrentCPUIndex()) {
+        doRemoteOp3Arg(IpiRemoteCall_VCPUInjectInterrupt, (word_t)vcpu, index, virq.words[0],      vcpu->vcpuTCB->tcbAffinity);
 #endif /* CONFIG_ENABLE_SMP */
     } else {
         vcpu->vgic.lr[index] = virq;
@@ -390,7 +388,7 @@ exception_t invokeVCPUInjectIRQ(vcpu_t *vcpu, unsigned long index, virq_t virq)
     return EXCEPTION_NONE;
 }
 
-exception_t decodeVCPUInjectIRQ(cap_t cap, word_t length, word_t *buffer)
+exception_t decodeVCPUInjectIRQ(cap_t cap, unsigned int length, word_t *buffer)
 {
     word_t vid, priority, group, index;
     vcpu_t *vcpu;
@@ -475,7 +473,7 @@ exception_t decodeVCPUInjectIRQ(cap_t cap, word_t length, word_t *buffer)
 
 exception_t decodeARMVCPUInvocation(
     word_t label,
-    word_t length,
+    unsigned int length,
     cptr_t cptr,
     cte_t *slot,
     cap_t cap,
@@ -501,7 +499,7 @@ exception_t decodeARMVCPUInvocation(
     }
 }
 
-exception_t decodeVCPUAckVPPI(cap_t cap, word_t length, word_t *buffer)
+exception_t decodeVCPUAckVPPI(cap_t cap, unsigned int length, word_t *buffer)
 {
     vcpu_t *vcpu = VCPU_PTR(cap_vcpu_cap_get_capVCPUPtr(cap));
 
diff --git a/src/arch/riscv/c_traps.c b/src/arch/riscv/c_traps.c
index ba4e4b30..ba647b36 100644
--- a/src/arch/riscv/c_traps.c
+++ b/src/arch/riscv/c_traps.c
@@ -26,7 +26,8 @@ void VISIBLE NORETURN restore_user_context(void)
     NODE_UNLOCK_IF_HELD;
 
 #ifdef ENABLE_SMP_SUPPORT
-    word_t sp = read_sscratch();
+    word_t sp;
+    asm volatile("csrr %0, sscratch" : "=r"(sp));
     sp -= sizeof(word_t);
     *((word_t *)sp) = cur_thread_reg;
 #endif
@@ -42,7 +43,8 @@ void VISIBLE NORETURN restore_user_context(void)
         LOAD_S " ra, (0*%[REGSIZE])(t0)  \n"
         LOAD_S "  sp, (1*%[REGSIZE])(t0)  \n"
         LOAD_S "  gp, (2*%[REGSIZE])(t0)  \n"
-        /* skip tp/x4, t0/x5, t1/x6, they are restored later */
+        /* skip tp */
+        /* skip x5/t0 */
         /* no-op store conditional to clear monitor state */
         /* this may succeed in implementations with very large reservations, but the saved ra is dead */
         "sc.w zero, zero, (t0)\n"
diff --git a/src/arch/riscv/common_riscv.lds b/src/arch/riscv/common_riscv.lds
index b45368c6..8d09ccc6 100644
--- a/src/arch/riscv/common_riscv.lds
+++ b/src/arch/riscv/common_riscv.lds
@@ -46,9 +46,6 @@ SECTIONS
     {
         . = ALIGN(4K);
 
-       /* Traps and fastpath */
-        *(.text.traps)
-        *(.text.fastpath)
 
         /* Standard kernel */
         *(.text)
diff --git a/src/arch/riscv/config.cmake b/src/arch/riscv/config.cmake
index 1179ece9..866601b0 100644
--- a/src/arch/riscv/config.cmake
+++ b/src/arch/riscv/config.cmake
@@ -10,8 +10,8 @@ config_string(
     KernelPTLevels PT_LEVELS "Number of page \
     table levels for RISC-V depends on the mode. For example there are: \
     2, 3 and 4 levels on Sv32, Sv39, Sv48 RISC-V paging modes respectively."
-    DEFAULT 3
-    UNDEF_DISABLED UNQUOTE
+    DEFAULT 3 UNDEF_DISABLED
+    UNQUOTE
     DEPENDS "KernelArchRiscV"
 )
 
@@ -27,17 +27,6 @@ config_option(
     DEPENDS "KernelArchRiscV"
 )
 
-config_option(
-    KernelRiscvUseClintMtime RISCV_USE_CLINT_MTIME "When reading the timestamp \
-    from the hardware, directly access the CLINT timer register (mtime) instead \
-    of using the rdtime instruction. This is a performance optimization, but \
-    only for platforms where executing the rdtime instruction results in a \
-    trap into M-mode software which then accesses the CLINT timer register. \
-    Note that this option requires S-mode access to the CLINT."
-    DEFAULT OFF
-    DEPENDS "KernelArchRiscV"
-)
-
 # Until RISC-V has instructions to count leading/trailing zeros, we provide
 # library implementations. Platforms that implement the bit manipulation
 # extension can override these settings to remove the library functions from
diff --git a/src/arch/riscv/head.S b/src/arch/riscv/head.S
index 6185a66a..990a18fd 100644
--- a/src/arch/riscv/head.S
+++ b/src/arch/riscv/head.S
@@ -30,7 +30,7 @@ _start:
   la sp, (kernel_stack_alloc + BIT(CONFIG_KERNEL_STACK_BITS))
   csrw sscratch, x0 /* zero sscratch for the init task */
 
-#ifdef CONFIG_ENABLE_SMP_SUPPORT
+#if CONFIG_MAX_NUM_NODES > 1
 /* setup the per-core stack */
   mv t0, a7
   slli t0, t0, CONFIG_KERNEL_STACK_BITS
diff --git a/src/arch/riscv/kernel/boot.c b/src/arch/riscv/kernel/boot.c
index 34d6f6db..02074ddc 100644
--- a/src/arch/riscv/kernel/boot.c
+++ b/src/arch/riscv/kernel/boot.c
@@ -14,7 +14,6 @@
 #include <object/interrupt.h>
 #include <arch/machine.h>
 #include <arch/kernel/boot.h>
-#include <arch/kernel/traps.h>
 #include <arch/kernel/vspace.h>
 #include <arch/benchmark.h>
 #include <linker.h>
@@ -22,11 +21,6 @@
 #include <machine.h>
 
 #ifdef ENABLE_SMP_SUPPORT
-/* SMP boot synchronization works based on a global variable with the initial
- * value 0, as the loader must zero all BSS variables. Secondary cores keep
- * spinning until the primary core has initialized all kernel structures and
- * then set it to 1.
- */
 BOOT_BSS static volatile word_t node_boot_lock;
 #endif
 
@@ -66,7 +60,9 @@ BOOT_CODE static bool_t arch_init_freemem(region_t ui_reg,
      * symbols are a reference in the kernel image window, but all allocations
      * are done in terms of the main kernel window, so we do some translation.
      */
-    res_reg[0] = paddr_to_pptr_reg(get_p_reg_kernel_img());
+    res_reg[0].start = (pptr_t)paddr_to_pptr(kpptr_to_paddr((void *)KERNEL_ELF_BASE));
+    res_reg[0].end = (pptr_t)paddr_to_pptr(kpptr_to_paddr((void *)ki_end));
+
     int index = 1;
 
     /* add the dtb region, if it is not empty */
@@ -112,6 +108,9 @@ BOOT_CODE static void init_irqs(cap_t root_cnode_cap)
     write_slot(SLOT_PTR(pptr_of_cap(root_cnode_cap), seL4_CapIRQControl), cap_irq_control_cap_new());
 }
 
+/* ASM symbol for the CPU initialisation trap. */
+extern char trap_entry[1];
+
 /* This and only this function initialises the CPU. It does NOT initialise any kernel state. */
 
 #ifdef CONFIG_HAVE_FPU
@@ -169,15 +168,8 @@ BOOT_CODE static bool_t try_init_kernel_secondary_core(word_t hart_id, word_t co
 
 BOOT_CODE static void release_secondary_cores(void)
 {
-    assert(0 == node_boot_lock); /* Sanity check for a proper lock state. */
     node_boot_lock = 1;
-    /* At this point in time the primary core (executing this code) already uses
-     * the seL4 MMU/cache setup. However, the secondary cores are still using
-     * the elfloader's MMU/cache setup, and thus the update of node_boot_lock
-     * may not be visible there if the setups differ. Currently, the mappings
-     * match, so a barrier is all that is needed.
-     */
-    fence_rw_rw();
+    fence_w_r();
 
     while (ksNumCPUs != CONFIG_MAX_NUM_NODES) {
 #ifdef ENABLE_SMP_CLOCK_SYNC_TEST_ON_BOOT
@@ -203,6 +195,10 @@ static BOOT_CODE bool_t try_init_kernel(
     cap_t it_pd_cap;
     cap_t it_ap_cap;
     cap_t ipcbuf_cap;
+    p_region_t boot_mem_reuse_p_reg = ((p_region_t) {
+        kpptr_to_paddr((void *)KERNEL_ELF_BASE), kpptr_to_paddr(ki_boot_end)
+    });
+    region_t boot_mem_reuse_reg = paddr_to_pptr_reg(boot_mem_reuse_p_reg);
     region_t ui_reg = paddr_to_pptr_reg((p_region_t) {
         ui_p_reg_start, ui_p_reg_end
     });
@@ -222,7 +218,7 @@ static BOOT_CODE bool_t try_init_kernel(
 
     ipcbuf_vptr = ui_v_reg.end;
     bi_frame_vptr = ipcbuf_vptr + BIT(PAGE_BITS);
-    extra_bi_frame_vptr = bi_frame_vptr + BIT(seL4_BootInfoFrameBits);
+    extra_bi_frame_vptr = bi_frame_vptr + BIT(BI_FRAME_SIZE_BITS);
 
     map_kernel_window();
 
@@ -426,7 +422,9 @@ static BOOT_CODE bool_t try_init_kernel(
     init_core_state(initial);
 
     /* convert the remaining free memory into UT objects and provide the caps */
-    if (!create_untypeds(root_cnode_cap)) {
+    if (!create_untypeds(
+            root_cnode_cap,
+            boot_mem_reuse_reg)) {
         printf("ERROR: could not create untypteds for kernel image boot memory\n");
         return false;
     }
diff --git a/src/arch/riscv/kernel/vspace.c b/src/arch/riscv/kernel/vspace.c
index 6d4cdfc5..0f3059ff 100644
--- a/src/arch/riscv/kernel/vspace.c
+++ b/src/arch/riscv/kernel/vspace.c
@@ -641,11 +641,6 @@ vm_rights_t CONST maskVMRights(vm_rights_t vm_rights, seL4_CapRights_t cap_right
             return VMReadWrite;
         }
     }
-    if (vm_rights == VMReadWrite &&
-        !seL4_CapRights_get_capAllowRead(cap_rights_mask) &&
-        seL4_CapRights_get_capAllowWrite(cap_rights_mask)) {
-        userError("Attempted to make unsupported write only mapping");
-    }
     return VMKernelOnly;
 }
 
diff --git a/src/arch/riscv/machine/capdl.c b/src/arch/riscv/machine/capdl.c
index 0450ecbe..7fd6be55 100644
--- a/src/arch/riscv/machine/capdl.c
+++ b/src/arch/riscv/machine/capdl.c
@@ -229,9 +229,7 @@ void debug_capDL(void)
 {
     printf("arch riscv\n");
     printf("objects {\n");
-#ifdef CONFIG_PRINTING
     print_objects();
-#endif
     printf("}\n");
 
     printf("caps {\n");
diff --git a/src/arch/riscv/object/tcb.c b/src/arch/riscv/object/tcb.c
index db3bcfc9..42b69ce5 100644
--- a/src/arch/riscv/object/tcb.c
+++ b/src/arch/riscv/object/tcb.c
@@ -21,3 +21,14 @@ exception_t CONST Arch_performTransfer(word_t arch, tcb_t *tcb_src, tcb_t *tcb_d
 {
     return EXCEPTION_NONE;
 }
+
+#ifdef ENABLE_SMP_SUPPORT
+void Arch_migrateTCB(tcb_t *thread)
+{
+#ifdef CONFIG_HAVE_FPU
+    if (nativeThreadUsingFPU(thread)) {
+        switchFpuOwner(NULL, thread->tcbAffinity);
+    }
+#endif
+}
+#endif
diff --git a/src/arch/riscv/platform_gen.h.in b/src/arch/riscv/platform_gen.h.in
index 491c0199..adc6e007 100644
--- a/src/arch/riscv/platform_gen.h.in
+++ b/src/arch/riscv/platform_gen.h.in
@@ -28,7 +28,7 @@
  */
 enum IRQConstants {
     PLIC_IRQ_OFFSET = 0,
-    PLIC_MAX_IRQ = PLIC_IRQ_OFFSET + (@CONFIGURE_MAX_IRQ@),
+    PLIC_MAX_IRQ = PLIC_IRQ_OFFSET + (@CONFIGURE_PLIC_MAX_NUM_INT@),
 #ifdef ENABLE_SMP_SUPPORT
     INTERRUPT_IPI_0,
     INTERRUPT_IPI_1,
diff --git a/src/arch/riscv/traps.S b/src/arch/riscv/traps.S
index e04551ec..a89f42a2 100644
--- a/src/arch/riscv/traps.S
+++ b/src/arch/riscv/traps.S
@@ -14,14 +14,16 @@
 
 #define REGBYTES (CONFIG_WORD_SIZE / 8)
 
+.section .text
+
+.global trap_entry
 .extern c_handle_syscall
 .extern c_handle_fastpath_reply_recv
 .extern c_handle_fastpath_call
 .extern c_handle_interrupt
 .extern c_handle_exception
 
-.section .text.traps, "ax"
-BEGIN_FUNC(trap_entry)
+trap_entry:
 
 #ifdef ENABLE_SMP_SUPPORT
 /* The sscratch contains the stack for the current core */
@@ -113,19 +115,14 @@ handle_syscall:
 
 #ifdef CONFIG_FASTPATH
   li t3, SYSCALL_CALL
-  bne a7, t3, .Ltest_replyrecv
-  j c_handle_fastpath_call
+  beq a7, t3, c_handle_fastpath_call
 
-.Ltest_replyrecv:
   li t3, SYSCALL_REPLY_RECV
 #ifdef CONFIG_KERNEL_MCS
   /* move reply to 3rd argument */
   mv a2, a6
 #endif
-  bne a7, t3, .Lslowpath
-  j c_handle_fastpath_reply_recv
-
-.Lslowpath:
+  beq a7, t3, c_handle_fastpath_reply_recv
 #endif
 
   /* move syscall number to 3rd argument */
@@ -143,5 +140,3 @@ interrupt:
   /* Save NextIP */
   STORE   x1, (34*REGBYTES)(t0)
   j c_handle_interrupt
-
-END_FUNC(trap_entry)


diff --git a/src/benchmark/benchmark_utilisation.c b/src/benchmark/benchmark_utilisation.c
index baf54499..49d9c4d9 100644
--- a/src/benchmark/benchmark_utilisation.c
+++ b/src/benchmark/benchmark_utilisation.c
@@ -56,7 +56,7 @@ void benchmark_track_utilisation_dump(void)
     /* Total counters */
 #ifdef CONFIG_ARM_ENABLE_PMU_OVERFLOW_INTERRUPT
     buffer[BENCHMARK_TOTAL_UTILISATION] =
-        (ARCH_NODE_STATE(ccnt_num_overflows) * 0xFFFFFFFFU) + NODE_STATE(benchmark_end_time) - NODE_STATE(benchmark_start_time);
+        (NODE_STATE(ccnt_num_overflows) * 0xFFFFFFFFU) + NODE_STATE(benchmark_end_time) - NODE_STATE(benchmark_start_time);
 #else
     buffer[BENCHMARK_TOTAL_UTILISATION] = NODE_STATE(benchmark_end_time) - NODE_STATE(
                                               benchmark_start_time); /* Overall time */
diff --git a/src/config/default_domain.c b/src/config/default_domain.c
index 97f0afc2..eff0206e 100644
--- a/src/config/default_domain.c
+++ b/src/config/default_domain.c
@@ -8,7 +8,7 @@
 #include <object/structures.h>
 #include <model/statedata.h>
 
-/* Default schedule. The length is in ms */
+/* Default schedule. */
 const dschedule_t ksDomSchedule[] = {
     { .domain = 0, .length = 1 },
 };
diff --git a/src/drivers/timer/generic_timer.c b/src/drivers/timer/generic_timer.c
index 47954783..69aa1c1f 100644
--- a/src/drivers/timer/generic_timer.c
+++ b/src/drivers/timer/generic_timer.c
@@ -26,10 +26,10 @@ BOOT_CODE void initGenericTimer(void)
 #ifdef CONFIG_KERNEL_MCS
     /* this sets the irq to UINT64_MAX */
     ackDeadlineIRQ();
+    SYSTEM_WRITE_WORD(CNT_CTL, BIT(0));
 #else /* CONFIG_KERNEL_MCS */
     resetTimer();
 #endif /* !CONFIG_KERNEL_MCS */
-    SYSTEM_WRITE_WORD(CNT_CTL, BIT(0));
 }
 
 /*
diff --git a/src/fastpath/fastpath.c b/src/fastpath/fastpath.c
index ac4b4b6f..cc8f261c 100644
--- a/src/fastpath/fastpath.c
+++ b/src/fastpath/fastpath.c
@@ -97,7 +97,7 @@ void NORETURN fastpath_call(word_t cptr, word_t msgInfo)
 #endif
 #ifdef CONFIG_ARCH_AARCH64
     /* Need to test that the ASID is still valid */
-    asid_t asid = cap_vspace_cap_get_capVSMappedASID(newVTable);
+    asid_t asid = cap_vtable_root_get_mappedASID(newVTable);
     asid_map_t asid_map = findMapForASID(asid);
     if (unlikely(asid_map_get_type(asid_map) != asid_map_asid_map_vspace ||
                  VSPACE_PTR(asid_map_asid_map_vspace_get_vspace_root(asid_map)) != cap_pd)) {
@@ -374,7 +374,7 @@ void NORETURN fastpath_reply_recv(word_t cptr, word_t msgInfo)
 #endif
 #ifdef CONFIG_ARCH_AARCH64
     /* Need to test that the ASID is still valid */
-    asid_t asid = cap_vspace_cap_get_capVSMappedASID(newVTable);
+    asid_t asid = cap_vtable_root_get_mappedASID(newVTable);
     asid_map_t asid_map = findMapForASID(asid);
     if (unlikely(asid_map_get_type(asid_map) != asid_map_asid_map_vspace ||
                  VSPACE_PTR(asid_map_asid_map_vspace_get_vspace_root(asid_map)) != cap_pd)) {
@@ -779,7 +779,7 @@ void NORETURN fastpath_vm_fault(vm_fault_type_t type)
 
 #ifdef CONFIG_ARCH_AARCH64
     /* Need to test that the ASID is still valid */
-    asid_t asid = cap_vspace_cap_get_capVSMappedASID(newVTable);
+    asid_t asid = cap_vtable_root_get_mappedASID(newVTable);
     asid_map_t asid_map = findMapForASID(asid);
     if (unlikely(asid_map_get_type(asid_map) != asid_map_asid_map_vspace ||
                  VSPACE_PTR(asid_map_asid_map_vspace_get_vspace_root(asid_map)) != cap_pd)) {
diff --git a/src/kernel/boot.c b/src/kernel/boot.c
index fd68ac2d..5dbc95d7 100644
--- a/src/kernel/boot.c
+++ b/src/kernel/boot.c
@@ -23,26 +23,6 @@ BOOT_BSS ndks_boot_t ndks_boot;
 BOOT_BSS rootserver_mem_t rootserver;
 BOOT_BSS static region_t rootserver_mem;
 
-/* Returns the physical region of the kernel image boot part, which is the part
- * that is no longer needed once booting is finished. */
-extern char ki_boot_end[1];
-BOOT_CODE p_region_t get_p_reg_kernel_img_boot(void)
-{
-    return (p_region_t) {
-        .start = kpptr_to_paddr((const void *)KERNEL_ELF_BASE),
-        .end   = kpptr_to_paddr(ki_boot_end)
-    };
-}
-
-/* Returns the physical region of the kernel image. */
-BOOT_CODE p_region_t get_p_reg_kernel_img(void)
-{
-    return (p_region_t) {
-        .start = kpptr_to_paddr((const void *)KERNEL_ELF_BASE),
-        .end   = kpptr_to_paddr(ki_end)
-    };
-}
-
 BOOT_CODE static void merge_regions(void)
 {
     /* Walk through reserved regions and see if any can be merged */
@@ -177,7 +157,7 @@ BOOT_CODE static word_t calculate_rootserver_size(v_region_t it_v_reg, word_t ex
     word_t size = BIT(CONFIG_ROOT_CNODE_SIZE_BITS + seL4_SlotBits);
     size += BIT(seL4_TCBBits); // root thread tcb
     size += BIT(seL4_PageBits); // ipc buf
-    size += BIT(seL4_BootInfoFrameBits); // boot info
+    size += BIT(BI_FRAME_SIZE_BITS); // boot info
     size += BIT(seL4_ASIDPoolBits);
     size += extra_bi_size_bits > 0 ? BIT(extra_bi_size_bits) : 0;
     size += BIT(seL4_VSpaceBits); // root vspace
@@ -228,12 +208,8 @@ BOOT_CODE static void create_rootserver_objects(pptr_t start, v_region_t it_v_re
     compile_assert(invalid_seL4_ASIDPoolBits, seL4_ASIDPoolBits == seL4_PageBits);
     rootserver.asid_pool = alloc_rootserver_obj(seL4_ASIDPoolBits, 1);
     rootserver.ipc_buf = alloc_rootserver_obj(seL4_PageBits, 1);
-    /* The boot info size must be at least one page. Due to the hard-coded order
-     * of allocations used in the current implementation here, it can't be any
-     * bigger.
-     */
-    compile_assert(invalid_seL4_BootInfoFrameBits, seL4_BootInfoFrameBits == seL4_PageBits);
-    rootserver.boot_info = alloc_rootserver_obj(seL4_BootInfoFrameBits, 1);
+    compile_assert(invalid_BI_FRAME_SIZE_BITS, BI_FRAME_SIZE_BITS == seL4_PageBits);
+    rootserver.boot_info = alloc_rootserver_obj(BI_FRAME_SIZE_BITS, 1);
 
     /* TCBs on aarch32 can be larger than page tables in certain configs */
 #if seL4_TCBBits >= seL4_PageTableBits
@@ -272,7 +248,7 @@ BOOT_CODE void write_slot(slot_ptr_t slot_ptr, cap_t cap)
 compile_assert(root_cnode_size_valid,
                CONFIG_ROOT_CNODE_SIZE_BITS < 32 - seL4_SlotBits &&
                BIT(CONFIG_ROOT_CNODE_SIZE_BITS) >= seL4_NumInitialCaps &&
-               CONFIG_ROOT_CNODE_SIZE_BITS >= (seL4_PageBits - seL4_SlotBits))
+               BIT(CONFIG_ROOT_CNODE_SIZE_BITS) >= (seL4_PageBits - seL4_SlotBits))
 
 BOOT_CODE cap_t
 create_root_cnode(void)
@@ -348,7 +324,7 @@ BOOT_CODE void populate_bi_frame(node_id_t node_id, word_t num_nodes,
                                  vptr_t ipcbuf_vptr, word_t extra_bi_size)
 {
     /* clear boot info memory */
-    clearMemory((void *)rootserver.boot_info, seL4_BootInfoFrameBits);
+    clearMemory((void *)rootserver.boot_info, BI_FRAME_SIZE_BITS);
     if (extra_bi_size) {
         clearMemory((void *)rootserver.extra_bi,
                     calculate_extra_bi_size_bits(extra_bi_size));
@@ -608,8 +584,7 @@ BOOT_CODE void init_core_state(tcb_t *scheduler_action)
     NODE_STATE(ksCurSC) = NODE_STATE(ksCurThread->tcbSchedContext);
     NODE_STATE(ksConsumed) = 0;
     NODE_STATE(ksReprogram) = true;
-    NODE_STATE(ksReleaseQueue.head) = NULL;
-    NODE_STATE(ksReleaseQueue.end) = NULL;
+    NODE_STATE(ksReleaseHead) = NULL;
     NODE_STATE(ksCurTime) = getCurrentTime();
 #endif
 }
@@ -727,8 +702,8 @@ BOOT_CODE static bool_t create_untypeds_for_region(
 {
     /* This code works with regions that wrap (where end < start), because the loop cuts up the
        region into size-aligned chunks, one for each cap. Memory chunks that are size-aligned cannot
-       themselves overflow, so they satisfy alignment, size, and overflow conditions. The region
-       [0..end) is not necessarily part of the kernel window (depending on the value of PPTR_BASE).
+       themselves overflow, so they satisfy alignement, size, and overflow conditionds. The region
+       [0..end) is not neccessarily part of the kernel window (depending on the value of PPTR_BASE).
        This is fine for device untypeds. For normal untypeds, the region is assumed to be fully in
        the kernel window. This is not checked here. */
     while (!is_reg_empty(reg)) {
@@ -764,7 +739,8 @@ BOOT_CODE static bool_t create_untypeds_for_region(
     return true;
 }
 
-BOOT_CODE bool_t create_untypeds(cap_t root_cnode_cap)
+BOOT_CODE bool_t create_untypeds(cap_t root_cnode_cap,
+                                 region_t boot_mem_reuse_reg)
 {
     seL4_SlotPos first_untyped_slot = ndks_boot.slot_pos_cur;
 
@@ -798,11 +774,7 @@ BOOT_CODE bool_t create_untypeds(cap_t root_cnode_cap)
         }
     }
 
-    /* There is a part of the kernel (code/data) that is only needed for the
-     * boot process. We can create UT objects for these frames, so the memory
-     * can be reused.
-     */
-    region_t boot_mem_reuse_reg = paddr_to_pptr_reg(get_p_reg_kernel_img_boot());
+    /* if boot_mem_reuse_reg is not empty, we can create UT objs from boot code/data frames */
     if (!create_untypeds_for_region(root_cnode_cap, false, boot_mem_reuse_reg, first_untyped_slot)) {
         printf("ERROR: creation of untypeds for recycled boot memory"
                " [%"SEL4_PRIx_word"..%"SEL4_PRIx_word"] failed\n",
diff --git a/src/kernel/sporadic.c b/src/kernel/sporadic.c
index 941479d8..066daf8d 100644
--- a/src/kernel/sporadic.c
+++ b/src/kernel/sporadic.c
@@ -73,9 +73,7 @@ static UNUSED bool_t refill_ordered(sched_context_t *sc)
 
     while (current != sc->scRefillTail) {
         if (!(refill_index(sc, current)->rTime + refill_index(sc, current)->rAmount <= refill_index(sc, next)->rTime)) {
-#ifdef CONFIG_PRINTING
             refill_print(sc);
-#endif
             return false;
         }
         current = next;
@@ -239,8 +237,8 @@ static inline void schedule_used(sched_context_t *sc, refill_t new)
 static bool_t refill_head_overlapping(sched_context_t *sc)
 {
     if (!refill_single(sc)) {
-        refill_t head = *refill_head(sc);
-        ticks_t tail = head.rTime + head.rAmount;
+        ticks_t amount = refill_head(sc)->rAmount;
+        ticks_t tail = refill_head(sc)->rTime + amount;
         return refill_index(sc, refill_next(sc, sc->scRefillHead))->rTime <= tail;
     } else {
         return false;
@@ -313,12 +311,6 @@ void refill_budget_check(ticks_t usage)
     REFILL_SANITY_END(sc);
 }
 
-static inline void merge_overlapping_head_refill(sched_context_t *sc)
-{
-    refill_t old_head = refill_pop_head(sc);
-    refill_head(sc)->rTime = old_head.rTime;
-    refill_head(sc)->rAmount += old_head.rAmount;
-}
 
 void refill_unblock_check(sched_context_t *sc)
 {
@@ -336,7 +328,9 @@ void refill_unblock_check(sched_context_t *sc)
 
         /* merge available replenishments */
         while (refill_head_overlapping(sc)) {
-            merge_overlapping_head_refill(sc);
+            refill_t old_head = refill_pop_head(sc);
+            refill_head(sc)->rTime = old_head.rTime;
+            refill_head(sc)->rAmount += old_head.rAmount;
         }
 
         assert(refill_sufficient(sc, 0));
diff --git a/src/kernel/thread.c b/src/kernel/thread.c
index 4983a40d..83a23d0d 100644
--- a/src/kernel/thread.c
+++ b/src/kernel/thread.c
@@ -574,24 +574,17 @@ void postpone(sched_context_t *sc)
 
 void setNextInterrupt(void)
 {
-    ticks_t next_interrupt = NODE_STATE(ksCurTime) +
-                             refill_head(NODE_STATE(ksCurThread)->tcbSchedContext)->rAmount;
+    time_t next_interrupt = NODE_STATE(ksCurTime) +
+                            refill_head(NODE_STATE(ksCurThread)->tcbSchedContext)->rAmount;
 
     if (numDomains > 1) {
         next_interrupt = MIN(next_interrupt, NODE_STATE(ksCurTime) + ksDomainTime);
     }
 
-    if (NODE_STATE(ksReleaseQueue.head) != NULL) {
-        next_interrupt = MIN(refill_head(NODE_STATE(ksReleaseQueue.head)->tcbSchedContext)->rTime, next_interrupt);
+    if (NODE_STATE(ksReleaseHead) != NULL) {
+        next_interrupt = MIN(refill_head(NODE_STATE(ksReleaseHead)->tcbSchedContext)->rTime, next_interrupt);
     }
 
-    /* We should never be attempting to schedule anything earlier than ksCurTime */
-    assert(next_interrupt >= NODE_STATE(ksCurTime));
-
-    /* Our lower bound ksCurTime is slightly in the past (at kernel entry) and
-       we are further subtracting getTimerPrecision(), so we may be setting a
-       deadline in the past. If that is the case, we assume the IRQ will be
-       raised immediately after we leave the kernel. */
     setDeadline(next_interrupt - getTimerPrecision());
 }
 
@@ -625,6 +618,7 @@ void endTimeslice(bool_t can_timeout_fault)
         handleTimeout(NODE_STATE(ksCurThread));
     } else if (refill_ready(NODE_STATE(ksCurSC)) && refill_sufficient(NODE_STATE(ksCurSC), 0)) {
         /* apply round robin */
+        assert(refill_sufficient(NODE_STATE(ksCurSC), 0));
         assert(!thread_state_get_tcbQueued(NODE_STATE(ksCurThread)->tcbState));
         SCHED_APPEND_CURRENT_TCB;
     } else {
@@ -679,35 +673,21 @@ void rescheduleRequired(void)
 }
 
 #ifdef CONFIG_KERNEL_MCS
-
-static inline bool_t PURE release_q_non_empty_and_ready(void)
-{
-    return NODE_STATE(ksReleaseQueue.head) != NULL
-           && refill_ready(NODE_STATE(ksReleaseQueue.head)->tcbSchedContext);
-}
-
-static void tcbReleaseDequeue(void)
-{
-    assert(NODE_STATE(ksReleaseQueue.head) != NULL);
-    assert(NODE_STATE(ksReleaseQueue.head)->tcbSchedPrev == NULL);
-    SMP_COND_STATEMENT(assert(NODE_STATE(ksReleaseQueue.head)->tcbAffinity == getCurrentCPUIndex()));
-
-    tcb_t *awakened = NODE_STATE(ksReleaseQueue.head);
-    assert(awakened != NODE_STATE(ksCurThread));
-    tcbReleaseRemove(awakened);
-    /* round robin threads should not be in the release queue */
-    assert(!isRoundRobin(awakened->tcbSchedContext));
-    /* threads should wake up on the correct core */
-    SMP_COND_STATEMENT(assert(awakened->tcbAffinity == getCurrentCPUIndex()));
-    /* threads HEAD refill should always be >= MIN_BUDGET */
-    assert(refill_sufficient(awakened->tcbSchedContext, 0));
-    possibleSwitchTo(awakened);
-}
-
 void awaken(void)
 {
-    while (unlikely(release_q_non_empty_and_ready())) {
-        tcbReleaseDequeue();
+    while (unlikely(NODE_STATE(ksReleaseHead) != NULL && refill_ready(NODE_STATE(ksReleaseHead)->tcbSchedContext))) {
+        tcb_t *awakened = tcbReleaseDequeue();
+        /* the currently running thread cannot have just woken up */
+        assert(awakened != NODE_STATE(ksCurThread));
+        /* round robin threads should not be in the release queue */
+        assert(!isRoundRobin(awakened->tcbSchedContext));
+        /* threads should wake up on the correct core */
+        SMP_COND_STATEMENT(assert(awakened->tcbAffinity == getCurrentCPUIndex()));
+        /* threads HEAD refill should always be >= MIN_BUDGET */
+        assert(refill_sufficient(awakened->tcbSchedContext, 0));
+        possibleSwitchTo(awakened);
+        /* changed head of release queue -> need to reprogram */
+        NODE_STATE(ksReprogram) = true;
     }
 }
 #endif
diff --git a/src/machine/capdl.c b/src/machine/capdl.c
index e09191f1..c67c88c1 100644
--- a/src/machine/capdl.c
+++ b/src/machine/capdl.c
@@ -122,13 +122,11 @@ static inline ticks_t sc_get_budget(sched_context_t *sc)
 void obj_sc_print_attrs(cap_t sc_cap)
 {
     sched_context_t *sc = SC_PTR(cap_sched_context_cap_get_capSCPtr(sc_cap));
-    ticks_t period = sc->scPeriod;
-    ticks_t budget = sc_get_budget(sc);
-    printf("(period: %"PRIu64" us (%"PRIu64" ticks), budget: %"PRIu64 " us "
-           "(%"PRIu64" ticks), %"SEL4_PRIu_word" bits)\n",
-           ticksToUs(period), period,
-           ticksToUs(budget), budget,
-           (word_t)cap_sched_context_cap_get_capSCSizeBits(sc_cap));
+    printf("(period: %lu, budget: %lu, %lu bits)\n",
+           (long unsigned int)ticksToUs(sc->scPeriod),
+           (long unsigned int)ticksToUs(sc_get_budget(sc)),
+           (word_t)cap_sched_context_cap_get_capSCSizeBits(sc_cap)
+          );
 }
 #endif /* CONFIG_KERNEL_MCS */
 
diff --git a/src/model/preemption.c b/src/model/preemption.c
index c835b2a3..fa0ae222 100644
--- a/src/model/preemption.c
+++ b/src/model/preemption.c
@@ -30,8 +30,8 @@ exception_t preemptionPoint(void)
         ksWorkUnitsCompleted = 0;
 #ifdef CONFIG_KERNEL_MCS
         updateTimestamp();
-        if (isIRQPending() || isCurDomainExpired()
-            || !(sc_active(NODE_STATE(ksCurSC)) && refill_sufficient(NODE_STATE(ksCurSC), NODE_STATE(ksConsumed)))) {
+        if (!(sc_active(NODE_STATE(ksCurSC)) && refill_sufficient(NODE_STATE(ksCurSC), NODE_STATE(ksConsumed)))
+            || isCurDomainExpired() || isIRQPending()) {
 #else
         if (isIRQPending()) {
 #endif
diff --git a/src/model/smp.c b/src/model/smp.c
index dd3a7a6e..0cd95216 100644
--- a/src/model/smp.c
+++ b/src/model/smp.c
@@ -15,15 +15,7 @@ void migrateTCB(tcb_t *tcb, word_t new_core)
 #ifdef CONFIG_DEBUG_BUILD
     tcbDebugRemove(tcb);
 #endif
-#ifdef CONFIG_HAVE_FPU
-    /* If the thread owns the FPU of the core it is currently running on (which
-     * is not necessarily the core, that we are now running on), then release
-     * that cores's FPU.
-     */
-    if (nativeThreadUsingFPU(tcb)) {
-        switchFpuOwner(NULL, tcb->tcbAffinity);
-    }
-#endif /* CONFIG_HAVE_FPU */
+    Arch_migrateTCB(tcb);
     tcb->tcbAffinity = new_core;
 #ifdef CONFIG_DEBUG_BUILD
     tcbDebugAppend(tcb);
diff --git a/src/model/statedata.c b/src/model/statedata.c
index 88d7bbdc..f3bf19bf 100644
--- a/src/model/statedata.c
+++ b/src/model/statedata.c
@@ -27,7 +27,7 @@ UP_STATE_DEFINE(word_t, ksReadyQueuesL2Bitmap[CONFIG_NUM_DOMAINS][L2_BITMAP_SIZE
 compile_assert(ksReadyQueuesL1BitmapBigEnough, (L2_BITMAP_SIZE - 1) <= wordBits)
 #ifdef CONFIG_KERNEL_MCS
 /* Head of the queue of threads waiting for their budget to be replenished */
-UP_STATE_DEFINE(tcb_queue_t, ksReleaseQueue);
+UP_STATE_DEFINE(tcb_t *, ksReleaseHead);
 #endif
 
 /* Current thread TCB pointer */
diff --git a/src/object/cnode.c b/src/object/cnode.c
index 8686b5a3..2ee57f69 100644
--- a/src/object/cnode.c
+++ b/src/object/cnode.c
@@ -800,25 +800,12 @@ bool_t PURE isMDBParentOf(cte_t *cte_a, cte_t *cte_b)
         if (badge == 0) {
             return true;
         }
-        return (badge == cap_notification_cap_get_capNtfnBadge(cte_b->cap)) &&
-               !mdb_node_get_mdbFirstBadged(cte_b->cteMDBNode);
+        return
+            (badge == cap_notification_cap_get_capNtfnBadge(cte_b->cap)) &&
+            !mdb_node_get_mdbFirstBadged(cte_b->cteMDBNode);
         break;
     }
 
-#ifdef CONFIG_ALLOW_SMC_CALLS
-    case cap_smc_cap: {
-        word_t badge;
-
-        badge = cap_smc_cap_get_capSMCBadge(cte_a->cap);
-        if (badge == 0) {
-            return true;
-        }
-        return (badge == cap_smc_cap_get_capSMCBadge(cte_b->cap)) &&
-               !mdb_node_get_mdbFirstBadged(cte_b->cteMDBNode);
-        break;
-    }
-#endif
-
     default:
         return true;
         break;
diff --git a/src/object/objecttype.c b/src/object/objecttype.c
index e23c5fe9..17b6d3e7 100644
--- a/src/object/objecttype.c
+++ b/src/object/objecttype.c
@@ -191,7 +191,7 @@ finaliseCap_ret_t finaliseCap(cap_t cap, bool_t final, bool_t exposed)
 #ifdef CONFIG_KERNEL_MCS
             sched_context_t *sc = SC_PTR(tcb->tcbSchedContext);
             if (sc) {
-                schedContext_unbindTCB(sc);
+                schedContext_unbindTCB(sc, tcb);
                 if (sc->scYieldFrom) {
                     schedContext_completeYieldTo(sc->scYieldFrom);
                 }
@@ -230,7 +230,6 @@ finaliseCap_ret_t finaliseCap(cap_t cap, bool_t final, bool_t exposed)
             }
             /* mark the sc as no longer valid */
             sc->scRefillMax = 0;
-            sc->scSporadic = false;
             fc_ret.remainder = cap_null_cap_new();
             fc_ret.cleanupInfo = cap_null_cap_new();
             return fc_ret;
diff --git a/src/object/schedcontext.c b/src/object/schedcontext.c
index 0c5d8eaa..118865bd 100644
--- a/src/object/schedcontext.c
+++ b/src/object/schedcontext.c
@@ -14,7 +14,7 @@ static exception_t invokeSchedContext_UnbindObject(sched_context_t *sc, cap_t ca
 {
     switch (cap_get_capType(cap)) {
     case cap_thread_cap:
-        schedContext_unbindTCB(sc);
+        schedContext_unbindTCB(sc, sc->scTcb);
         break;
     case cap_notification_cap:
         schedContext_unbindNtfn(sc);
@@ -317,20 +317,19 @@ void schedContext_bindTCB(sched_context_t *sc, tcb_t *tcb)
     }
 }
 
-void schedContext_unbindTCB(sched_context_t *sc)
+void schedContext_unbindTCB(sched_context_t *sc, tcb_t *tcb)
 {
-    tcb_t *tcb = sc->scTcb;
-    assert(tcb != NULL);
+    assert(sc->scTcb == tcb);
 
     /* tcb must already be stalled at this point */
     if (tcb == NODE_STATE(ksCurThread)) {
         rescheduleRequired();
     }
 
-    tcbSchedDequeue(tcb);
-    tcbReleaseRemove(tcb);
+    tcbSchedDequeue(sc->scTcb);
+    tcbReleaseRemove(sc->scTcb);
 
-    tcb->tcbSchedContext = NULL;
+    sc->scTcb->tcbSchedContext = NULL;
     sc->scTcb = NULL;
 }
 
@@ -338,7 +337,7 @@ void schedContext_unbindAllTCBs(sched_context_t *sc)
 {
     if (sc->scTcb) {
         SMP_COND_STATEMENT(remoteTCBStall(sc->scTcb));
-        schedContext_unbindTCB(sc);
+        schedContext_unbindTCB(sc, sc->scTcb);
     }
 }
 
diff --git a/src/object/schedcontrol.c b/src/object/schedcontrol.c
index b1e85da9..86a16686 100644
--- a/src/object/schedcontrol.c
+++ b/src/object/schedcontrol.c
@@ -13,6 +13,10 @@
 static exception_t invokeSchedControl_ConfigureFlags(sched_context_t *target, word_t core, ticks_t budget,
                                                      ticks_t period, word_t max_refills, word_t badge, word_t flags)
 {
+
+    target->scBadge = badge;
+    target->scSporadic = (flags & seL4_SchedContext_Sporadic) != 0;
+
     /* don't modify parameters of tcb while it is in a sorted queue */
     if (target->scTcb) {
         /* possibly stall a remote core */
@@ -69,9 +73,6 @@ static exception_t invokeSchedControl_ConfigureFlags(sched_context_t *target, wo
         }
     }
 
-    target->scBadge = badge;
-    target->scSporadic = (flags & seL4_SchedContext_Sporadic) != 0;
-
     return EXCEPTION_NONE;
 }
 
diff --git a/src/object/tcb.c b/src/object/tcb.c
index 4c54fee0..3c3ae9d5 100644
--- a/src/object/tcb.c
+++ b/src/object/tcb.c
@@ -78,39 +78,6 @@ static inline void removeFromBitmap(word_t cpu, word_t dom, word_t prio)
     }
 }
 
-tcb_queue_t tcb_queue_remove(tcb_queue_t queue, tcb_t *tcb)
-{
-    tcb_t *before;
-    tcb_t *after;
-
-    before = tcb->tcbSchedPrev;
-    after = tcb->tcbSchedNext;
-
-    if (queue.head == tcb && queue.end == tcb) {
-        queue.head = NULL;
-        queue.end = NULL;
-    } else {
-        if (queue.head == tcb) {
-            after->tcbSchedPrev = NULL;
-            tcb->tcbSchedNext = NULL;
-            queue.head = after;
-        } else {
-            if (queue.end == tcb) {
-                before->tcbSchedNext = NULL;
-                tcb->tcbSchedPrev = NULL;
-                queue.end = before;
-            } else {
-                before->tcbSchedNext = after;
-                after->tcbSchedPrev = before;
-                tcb->tcbSchedPrev = NULL;
-                tcb->tcbSchedNext = NULL;
-            }
-        }
-    }
-
-    return queue;
-}
-
 /* Add TCB to the head of a scheduler queue */
 void tcbSchedEnqueue(tcb_t *tcb)
 {
@@ -130,11 +97,17 @@ void tcbSchedEnqueue(tcb_t *tcb)
         idx = ready_queues_index(dom, prio);
         queue = NODE_STATE_ON_CORE(ksReadyQueues[idx], tcb->tcbAffinity);
 
-        if (tcb_queue_empty(queue)) {
+        if (!queue.end) { /* Empty list */
+            queue.end = tcb;
             addToBitmap(SMP_TERNARY(tcb->tcbAffinity, 0), dom, prio);
+        } else {
+            queue.head->tcbSchedPrev = tcb;
         }
+        tcb->tcbSchedPrev = NULL;
+        tcb->tcbSchedNext = queue.head;
+        queue.head = tcb;
 
-        NODE_STATE_ON_CORE(ksReadyQueues[idx], tcb->tcbAffinity) = tcb_queue_prepend(queue, tcb);
+        NODE_STATE_ON_CORE(ksReadyQueues[idx], tcb->tcbAffinity) = queue;
 
         thread_state_ptr_set_tcbQueued(&tcb->tcbState, true);
     }
@@ -159,11 +132,17 @@ void tcbSchedAppend(tcb_t *tcb)
         idx = ready_queues_index(dom, prio);
         queue = NODE_STATE_ON_CORE(ksReadyQueues[idx], tcb->tcbAffinity);
 
-        if (tcb_queue_empty(queue)) {
+        if (!queue.head) { /* Empty list */
+            queue.head = tcb;
             addToBitmap(SMP_TERNARY(tcb->tcbAffinity, 0), dom, prio);
+        } else {
+            queue.end->tcbSchedNext = tcb;
         }
+        tcb->tcbSchedPrev = queue.end;
+        tcb->tcbSchedNext = NULL;
+        queue.end = tcb;
 
-        NODE_STATE_ON_CORE(ksReadyQueues[idx], tcb->tcbAffinity) = tcb_queue_append(queue, tcb);
+        NODE_STATE_ON_CORE(ksReadyQueues[idx], tcb->tcbAffinity) = queue;
 
         thread_state_ptr_set_tcbQueued(&tcb->tcbState, true);
     }
@@ -174,7 +153,6 @@ void tcbSchedDequeue(tcb_t *tcb)
 {
     if (thread_state_get_tcbQueued(tcb->tcbState)) {
         tcb_queue_t queue;
-        tcb_queue_t new_queue;
         dom_t dom;
         prio_t prio;
         word_t idx;
@@ -184,15 +162,24 @@ void tcbSchedDequeue(tcb_t *tcb)
         idx = ready_queues_index(dom, prio);
         queue = NODE_STATE_ON_CORE(ksReadyQueues[idx], tcb->tcbAffinity);
 
-        new_queue = tcb_queue_remove(queue, tcb);
+        if (tcb->tcbSchedPrev) {
+            tcb->tcbSchedPrev->tcbSchedNext = tcb->tcbSchedNext;
+        } else {
+            queue.head = tcb->tcbSchedNext;
+            if (likely(!tcb->tcbSchedNext)) {
+                removeFromBitmap(SMP_TERNARY(tcb->tcbAffinity, 0), dom, prio);
+            }
+        }
 
-        NODE_STATE_ON_CORE(ksReadyQueues[idx], tcb->tcbAffinity) = new_queue;
+        if (tcb->tcbSchedNext) {
+            tcb->tcbSchedNext->tcbSchedPrev = tcb->tcbSchedPrev;
+        } else {
+            queue.end = tcb->tcbSchedPrev;
+        }
 
-        thread_state_ptr_set_tcbQueued(&tcb->tcbState, false);
+        NODE_STATE_ON_CORE(ksReadyQueues[idx], tcb->tcbAffinity) = queue;
 
-        if (likely(tcb_queue_empty(new_queue))) {
-            removeFromBitmap(SMP_TERNARY(tcb->tcbAffinity, 0), dom, prio);
-        }
+        thread_state_ptr_set_tcbQueued(&tcb->tcbState, false);
     }
 }
 
@@ -270,68 +257,82 @@ tcb_queue_t tcbEPDequeue(tcb_t *tcb, tcb_queue_t queue)
 }
 
 #ifdef CONFIG_KERNEL_MCS
-
 void tcbReleaseRemove(tcb_t *tcb)
 {
     if (likely(thread_state_get_tcbInReleaseQueue(tcb->tcbState))) {
-        tcb_queue_t queue = NODE_STATE_ON_CORE(ksReleaseQueue, tcb->tcbAffinity);
-
-        if (queue.head == tcb) {
+        if (tcb->tcbSchedPrev) {
+            tcb->tcbSchedPrev->tcbSchedNext = tcb->tcbSchedNext;
+        } else {
+            NODE_STATE_ON_CORE(ksReleaseHead, tcb->tcbAffinity) = tcb->tcbSchedNext;
+            /* the head has changed, we might need to set a new timeout */
             NODE_STATE_ON_CORE(ksReprogram, tcb->tcbAffinity) = true;
         }
 
-        NODE_STATE_ON_CORE(ksReleaseQueue, tcb->tcbAffinity) = tcb_queue_remove(queue, tcb);
+        if (tcb->tcbSchedNext) {
+            tcb->tcbSchedNext->tcbSchedPrev = tcb->tcbSchedPrev;
+        }
 
+        tcb->tcbSchedNext = NULL;
+        tcb->tcbSchedPrev = NULL;
         thread_state_ptr_set_tcbInReleaseQueue(&tcb->tcbState, false);
     }
 }
 
-static inline ticks_t PURE tcbReadyTime(tcb_t *tcb)
-{
-    return refill_head(tcb->tcbSchedContext)->rTime;
-}
-
-static inline bool_t PURE time_after(tcb_t *tcb, ticks_t new_time)
+void tcbReleaseEnqueue(tcb_t *tcb)
 {
-    return tcb != NULL && new_time >= tcbReadyTime(tcb);
-}
+    assert(thread_state_get_tcbInReleaseQueue(tcb->tcbState) == false);
+    assert(thread_state_get_tcbQueued(tcb->tcbState) == false);
 
-static tcb_t *find_time_after(tcb_t *tcb, ticks_t new_time)
-{
-    tcb_t *after = tcb;
+    tcb_t *before = NULL;
+    tcb_t *after = NODE_STATE_ON_CORE(ksReleaseHead, tcb->tcbAffinity);
 
-    while (time_after(after, new_time)) {
+    /* find our place in the ordered queue */
+    while (after != NULL &&
+           refill_head(tcb->tcbSchedContext)->rTime >= refill_head(after->tcbSchedContext)->rTime) {
+        before = after;
         after = after->tcbSchedNext;
     }
 
-    return after;
+    if (before == NULL) {
+        /* insert at head */
+        NODE_STATE_ON_CORE(ksReleaseHead, tcb->tcbAffinity) = tcb;
+        NODE_STATE_ON_CORE(ksReprogram, tcb->tcbAffinity) = true;
+    } else {
+        before->tcbSchedNext = tcb;
+    }
+
+    if (after != NULL) {
+        after->tcbSchedPrev = tcb;
+    }
+
+    tcb->tcbSchedNext = after;
+    tcb->tcbSchedPrev = before;
+
+    thread_state_ptr_set_tcbInReleaseQueue(&tcb->tcbState, true);
 }
 
-void tcbReleaseEnqueue(tcb_t *tcb)
+tcb_t *tcbReleaseDequeue(void)
 {
-    assert(thread_state_get_tcbInReleaseQueue(tcb->tcbState) == false);
-    assert(thread_state_get_tcbQueued(tcb->tcbState) == false);
+    assert(NODE_STATE(ksReleaseHead) != NULL);
+    assert(NODE_STATE(ksReleaseHead)->tcbSchedPrev == NULL);
+    SMP_COND_STATEMENT(assert(NODE_STATE(ksReleaseHead)->tcbAffinity == getCurrentCPUIndex()));
 
-    ticks_t new_time;
-    tcb_queue_t queue;
+    tcb_t *detached_head = NODE_STATE(ksReleaseHead);
+    NODE_STATE(ksReleaseHead) = NODE_STATE(ksReleaseHead)->tcbSchedNext;
 
-    new_time = tcbReadyTime(tcb);
-    queue = NODE_STATE_ON_CORE(ksReleaseQueue, tcb->tcbAffinity);
+    if (NODE_STATE(ksReleaseHead)) {
+        NODE_STATE(ksReleaseHead)->tcbSchedPrev = NULL;
+    }
 
-    if (tcb_queue_empty(queue) || new_time < tcbReadyTime(queue.head)) {
-        NODE_STATE_ON_CORE(ksReleaseQueue, tcb->tcbAffinity) = tcb_queue_prepend(queue, tcb);
-        NODE_STATE_ON_CORE(ksReprogram, tcb->tcbAffinity) = true;
-    } else {
-        if (tcbReadyTime(queue.end) <= new_time) {
-            NODE_STATE_ON_CORE(ksReleaseQueue, tcb->tcbAffinity) = tcb_queue_append(queue, tcb);
-        } else {
-            tcb_t *after;
-            after = find_time_after(queue.head, new_time);
-            tcb_queue_insert(tcb, after);
-        }
+    if (detached_head->tcbSchedNext) {
+        detached_head->tcbSchedNext->tcbSchedPrev = NULL;
+        detached_head->tcbSchedNext = NULL;
     }
 
-    thread_state_ptr_set_tcbInReleaseQueue(&tcb->tcbState, true);
+    thread_state_ptr_set_tcbInReleaseQueue(&detached_head->tcbState, false);
+    NODE_STATE(ksReprogram) = true;
+
+    return detached_head;
 }
 #endif
 
@@ -1029,6 +1030,7 @@ static bool_t validFaultHandler(cap_t cap)
         if (!cap_endpoint_cap_get_capCanSend(cap) ||
             (!cap_endpoint_cap_get_capCanGrant(cap) &&
              !cap_endpoint_cap_get_capCanGrantReply(cap))) {
+            current_syscall_error.type = seL4_InvalidCapability;
             return false;
         }
         break;
@@ -1036,6 +1038,7 @@ static bool_t validFaultHandler(cap_t cap)
         /* just has no fault endpoint */
         break;
     default:
+        current_syscall_error.type = seL4_InvalidCapability;
         return false;
     }
     return true;
@@ -1261,7 +1264,6 @@ exception_t decodeSetTimeoutEndpoint(cap_t cap, cte_t *slot)
     /* timeout handler */
     if (!validFaultHandler(thCap)) {
         userError("TCB SetTimeoutEndpoint: timeout endpoint cap invalid.");
-        current_syscall_error.type = seL4_InvalidCapability;
         current_syscall_error.invalidCapNumber = 1;
         return EXCEPTION_SYSCALL_ERROR;
     }
@@ -1536,7 +1538,6 @@ exception_t decodeSetSpace(cap_t cap, word_t length, cte_t *slot, word_t *buffer
     /* fault handler */
     if (!validFaultHandler(fhCap)) {
         userError("TCB SetSpace: fault endpoint cap invalid.");
-        current_syscall_error.type = seL4_InvalidCapability;
         current_syscall_error.invalidCapNumber = 1;
         return EXCEPTION_SYSCALL_ERROR;
     }
@@ -1695,11 +1696,9 @@ static inline exception_t installTCBCap(tcb_t *target, cap_t tCap, cte_t *slot,
     }
 
     /* cteDelete on a cap installed in the tcb cannot fail */
-    if (cap_get_capType(newCap) != cap_null_cap) {
-        if (sameObjectAs(newCap, srcSlot->cap) &&
-            sameObjectAs(tCap, slot->cap)) {
-            cteInsert(newCap, srcSlot, rootSlot);
-        }
+    if (sameObjectAs(newCap, srcSlot->cap) &&
+        sameObjectAs(tCap, slot->cap)) {
+        cteInsert(newCap, srcSlot, rootSlot);
     }
     return e;
 }
@@ -1870,7 +1869,7 @@ exception_t invokeTCB_ThreadControlSched(tcb_t *target, cte_t *slot,
         if (sc != NULL && sc != target->tcbSchedContext) {
             schedContext_bindTCB(sc, target);
         } else if (sc == NULL && target->tcbSchedContext != NULL) {
-            schedContext_unbindTCB(target->tcbSchedContext);
+            schedContext_unbindTCB(target->tcbSchedContext, target);
         }
     }
 
 
 
diff --git a/src/smp/ipi.c b/src/smp/ipi.c
index ccc52a2b..bdae66c2 100644
--- a/src/smp/ipi.c
+++ b/src/smp/ipi.c
@@ -131,7 +131,6 @@ void generic_ipi_send_mask(irq_t ipi, word_t mask, bool_t isBlocking)
             target_cores[nr_target_cores] = index;
             nr_target_cores++;
         } else {
-            IPI_MEM_BARRIER;
             ipi_send_target(ipi, cpuIndexToID(index));
         }
         mask &= ~BIT(index);
